{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2515,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from ollama import Client\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import ipaddress\n",
    "from datetime import datetime\n",
    "import shlex\n",
    "import math\n",
    "from typing import Optional, Dict, Any\n",
    "import argparse, secrets, hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2516,
   "metadata": {},
   "outputs": [],
   "source": [
    "RYU_CONTROLLER_IP = \"127.0.0.1\"\n",
    "RYU_CONTROLLER_PORT = 8080\n",
    "url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}/stats/flowentry/delete\"\n",
    "# ONOS Controller Details\n",
    "Ryu_BASE_URL = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}/stats/flowentry/\"\n",
    "# Define sudo password\n",
    "sudo_password = \"test@irciss008\" #your localhost password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2517,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_to_host = {\n",
    "    \"10.0.1.1\": \"h1\",\n",
    "    \"10.0.1.2\": \"h2\",\n",
    "    \"10.0.1.3\": \"h3\",\n",
    "    \"10.0.1.4\": \"h4\"\n",
    "            }\n",
    "\n",
    "host_to_ip = {\n",
    "    \"h1\": \"10.0.1.1\",\n",
    "    \"h2\": \"10.0.1.2\",\n",
    "    \"h3\": \"10.0.1.3\",\n",
    "    \"h4\": \"10.0.1.4\",\n",
    "}\n",
    "\n",
    "# --- Diamond topology wiring helpers (drop-in) ---\n",
    "\n",
    "SW_OF = {\n",
    "    \"1\": \"of:0000000000000001\",  # s1\n",
    "    \"2\": \"of:0000000000000002\",  # s2\n",
    "    \"3\": \"of:0000000000000003\",  # s3\n",
    "    \"4\": \"of:0000000000000004\",  # s4\n",
    "}\n",
    "\n",
    "HOSTS = {\n",
    "    \"h1\": \"10.0.1.1\",\n",
    "    \"h2\": \"10.0.1.2\",\n",
    "    \"h3\": \"10.0.1.3\",\n",
    "    \"h4\": \"10.0.1.4\",\n",
    "}\n",
    "\n",
    "# Host attachment (edge switch, access port) from your Mininet build\n",
    "HOST_ATTACH = {\n",
    "    HOSTS[\"h1\"]: (SW_OF[\"1\"], 3),  # h1 -> s1:3\n",
    "    HOSTS[\"h2\"]: (SW_OF[\"1\"], 4),  # h2 -> s1:4\n",
    "    HOSTS[\"h3\"]: (SW_OF[\"4\"], 3),  # h3 -> s4:3\n",
    "    HOSTS[\"h4\"]: (SW_OF[\"4\"], 4),  # h4 -> s4:4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2518,
   "metadata": {},
   "outputs": [],
   "source": [
    "switch_id_for_llm_assurance = None\n",
    "llm_caller_flag = 0\n",
    "gl_cookie = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_flow(base, flow):\n",
    "    \"\"\"\n",
    "    Deletes a specific flow rule from a switch using OFPFC_DELETE_STRICT.\n",
    "    'flow' must be a valid Ryu flow rule dictionary.\n",
    "    \"\"\"\n",
    "    # Use OFPFC_DELETE_STRICT to delete the exact flow\n",
    "    # We must include match, priority, and cookie (if it exists)\n",
    "    payload = {\n",
    "        \"dpid\": flow.get(\"dpid\"),\n",
    "        \"cmd\": \"OFPFC_DELETE_STRICT\",\n",
    "        \"table_id\": flow.get(\"table_id\", 0),\n",
    "        \"priority\": flow.get(\"priority\", 0),\n",
    "        \"cookie\": flow.get(\"cookie\", 0),\n",
    "        \"cookie_mask\": flow.get(\"cookie_mask\", 0) if flow.get(\"cookie\") else 0,\n",
    "        \"match\": flow.get(\"match\", {})\n",
    "    }\n",
    "    \n",
    "    # If a cookie was specified, use a full mask for an exact match\n",
    "    if payload[\"cookie\"] != 0:\n",
    "        payload[\"cookie_mask\"] = 0xffffffffffffffff\n",
    "\n",
    "    r = requests.post(f\"{base}/stats/flowentry/delete\", json=payload, timeout=5)\n",
    "    if r.status_code != 200:\n",
    "        print(f\"Warning: Delete failed [{r.status_code}]: {r.text}\")\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def ryu_get(base, path):\n",
    "    r = requests.get(f\"{base}{path}\", timeout=5)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def wait_for_dpids(base, expect, timeout=30):\n",
    "    want = set(expect); start = time.time()\n",
    "    seen = set()\n",
    "    while time.time() - start < timeout:\n",
    "        try:\n",
    "            seen = set(ryu_get(base, \"/stats/switches\"))\n",
    "            if want.issubset(seen):\n",
    "                return sorted(seen)\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "    return sorted(seen)\n",
    "\n",
    "def add_flow(base, flow):\n",
    "    r = requests.post(f\"{base}/stats/flowentry/add\", json=flow, timeout=5)\n",
    "    if r.status_code != 200:\n",
    "        raise SystemExit(f\"Add failed [{r.status_code}]: {r.text}\")\n",
    "    return True\n",
    "\n",
    "def get_flows(base, dpid):\n",
    "    return ryu_get(base, f\"/stats/flow/{dpid}\")\n",
    "\n",
    "\n",
    "def find_by_cookie(flow_dump, cookie):\n",
    "    for f in flow_dump:\n",
    "        if int(f.get(\"cookie\", -1)) == int(cookie):\n",
    "            return f\n",
    "    return None\n",
    "\n",
    "def roughly_matches(entry, want_match, want_prio):\n",
    "    def norm(m):\n",
    "        m = dict(m)\n",
    "        if \"dl_type\" in m and \"eth_type\" not in m: m[\"eth_type\"] = m.pop(\"dl_type\")\n",
    "        if \"nw_proto\" in m and \"ip_proto\" not in m: m[\"ip_proto\"] = m.pop(\"nw_proto\")\n",
    "        return m\n",
    "    return entry.get(\"priority\") == want_prio and norm(entry.get(\"match\", {})) == norm(want_match)\n",
    "\n",
    "def exists(base, dpid, match, priority, cookie=None, retries=3, delay=0.5):\n",
    "    for _ in range(retries):\n",
    "        dump = get_flows(base, dpid).get(str(dpid), [])\n",
    "        if cookie is not None:\n",
    "            # Cookie-only check: if not found, do NOT fall back to match/priority\n",
    "            found = find_by_cookie(dump, cookie)\n",
    "            if found:\n",
    "                return True, found\n",
    "        else:\n",
    "            # Only use match+priority when no cookie is specified\n",
    "            for f in dump:\n",
    "                if roughly_matches(f, match, priority):\n",
    "                    return True, f\n",
    "        time.sleep(delay)\n",
    "    return False, None\n",
    "\n",
    "def fingerprint(flow):\n",
    "    h = hashlib.sha1(json.dumps({\n",
    "        \"table_id\": flow.get(\"table_id\", 0),\n",
    "        \"priority\": flow.get(\"priority\"),\n",
    "        \"match\": flow.get(\"match\"),\n",
    "        \"actions\": flow.get(\"actions\", [])\n",
    "    }, sort_keys=True).encode()).hexdigest()\n",
    "    return h[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2520,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATION_PROMPT_RYU = \"\"\"\n",
    "You are a meticulous network engineer. Convert the user's **intent** into a single Ryu (OpenFlow) flow rule\n",
    "expressed as a **JSON object only** (no comments, no code fences, no extra text).\n",
    "\n",
    "Return exactly one JSON object with this schema (keys in lower-case snake_case):\n",
    "\n",
    "{\n",
    "  \"dpid\": <int>,                  // datapath ID (switch ID). Example: 1\n",
    "  \"table_id\": <int>,              // default 0 unless the intent clearly states otherwise\n",
    "  \"priority\": <int>,              // guidance below\n",
    "  \"match\": {                      // match conditions\n",
    "      // Common keys (use only those needed by the intent):\n",
    "      \"in_port\": <int>,\n",
    "      \"eth_type\": <int>,          // IPv4=2048 (0x0800), IPv6=34525 (0x86DD)\n",
    "      \"ip_proto\": <int>,          // TCP=6, UDP=17, ICMPv4=1, ICMPv6=58\n",
    "      \"ipv4_src\": \"<ip[/mask]>\",\n",
    "      \"ipv4_dst\": \"<ip[/mask]>\",\n",
    "      \"ipv6_dst\": \"<ip6[/mask]>\",\n",
    "      \"tcp_src\": <int>,\n",
    "      \"tcp_dst\": <int>,\n",
    "      \"udp_src\": <int>,\n",
    "      \"udp_dst\": <int>,\n",
    "      \"icmpv4_type\": <int>        // e.g., 8 for echo request (ping)\n",
    "  },\n",
    "  \"actions\": [                    // ordered list of actions\n",
    "      // For forwarding use: {\"type\":\"OUTPUT\",\"port\":<int|IN_PORT|FLOOD|CONTROLLER|LOCAL>}\n",
    "      // For queue/QoS:      {\"type\":\"SET_QUEUE\",\"queue_id\":<int>}\n",
    "      // For VLAN ops:       {\"type\":\"PUSH_VLAN\",\"ethertype\":33024} (0x8100), \n",
    "      //                     {\"type\":\"SET_FIELD\",\"field\":\"vlan_vid\",\"value\":<int>}\n",
    "      // To drop:            []   (empty list)\n",
    "  ]\n",
    "}\n",
    "\n",
    "Rules of interpretation:\n",
    "- \"in switch N\" or \"on switch N\" → dpid=N. If not specified, infer the most reasonable dpid from context; otherwise omit only if impossible.\n",
    "- IPv4 traffic → include eth_type=2048. IPv6 traffic → include eth_type=34525.\n",
    "- ICMP/ICMPv6 ping (echo request) → ip_proto=1 (or 58 for v6) AND icmpv4_type=8 when clearly IPv4.\n",
    "- \"HTTP\" → TCP port 80; \"HTTPS\" → TCP port 443; \"DNS\" → UDP/TCP port 53 (use UDP unless otherwise stated).\n",
    "- \"send out/through port X\", \"via interface X\" → OUTPUT to port X.\n",
    "- \"block\", \"drop\", \"deny\" → actions must be an empty list [].\n",
    "- QoS/priority queues → include SET_QUEUE with the specific queue_id when requested.\n",
    "- table_id=0 unless the intent explicitly mentions another table.\n",
    "- Priority guideline (use the highest that fits the specificity):\n",
    "    300 → explicit block/deny rules\n",
    "    200 → highly specific forwarding with QoS/VLAN modifications\n",
    "    100 → specific L3/L4 matches (e.g., dst IP and TCP/UDP port)\n",
    "     50 → catch-all/less specific fall-through (e.g., only ip_proto or in_port)\n",
    "\n",
    "Constraints:\n",
    "- Output must be a single JSON object. No surrounding text, no markdown, no explanations.\n",
    "- Use **only** fields necessary to satisfy the intent; do not invent values.\n",
    "- Numeric fields may be written as integers (not strings). Use lowercase key names as shown.\n",
    "- If something is ambiguous, choose the most conservative, commonly-used OpenFlow interpretation for Ryu.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2521,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFLICT_PROMPT_RYU = \"\"\"\n",
    "You are an expert network engineer analyzing two Ryu (OpenFlow) flow rules to decide whether they **conflict**.\n",
    "\n",
    "### **Primary Goal**\n",
    "A conflict exists if the two rules apply to the **same switch (dpid)** and **same table_id**, their **matches overlap** (can match the same packet), AND at least one of the following is true:\n",
    "1.  Their **actions are different** (e.g., `OUTPUT:1` vs. `OUTPUT:2`, or `[]` vs. `OUTPUT:1`).\n",
    "2.  They are **redundant** (they overlap and have the **same action**).\n",
    "\n",
    "### **Guiding Principles**\n",
    "- If the `dpid` or `table_id` are different, they do **not** conflict.\n",
    "- If the matches are **mutually exclusive** (e.g., they match on different L2 protocols like `LLDP` vs. `IPv4`, or different ports like `in_port: 1` vs `in_port: 2`), they do **not** conflict.\n",
    "\n",
    "---\n",
    "### **Examples**\n",
    "\n",
    "**Example 1: Conflict (Different Action)**\n",
    "Flow 1: {\"dpid\": 1, \"priority\": 100, \"match\": {\"eth_type\": 2048}, \"actions\": [\"OUTPUT:1\"]}\n",
    "Flow 2: {\"dpid\": 1, \"priority\": 200, \"match\": {\"eth_type\": 2048, \"ip_proto\": 6}, \"actions\": [\"OUTPUT:2\"]}\n",
    "Output: {\n",
    "    \"conflict_status\": 1,\n",
    "    \"conflict_explanation\": \"Rules overlap (Flow 2 is a subset of Flow 1) but have different OUTPUT actions.\"\n",
    "}\n",
    "\n",
    "**Example 2: No Conflict (Mutual Exclusion)**\n",
    "Flow 1: {\"dpid\": 4, \"priority\": 100, \"match\": {\"eth_type\": 2048, \"ipv4_dst\": \"10.0.1.4\"}, \"actions\": [\"OUTPUT:4\"]}\n",
    "Flow 2: {\"dpid\": 4, \"priority\": 65535, \"match\": {\"dl_dst\": \"01:80:c2:00:00:00\"}, \"actions\": [\"OUTPUT:CONTROLLER\"]}\n",
    "Output: {\n",
    "    \"conflict_status\": 0,\n",
    "    \"conflict_explanation\": \"Matches are mutually exclusive. Flow 1 matches IPv4 packets (eth_type 2048), while Flow 2 matches a non-IPv4 L2 protocol (LLDP).\"\n",
    "}\n",
    "\n",
    "**Example 3: Conflict (Redundancy / Same Action)**\n",
    "Flow 1: {\"dpid\": 1, \"priority\": 100, \"match\": {\"eth_type\": 2048, \"ip_proto\": 6}, \"actions\": [\"OUTPUT:1\"]}\n",
    "Flow 2: {\"dpid\": 1, \"priority\": 90, \"match\": {\"eth_type\": 2048}, \"actions\": [\"OUTPUT:1\"]}\n",
    "Output: {\n",
    "    \"conflict_status\": 1,\n",
    "    \"conflict_explanation\": \"Rules are redundant. The matches overlap (Flow 1 is a subset of Flow 2) and they have the same action.\"\n",
    "}\n",
    "---\n",
    "\n",
    "### **Input Format**\n",
    "You will be provided with **two JSON flow rules** in the following format:\n",
    "\n",
    "**Flow 1:**\n",
    "<JSON for Flow 1>\n",
    "\n",
    "**Flow 2:**\n",
    "<JSON for Flow 2>\n",
    "\n",
    "### **Expected Output Format**\n",
    "Respond strictly in valid JSON format, using the schema below:\n",
    "\n",
    "{\n",
    "    \"conflict_status\": <integer>,\n",
    "    \"conflict_explanation\": \"<conflict explanation, if any>\"\n",
    "}\n",
    "\n",
    "Field Descriptions:\n",
    "   - 'conflict_status' should be 1 if a conflict exists, 0 otherwise.\n",
    "   - 'conflict_explanation' is a brief explanation if a conflict exists, otherwise an empty string \"\".\n",
    "\n",
    "NO EXTRA TEXT, COMMENTS, OR EXPLANATIONS OUTSIDE JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2522,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICING_PROMPT = \"\"\"You are tasked with analyzing a natural language intent to determine if it contains a command to create or use a queue/slice in an OpenFlow switch. You should respond in JSON format.\n",
    "\n",
    "### Rules for Interpretation:\n",
    "1. **Queue/Slice Detection:**  \n",
    "   - The intent is considered related to queue/slice if it contains commands such as:\n",
    "     - \"create queue\", \"create slices\", \"slice the network\", \"implement slicing\", \"slice the flow\", \"make flowspace slicing\", \"do slicing\", \"slice\", \"implement queue\", \"do queuing\", \"assign queue\", \"assign slice\", or any similar phrasing.\n",
    "   - If the intent does not mention creating or using a queue/slice, set the field `\"use_queue\"` to `0`.\n",
    "\n",
    "2. **Switch, Queue, and Port Identification (Data-plane port/interface):**  \n",
    "   - If the intent specifies a **switch ID** (e.g., \"switch 4\" or \"openflow:4\" or \"node 4\" or \"openflow 4\"), populate the `\"switch_id\"` field with its value.  \n",
    "   - If the intent specifies a **Queue ID or slice ID** (e.g., \"queue 4\" or \"4th queue\" or \"fourth queue\" or \"slice 1\" or \"first slice\"), populate the `\"queue_id\"` field with its value.  \n",
    "   - If the intent specifies a **port ID** referring to the device interface/output (e.g., \"port 2\" or \"interface 2\" or \"ethernet 2\" or \"output node connector 2\" or \"second port\" or \"second interface\"), populate the `\"port_id\"` field with its value. If there are multiple instances of \"port_id\" present, take the one which indicates the **output port or outgoing interface**.\n",
    "   - If the intent does not specify a switch ID or queue ID or port ID, set the respective field to an empty string (`\"\"`).\n",
    "\n",
    "3. **Traffic Type and L4 Destination Port Extraction (Protocol/Service):**\n",
    "   - Detect the **Layer-4 protocol** if mentioned: `\"tcp\"` or `\"udp\"`. Populate this in `\"traffic_type\"`. If not specified, set `\"traffic_type\"` to `\"\"`.\n",
    "   - Detect the **destination application port number** (Layer-4 port), if specified as a number (e.g., \"port 80\", \"UDP port 53\") or implied via a service reference such as \"HTTP (TCP port 80)\". Populate this number (as a string) in `\"l4_port\"`.  \n",
    "   - Do **not** confuse the L4 port (e.g., 80/53) with the device/interface port (e.g., switch port 2). The former goes to `\"l4_port\"`, the latter to `\"port_id\"`.\n",
    "   - If multiple L4 ports are mentioned, prefer the **destination/service port** used by the traffic selector (e.g., \"traffic destined for port 80\"). If still ambiguous, choose the first explicit destination/service port mentioned.\n",
    "   - If the L4 destination port is not specified, set `\"l4_port\"` to `\"\"`.\n",
    "\n",
    "4. **Negative Constraint**: Intents that only contain commands like \"block\", \"drop\", \"deny\", or \"forward\" without any explicit mention of \"queue\" or \"slice\" are not queue-related, and use_queue must be 0.\n",
    "\n",
    "5. **Output Format:**  \n",
    "   - Respond strictly in valid JSON format adhering to the following schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"use_queue\": <integer>,\n",
    "  \"switch_id\": \"<string>\",\n",
    "  \"queue_id\": \"<string>\",\n",
    "  \"port_id\": \"<string>\",\n",
    "  \"traffic_type\": \"<string>\",\n",
    "  \"l4_port\": \"<string>\"\n",
    "}\n",
    "\n",
    "Field Description:\n",
    "use_queue: 1 if the intent commands to create or use a queue/slice, 0 otherwise.\n",
    "switch_id: Switch ID if specified in the intent, otherwise \"\".\n",
    "queue_id: Queue/Slice ID if specified in the intent, otherwise \"\".\n",
    "port_id: Device/interface port if specified (output/outgoing preferred), otherwise \"\".\n",
    "traffic_type: \"tcp\" or \"udp\" if specified, otherwise \"\".\n",
    "l4_port: Destination application port number (e.g., \"80\", \"53\") if specified, otherwise \"\".\n",
    "\n",
    "No Additional Text:\n",
    "Do not include any comments, explanations, or outputs outside the JSON format.\n",
    "\n",
    "Example Inputs and Outputs:\n",
    "\n",
    "Input Intent:\n",
    "\"Create a queue in switch 4 on port 3 for slicing the flow.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"switch 4\",\n",
    "\"queue_id\": \"\",\n",
    "\"port_id\": \"port 3\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Send all video traffic through queue 0 of openflow:2.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"openflow 2\",\n",
    "\"queue_id\": \"0\",\n",
    "\"port_id\": \"\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Configure switch 5 for traffic management.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 0,\n",
    "\"switch_id\": \"switch 5\",\n",
    "\"queue_id\": \"\",\n",
    "\"port_id\": \"\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Monitor traffic flow on port 1.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 0,\n",
    "\"switch_id\": \"\",\n",
    "\"queue_id\": \"\",\n",
    "\"port_id\": \"port 1\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"In switch 3, if the incoming traffic in port 1 is TCP traffic destined for port 80, then pass it via interface 2, assigning it to queue 0 for prioritized handling.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"switch 3\",\n",
    "\"queue_id\": \"0\",\n",
    "\"port_id\": \"interface 2\",\n",
    "\"traffic_type\": \"tcp\",\n",
    "\"l4_port\": \"80\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"For node 1, route HTTP (TCP port 80) traffic targeting 10.0.0.3/32 through port 2 with traffic assigned to queue 0.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"node 1\",\n",
    "\"queue_id\": \"0\",\n",
    "\"port_id\": \"port 2\",\n",
    "\"traffic_type\": \"tcp\",\n",
    "\"l4_port\": \"80\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Forward UDP traffic on port 53 destined for 10.0.0.9 via interface 5 of switch 2, assigning it to queue 3.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"switch 2\",\n",
    "\"queue_id\": \"3\",\n",
    "\"port_id\": \"interface 5\",\n",
    "\"traffic_type\": \"udp\",\n",
    "\"l4_port\": \"53\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2523,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_models_translate_real = [\n",
    "\"codestral:22b\",\n",
    "\"command-r:35b\",\n",
    "\"huihui_ai/qwq-abliterated:latest\",\n",
    "]\n",
    "\n",
    "my_models_conflict_real = [  \n",
    "\"huihui_ai/qwq-fusion:latest\",\n",
    "\"qwq:latest\"\n",
    "]\n",
    "\n",
    "context_examples = [3, 6]\n",
    "\n",
    "default_model = \"llama2:7b\"\n",
    "\n",
    "ollama_embedding_url = \"http://localhost:11434\"\n",
    "ollama_server_url = \"http://localhost:11435\"  \n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=default_model,\n",
    "    base_url=ollama_embedding_url,\n",
    ")\n",
    "\n",
    "client = Client(host=ollama_server_url , timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom dataset from CSV\n",
    "custom_dataset = pd.read_csv('Intent2Flow-Ryu.csv')\n",
    "\n",
    "# Ensure proper column names and format\n",
    "if not {'instruction', 'output'}.issubset(custom_dataset.columns):\n",
    "    raise ValueError(\"The dataset must have 'instruction' and 'output' columns.\")\n",
    "\n",
    "# Split into train and test (50/50 split for example)\n",
    "#trainset, testset = train_test_split(custom_dataset, test_size=0.5, random_state=42, shuffle=True)\n",
    "trainset = custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2525,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_intent_to_store(\n",
    "    file_path,\n",
    "    nl_intent,\n",
    "    json_flow_rule,\n",
    "    device_id,\n",
    "    flow_id,\n",
    "    intent_type,\n",
    "    intent_specificity\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends an intent record to the IntentStore file in JSONL format.\n",
    "    \"\"\"\n",
    "    record = {\n",
    "        \"nl_intent\": nl_intent,\n",
    "        \"json_flow_rule\": json_flow_rule,\n",
    "        \"device_id\": device_id,\n",
    "        \"flow_id\": flow_id,\n",
    "        \"intent_type\": intent_type,\n",
    "        \"intent_specificity\": intent_specificity\n",
    "    }\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "def read_intents_from_store(file_path):\n",
    "    \"\"\"\n",
    "    Yields each intent record from the IntentStore file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            intent = json.loads(line)\n",
    "            yield intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LLM_conflict(existing_intent_flow_json, new_intent_flow_json):\n",
    "    system_prompt = CONFLICT_PROMPT_RYU\n",
    "\n",
    "    for model in my_models_conflict_real:\n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            try:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "                response = client.generate(model=model,\n",
    "                    options={'temperature': 0.3, 'num_ctx': 8192, 'top_p': 0.5, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                    stream=False,\n",
    "                    system=system_prompt,\n",
    "                    prompt=f\"Flow 1:\\n{json.dumps(existing_intent_flow_json, indent=2)}\\n\\nFlow 2:\\n{json.dumps(new_intent_flow_json, indent=2)}\",\n",
    "                    format='json'\n",
    "                )\n",
    "\n",
    "                output = response['response'].strip()\n",
    "\n",
    "                response_json = json.loads(output)\n",
    "\n",
    "                if 'conflict_status' not in response_json:\n",
    "                    #print(\"\\nWarning: 'conflict_status' key is missing in the response.\\n\")\n",
    "                    break\n",
    "                else:\n",
    "                    valid_conflict_response = True\n",
    "                    conflict_status = response_json.get('conflict_status', 0)\n",
    "                    # Ensure conflict_status is an integer\n",
    "                    if isinstance(conflict_status, str):\n",
    "                        conflict_status = int(conflict_status)\n",
    "\n",
    "                    return valid_conflict_response, conflict_status, response_json['conflict_explanation']             \n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Exception found: \", e)\n",
    "                sys.stdout.flush()\n",
    "                if(count<15):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"\\n\",model, \" failed to produce valid JSON for conflict info after 15 tries. Going to next model\\n\")\n",
    "                    break               \n",
    "    \n",
    "    return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conflict(device_id, new_intent_flow_json):\n",
    "\n",
    "    # existing_flows = get_flows_for_device_ONOS(device_id) # <--- OLD\n",
    "\n",
    "    # Use your new Ryu helper function from [cell 5]\n",
    "    # Note: get_flows returns a dict like {\"1\": [...flows...], \"2\": [...]}.\n",
    "    base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "    all_flows_dict = get_flows(base_url, device_id)\n",
    "    existing_flows = all_flows_dict.get(str(device_id), []) # <--- NEW\n",
    "\n",
    "    # The rest of this function should now work, as run_LLM_conflict\n",
    "    # will receive two Ryu-formatted JSON objects.\n",
    "    for existing_flow in existing_flows:\n",
    "\n",
    "        # --- START: ROBUST HARD-CODED EXCLUSION ---\n",
    "        try:\n",
    "            match1 = new_intent_flow_json.get(\"match\", {})\n",
    "            match2 = existing_flow.get(\"match\", {})\n",
    "\n",
    "            # 1. Check if a rule matches the reserved L2 control range (01:80:c2:00:00:0x)\n",
    "            dl_dst_1 = str(match1.get('dl_dst', ''))\n",
    "            dl_dst_2 = str(match2.get('dl_dst', ''))\n",
    "            \n",
    "            # This prefix precisely covers 00-0F\n",
    "            is_m1_l2_special = dl_dst_1.startswith('01:80:c2:00:00:0') \n",
    "            is_m2_l2_special = dl_dst_2.startswith('01:80:c2:00:00:0')\n",
    "\n",
    "            # 2. Check if a rule matches IPv4 or ARP\n",
    "            # (eth_type can be int 2048 or hex str 0x0800)\n",
    "            eth_type_1 = str(match1.get('eth_type', '')).lower()\n",
    "            eth_type_2 = str(match2.get('eth_type', '')).lower()\n",
    "            \n",
    "            is_m1_ip_arp = eth_type_1 in ['2048', '0x0800', '2054', '0x0806'] or 'ipv4_dst' in match1 or 'ipv4_src' in match1\n",
    "            is_m2_ip_arp = eth_type_2 in ['2048', '0x0800', '2054', '0x0806'] or 'ipv4_dst' in match2 or 'ipv4_src' in match2\n",
    "\n",
    "            # 3. If one matches L2 control and the other matches IP/ARP, they are exclusive\n",
    "            if (is_m1_l2_special and is_m2_ip_arp) or (is_m1_ip_arp and is_m2_l2_special):\n",
    "                continue # Skip this pair, they are mutually exclusive\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Pre-filter check failed: {e}\")\n",
    "            pass # Ignore errors and let the LLM handle it\n",
    "        # --- END: ROBUST HARD-CODED EXCLUSION ---\n",
    "\n",
    "        valid_conflict_response, conflict_status, conflict_details = run_LLM_conflict(existing_flow, new_intent_flow_json)\n",
    "\n",
    "        if (valid_conflict_response == False):\n",
    "            return 2, None, None\n",
    "        elif (conflict_status == 1):\n",
    "            return conflict_status, conflict_details, existing_flow\n",
    "\n",
    "    return 0, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LLM_IBN(intent, device_id):\n",
    "\n",
    "    for num_examples in context_examples:\n",
    "        for model in my_models_translate_real:\n",
    "            \n",
    "            example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "                [{\"instruction\": trainset.iloc[0][\"instruction\"], \"output\": trainset.iloc[0][\"output\"]}],\n",
    "                ollama_emb,\n",
    "                Chroma,\n",
    "                input_keys=[\"instruction\"],\n",
    "                k=num_examples,\n",
    "                vectorstore_kwargs={\"fetch_k\": min(num_examples, len(trainset))}\n",
    "                )\n",
    "            # Clear and add all remaining examples from the trainset\n",
    "            example_selector.vectorstore.reset_collection()\n",
    "            \n",
    "            for _, row in trainset.iterrows():\n",
    "                example_selector.add_example({\n",
    "                    \"instruction\": row[\"instruction\"],\n",
    "                    \"output\": row[\"output\"]\n",
    "                })\n",
    "            \n",
    "            system_prompt = TRANSLATION_PROMPT_RYU\n",
    "            count = 0\n",
    "\n",
    "            while True:\n",
    "                count+=1\n",
    "                try:\n",
    "                    time.sleep(0.1)\n",
    "                    if num_examples > 0:\n",
    "                        examples = example_selector.select_examples({\"instruction\": intent})\n",
    "                        example_str = \"\\n\\n\\n\".join(map(lambda x: \"Input: \" + x[\"instruction\"] + \"\\n\\nOutput: \" + x[\"output\"], examples))\n",
    "                        system_prompt += example_str + \"\\n\\n\\n\"  \n",
    "                    \n",
    "                    response = client.generate(model=model,\n",
    "                        options={'temperature': 0.6, 'num_ctx': 8192, 'top_p': 0.3, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                        #options={'device': 'cpu'},\n",
    "                        stream=False,\n",
    "                        system=system_prompt,\n",
    "                        prompt=intent,\n",
    "                        format='json'\n",
    "                    )\n",
    "                    actual_output = response['response']\n",
    "                    #print(\"\\nTranslated by: \", model)\n",
    "                    break\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Exception on Input: \", e)\n",
    "                    print(\"\\nCheck example_str same or not: \\n\",example_str)\n",
    "                    sys.stdout.flush()\n",
    "                    if(count<15):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"\\n\",model, \" failed to produce valid JSON for translation info after 15 tries. Going to next model\\n\")\n",
    "                        break \n",
    "            try:\n",
    "                \n",
    "                flow_json = json.loads(actual_output)\n",
    "                \n",
    "                #for flow in flow_json.get(\"flows\", []):  # Iterate over all flows\n",
    "                    #flow[\"deviceId\"] = device_id  # Replace the device ID\n",
    "\n",
    "                #print(json.dumps(flow_json))\n",
    "\n",
    "                return flow_json\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(\"Exception found: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LLM_Slice(intent):\n",
    "\n",
    "    system_prompt = SLICING_PROMPT\n",
    "    \n",
    "    for model in my_models_translate_real:     \n",
    "        try:\n",
    "            time.sleep(0.1)             \n",
    "            response = client.generate(model=model,\n",
    "                options={'temperature': 0.3, 'num_ctx': 8192, 'top_p': 0.5, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                #options={'device': 'cpu'},\n",
    "                stream=False,\n",
    "                system=system_prompt,\n",
    "                prompt=intent,\n",
    "                format='json'\n",
    "            )\n",
    "            \n",
    "            output = response['response'].strip()\n",
    "            response_json = json.loads(output)\n",
    "            \n",
    "            #print(\"\\nCheckpoint*******Exiting LLM Slicing detection\\n\\n******\")\n",
    "            return response_json            \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Exception found: \", e)\n",
    "            sys.stdout.flush()\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this function to [cell 5] or [cell 6]\n",
    "def extract_dpid_for_ryu(intent: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Extracts the integer datapath ID (dpid) for Ryu.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\b(?:switch|dpid|node|device)(?:_id)?\\s*(\\d+)', intent, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None # Or a default, like 1\n",
    "\n",
    "def get_iface_for_port(device_id: int, port_no: int | str) -> str:\n",
    "    \"\"\"\n",
    "    Builds the Mininet/OVS interface name (e.g., s4-eth1) from a\n",
    "    Ryu-style integer DPID (device_id) and port number.\n",
    "    \n",
    "    This is a Ryu-compatible replacement and does not query any controller.\n",
    "    \"\"\"\n",
    "    # This logic assumes your switch names are s1, s2, s3, etc.\n",
    "    # which is consistent with your get_switch_name_from_device_id function\n",
    "    switch_name = f\"s{device_id}\"\n",
    "    \n",
    "    return f\"{switch_name}-eth{port_no}\"\n",
    "\n",
    "def classify_ryu_flow_rule(flow_rule: dict):\n",
    "    \"\"\"\n",
    "    Classify a Ryu flow rule into a type and compute its specificity.\n",
    "    \n",
    "    This version correctly handles both:\n",
    "    - LLM-generated actions (list of dicts): [{\"type\": \"OUTPUT\", ...}]\n",
    "    - Ryu API actions (list of strings): [\"OUTPUT:1\"]\n",
    "    \n",
    "    Returns: (rule_type: str, specificity: float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Rule Type Detection ---\n",
    "    actions = flow_rule.get(\"actions\", [])\n",
    "    rule_type = \"unknown\" # Default\n",
    "\n",
    "    if not actions:\n",
    "        # An empty action list [] means \"drop\"\n",
    "        rule_type = \"security\" \n",
    "    else:\n",
    "        # --- START OF FIX ---\n",
    "        # Get all unique action types, handling both str and dict\n",
    "        action_types = set()\n",
    "        for action in actions:\n",
    "            if isinstance(action, dict):\n",
    "                # Handles LLM-generated format: {\"type\": \"OUTPUT\", \"port\": 1}\n",
    "                action_types.add(action.get(\"type\", \"\"))\n",
    "            elif isinstance(action, str):\n",
    "                # Handles Ryu API format: \"OUTPUT:1\" or \"OUTPUT:CONTROLLER\"\n",
    "                action_types.add(action.split(':', 1)[0])\n",
    "        # --- END OF FIX ---\n",
    "        \n",
    "        if \"SET_QUEUE\" in action_types:\n",
    "            rule_type = \"qos\"\n",
    "        elif \"OUTPUT\" in action_types or \"CONTROLLER\" in action_types:\n",
    "            # Any rule that forwards or sends to controller is \"forwarding\"\n",
    "            rule_type = \"forwarding\"\n",
    "        # Handle other action types if needed\n",
    "        elif \"GOTO_TABLE\" in action_types: \n",
    "             rule_type = \"forwarding\"\n",
    "        else:\n",
    "            # Other rules (e.g., SET_FIELD, PUSH_VLAN) also fall under forwarding/modification\n",
    "            rule_type = \"forwarding\"\n",
    "\n",
    "    # --- 2. Specificity Computation ---\n",
    "    specificity = 0.0\n",
    "    match = flow_rule.get(\"match\", {})\n",
    "    \n",
    "    if not match:\n",
    "        return rule_type, 0.0 # Empty match has zero specificity\n",
    "\n",
    "    for key, value in match.items():\n",
    "        specificity += 1.0 # Base point for the key existing\n",
    "        \n",
    "        if key in (\"ipv4_src\", \"ipv4_dst\", \"ipv6_src\", \"ipv6_dst\"):\n",
    "            try:\n",
    "                ip_net = ipaddress.ip_network(value, strict=False) \n",
    "                if \"v6\" in key:\n",
    "                    specificity += (ip_net.prefixlen / 128.0)\n",
    "                else:\n",
    "                    specificity += (ip_net.prefixlen / 32.0)\n",
    "            except Exception:\n",
    "                pass \n",
    "                \n",
    "    return rule_type, specificity\n",
    "\n",
    "def classify_ryu_flow_rule_old(flow_rule: dict):\n",
    "    \"\"\"\n",
    "    Classify a Ryu flow rule into a type and compute its specificity.\n",
    "    \n",
    "    Returns: (rule_type: str, specificity: float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Rule Type Detection ---\n",
    "    actions = flow_rule.get(\"actions\", [])\n",
    "    rule_type = \"unknown\" # Default\n",
    "\n",
    "    if not actions:\n",
    "        # An empty action list [] means \"drop\"\n",
    "        rule_type = \"security\" \n",
    "    else:\n",
    "        # Get all unique action types, e.g., {\"OUTPUT\", \"SET_QUEUE\"}\n",
    "        action_types = {action.get(\"type\", \"\") for action in actions}\n",
    "        \n",
    "        if \"SET_QUEUE\" in action_types:\n",
    "            rule_type = \"qos\"\n",
    "        elif \"OUTPUT\" in action_types or \"CONTROLLER\" in action_types:\n",
    "            # Any rule that forwards or sends to controller is \"forwarding\"\n",
    "            rule_type = \"forwarding\"\n",
    "        else:\n",
    "            # Other rules (e.g., SET_FIELD, PUSH_VLAN) also fall under forwarding/modification\n",
    "            rule_type = \"forwarding\"\n",
    "\n",
    "    # --- 2. Specificity Computation ---\n",
    "    specificity = 0.0\n",
    "    match = flow_rule.get(\"match\", {})\n",
    "    \n",
    "    if not match:\n",
    "        return rule_type, 0.0 # Empty match has zero specificity\n",
    "\n",
    "    for key, value in match.items():\n",
    "        specificity += 1.0 # Base point for the key existing\n",
    "        \n",
    "        # Add extra specificity for IP masks (just like your ONOS logic)\n",
    "        if key in (\"ipv4_src\", \"ipv4_dst\", \"ipv6_src\", \"ipv6_dst\"):\n",
    "            try:\n",
    "                # Value is like \"10.0.1.1/32\" or just \"10.0.1.1\"\n",
    "                # strict=False allows single IPs\n",
    "                ip_net = ipaddress.ip_network(value, strict=False) \n",
    "                \n",
    "                # Add fractional specificity based on mask length\n",
    "                if \"v6\" in key:\n",
    "                    specificity += (ip_net.prefixlen / 128.0)\n",
    "                else:\n",
    "                    specificity += (ip_net.prefixlen / 32.0)\n",
    "            except Exception:\n",
    "                # If parsing fails, just count the key (already done)\n",
    "                pass \n",
    "                \n",
    "    return rule_type, specificity\n",
    "\n",
    "def resolve_ryu_conflict(rule1, rule2):\n",
    "    \"\"\"\n",
    "    Resolve conflict between two Ryu rules using Type > Specificity > Priority.\n",
    "    Returns: winner_rule, loser_rule\n",
    "    \"\"\"\n",
    "    # This priority map is the core of your policy\n",
    "    type_priority = {\"security\": 3, \"qos\": 2, \"forwarding\": 1, \"unknown\": 0}\n",
    "\n",
    "    # Get type and specificity for both rules using our new classifier\n",
    "    type1, spec1 = classify_ryu_flow_rule(rule1)\n",
    "    type2, spec2 = classify_ryu_flow_rule(rule2)\n",
    "\n",
    "    # Get OpenFlow priorities (default to 0 if not present)\n",
    "    p1 = rule1.get(\"priority\", 0)\n",
    "    p2 = rule2.get(\"priority\", 0)\n",
    "\n",
    "    # --- Resolution Hierarchy ---\n",
    "    \n",
    "    # 1. Type-based resolution\n",
    "    type1_score = type_priority.get(type1, 0)\n",
    "    type2_score = type_priority.get(type2, 0)\n",
    "    \n",
    "    if type1_score > type2_score:\n",
    "        return rule1, rule2\n",
    "    elif type2_score > type1_score:\n",
    "        return rule2, rule1\n",
    "\n",
    "    # 2. Specificity-based resolution (if types are equal)\n",
    "    if spec1 > spec2:\n",
    "        return rule1, rule2\n",
    "    elif spec2 > spec1:\n",
    "        return rule2, rule1\n",
    "\n",
    "    # 3. Priority-based resolution (if type and specificity are equal)\n",
    "    if p1 > p2:\n",
    "        return rule1, rule2\n",
    "    elif p2 > p1:\n",
    "        return rule2, rule1\n",
    "\n",
    "    # All are equal - this is a true tie\n",
    "    return None, None\n",
    "\n",
    "def adjust_priority_ryu(winner_rule: dict, loser_rule: dict, step: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Adjusts the priority of the winning Ryu flow rule so it overrides the losing one.\n",
    "    This version caps the priority at 65535 to prevent overflow errors.\n",
    "    \"\"\"\n",
    "    loser_priority = loser_rule.get(\"priority\", 0)\n",
    "    winner_priority = winner_rule.get(\"priority\", 0)\n",
    "\n",
    "    # Set new priority to be at least 'step' higher than the loser\n",
    "    new_priority = max(winner_priority, loser_priority + step)\n",
    "    \n",
    "    # --- START OF FIX ---\n",
    "    # Cap the new priority at the maximum allowed OpenFlow value\n",
    "    if new_priority > 65535:\n",
    "        print(f\"Warning: Calculated priority ({new_priority}) exceeds max (65535). Capping at 65535.\")\n",
    "        new_priority = 65535\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    winner_rule[\"priority\"] = new_priority\n",
    "    return winner_rule\n",
    "\n",
    "\n",
    "def adjust_priority_ryu_old(winner_rule: dict, loser_rule: dict, step: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Adjusts the priority of the winning Ryu flow rule so it overrides the losing one.\n",
    "    This modifies winner_rule in-place and returns it.\n",
    "    \"\"\"\n",
    "    loser_priority = loser_rule.get(\"priority\", 0)\n",
    "    winner_priority = winner_rule.get(\"priority\", 0)\n",
    "\n",
    "    # Set new priority to be at least 'step' higher than the loser\n",
    "    new_priority = max(winner_priority, loser_priority + step)\n",
    "    winner_rule[\"priority\"] = new_priority\n",
    "\n",
    "    return winner_rule\n",
    "\n",
    "def extract_inner_flow(rule):\n",
    "    return rule[\"flows\"][0] if \"flows\" in rule else rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_IBN(intent):\n",
    "\n",
    "    #current_time = time.time()\n",
    "    device_id = extract_dpid_for_ryu(intent)\n",
    "\n",
    "    if device_id is None:\n",
    "        print(\"Error: Could not determine switch DPID from intent.\")\n",
    "        return False, None, None, None, None\n",
    "\n",
    "    global switch_id_for_llm_assurance \n",
    "    switch_id_for_llm_assurance = device_id\n",
    "\n",
    "    intent_JSON = run_LLM_IBN(intent, device_id)\n",
    "\n",
    "    intent_JSON[\"dpid\"] = device_id\n",
    "\n",
    "    conflict_status, conflict_details, which_flow_conflict = conflict(device_id, intent_JSON)\n",
    "\n",
    "    if((conflict_status != 1) and (conflict_status != 0)):\n",
    "        print(\"\\nCheck Conflict Detection Module, LLM did not produce a valid JSON for conflict detection.\\n\")\n",
    "        return False, None, None, None, None\n",
    "\n",
    "    elif (conflict_status == 1):\n",
    "\n",
    "        winner, non_winner = resolve_ryu_conflict(intent_JSON, which_flow_conflict)\n",
    "\n",
    "        winner_rule_inner = extract_inner_flow(winner)\n",
    "        existing_rule_inner = extract_inner_flow(which_flow_conflict)\n",
    "\n",
    "        if winner is None:\n",
    "            print(\"\\nConflict resolution resulted in a tie. The new rule and an existing rule has same type, specificity and priority\\n\")\n",
    "            print(\"\\nThe New flow rule:\\n\", intent_JSON, \"\\nThe existing flow rule: \\n\", which_flow_conflict)\n",
    "            print(\"\\nExisting Flow Rule Location: In switch: \",device_id, \"\\nConflict Details : \\n\",  conflict_details)\n",
    "            return False, \"Tie\", None, None, None\n",
    "        elif (winner_rule_inner == existing_rule_inner):\n",
    "            print(\"\\nExisting Flow Rule that Conflicts: \\n\", winner)\n",
    "            print(\"\\nThe New Flow Rule Attempted to Install based on the Given Intent: \\n\", non_winner)\n",
    "            print(\"\\nConflict Details : \\n\",  conflict_details)\n",
    "            return False, \"existing_rule_win\", None, None, None\n",
    "        else:\n",
    "            print(\"Conflict Resolved. Winner Flow Rule: \\n\", winner)\n",
    "            print(\"\\nShadowed Flow Rule: \\n\", non_winner)\n",
    "            print(\"\\nShadowed Flow Rule Location: In switch: \",device_id, \"\\nConflict Details : \\n\",  conflict_details)\n",
    "            # After resolving conflict and deciding rule1 is the winner:\n",
    "            updated_flow_json = adjust_priority_ryu(winner, non_winner) #argument order important; winner first.\n",
    "            intent_JSON = updated_flow_json   \n",
    "\n",
    "    # --- 4a. Ensure a Cookie ---\n",
    "    if \"cookie\" not in intent_JSON or intent_JSON[\"cookie\"] == 0:\n",
    "        # Generate a new, random 64-bit cookie\n",
    "        intent_JSON[\"cookie\"] = secrets.randbits(64) \n",
    "        print(f\"Assigned new tracking cookie: {intent_JSON['cookie']}\")\n",
    "        global gl_cookie\n",
    "        gl_cookie = intent_JSON['cookie']\n",
    "\n",
    "    try:\n",
    "        base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "        success = add_flow(base_url, intent_JSON) # <-- Ryu function from [cell 5]\n",
    "        if not success:\n",
    "            raise Exception(\"add_flow returned False\")\n",
    "        print(f\"Successfully pushed flow to dpid: {device_id}\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Exception found while installing flow rule: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            return False, None, None, None, None\n",
    "    \n",
    "    try:\n",
    "        base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "        cookie_to_check = intent_JSON.get(\"cookie\")\n",
    "        match_to_check = intent_JSON.get(\"match\", {})\n",
    "        prio_to_check = intent_JSON.get(\"priority\", 0)\n",
    "\n",
    "        time.sleep(0.5) # Give switch time to install\n",
    "        \n",
    "        # Use exists() from [cell 5]\n",
    "        verification_status, operational_flow_rule = exists(\n",
    "            base_url, \n",
    "            device_id, \n",
    "            match_to_check, \n",
    "            prio_to_check,\n",
    "            cookie=cookie_to_check \n",
    "        )\n",
    "        \n",
    "        if(verification_status == True):\n",
    "            # Get a unique ID for logging\n",
    "            flow_id = operational_flow_rule.get(\"cookie\", 0)\n",
    "            if flow_id == 0:\n",
    "                # If no cookie, fingerprint it\n",
    "                flow_id = fingerprint(operational_flow_rule) # fingerprint() is from [cell 5]\n",
    "\n",
    "            return True, flow_id, device_id, intent_JSON, operational_flow_rule\n",
    "        else:\n",
    "             print(\"Flow verification failed. Rule not found in switch.\")\n",
    "             return False, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Exception found while verifying flow rule: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            return False, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_switch_id(intent: str):\n",
    "    \"\"\"\n",
    "    Extract the switch ID from a natural language intent.\n",
    "    \n",
    "    Parameters:\n",
    "        intent (str): The natural language intent.\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted switch ID (e.g., 'openflow:1') or None if not found.\n",
    "    \"\"\"\n",
    "    # Mapping of ordinal words to numeric values\n",
    "    ordinals = {\n",
    "        \"first\": 1,\n",
    "        \"second\": 2,\n",
    "        \"third\": 3,\n",
    "        \"fourth\": 4,\n",
    "        \"fifth\": 5,\n",
    "        \"sixth\": 6,\n",
    "        \"seventh\": 7,\n",
    "        \"eighth\": 8,\n",
    "        \"ninth\": 9,\n",
    "        \"tenth\": 10\n",
    "    }\n",
    "\n",
    "    # Match patterns like 'openflow:1'\n",
    "    match = re.search(r'openflow[:\\s](\\d+)', intent, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"openflow:{match.group(1)}\"\n",
    "\n",
    "    # Match patterns like 'switch 1', 'router 2', 'node 3'\n",
    "    match = re.search(r'\\b(?:switch|router|node|device)(?:\\s*number)?\\s*(\\d+)', intent, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f\"openflow:{match.group(1)}\"\n",
    "\n",
    "    # Match ordinal words (e.g., 'fourth switch', 'second router')\n",
    "    match = re.search(r'\\b(?:switch|router|node|device)\\s*(\\w+)', intent, re.IGNORECASE)\n",
    "    if match:\n",
    "        ordinal_word = match.group(1).lower()\n",
    "        if ordinal_word in ordinals:\n",
    "            return f\"openflow:{ordinals[ordinal_word]}\"\n",
    "\n",
    "    # Match standalone ordinal words (e.g., 'fourth' without 'switch')\n",
    "    for word, number in ordinals.items():\n",
    "        if word in intent.lower():\n",
    "            return f\"openflow:{number}\"\n",
    "\n",
    "    return None\n",
    "\n",
    "def execute_command(command):\n",
    "    \"\"\"\n",
    "    Runs a command with sudo password automation.\n",
    "    \"\"\"\n",
    "    full_command = f\"echo {sudo_password} | sudo -S {command}\"\n",
    "    try:\n",
    "        result = subprocess.run(full_command, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            return result.stdout.strip()\n",
    "        else:\n",
    "            raise Exception(f\"Error executing command: {result.stderr.strip()}\")\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def get_switch_port_mapping():\n",
    "    try:\n",
    "        # Commands to list port and QoS configurations\n",
    "        list_ports_command = \"sudo -S ovs-vsctl list port\"\n",
    "        list_qos_command = \"sudo -S ovs-vsctl list qos\"\n",
    "        # Fetch port and QoS data\n",
    "        ports_output = execute_command(list_ports_command)\n",
    "        qos_output = execute_command(list_qos_command)\n",
    "\n",
    "        # Parse QoS data into a dictionary\n",
    "        qos_mapping = {}\n",
    "        current_qos = None\n",
    "        for line in qos_output.splitlines():\n",
    "            if line.startswith(\"_uuid\"):\n",
    "                current_qos = line.split(\":\")[1].strip()\n",
    "            elif line.startswith(\"queues\") and current_qos:\n",
    "                qos_mapping[current_qos] = line.split(\":\")[1].strip()\n",
    "\n",
    "        # Create a dictionary to store switch-to-port mapping\n",
    "        switch_port_dict = {}\n",
    "\n",
    "        # Parse ports data and check for QoS\n",
    "        current_port = None\n",
    "        for line in ports_output.splitlines():\n",
    "            if line.startswith(\"name\"):\n",
    "                current_port = line.split(\":\")[1].strip()\n",
    "            elif line.startswith(\"qos\") and \"[]\" not in line and current_port:\n",
    "                qos_uuid = line.split(\":\")[1].strip()\n",
    "\n",
    "                # Extract the OpenFlow switch ID and port number\n",
    "                if \"-\" in current_port:\n",
    "                    switch, port = current_port.split(\"-\")\n",
    "                    switch_id = f\"openflow:{switch[1:]}\"  # e.g., \"s1\" -> \"openflow:1\"\n",
    "                    port_number = port[3:]  # e.g., \"eth2\" -> \"2\"\n",
    "\n",
    "                    # Add to dictionary\n",
    "                    if switch_id not in switch_port_dict:\n",
    "                        switch_port_dict[switch_id] = []\n",
    "                    switch_port_dict[switch_id].append(port_number)\n",
    "\n",
    "                current_port = None\n",
    "\n",
    "        return switch_port_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return {}\n",
    "    \n",
    "def extract_port_number(text: str):\n",
    "    \"\"\"\n",
    "    Extract the Ethernet port number from a natural language text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text containing the port reference.\n",
    "    \n",
    "    Returns:\n",
    "        int: Extracted port number or None if not found.\n",
    "    \"\"\"\n",
    "    # Mapping of ordinal words to numeric values\n",
    "    ordinals = {\n",
    "        \"first\": 1,\n",
    "        \"second\": 2,\n",
    "        \"third\": 3,\n",
    "        \"fourth\": 4,\n",
    "        \"fifth\": 5,\n",
    "        \"sixth\": 6,\n",
    "        \"seventh\": 7,\n",
    "        \"eighth\": 8,\n",
    "        \"ninth\": 9,\n",
    "        \"tenth\": 10\n",
    "    }\n",
    "\n",
    "    # Match explicit numbers after keywords\n",
    "    match = re.search(r'\\b(?:port|interface|output\\s+node\\s+connector|ethernet)\\s*(\\d+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "\n",
    "    # Match ordinal words (e.g., 'second port', 'third interface')\n",
    "    match = re.search(r'\\b(?:port|interface|output\\s+node\\s+connector|ethernet)\\s*(\\w+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        ordinal_word = match.group(1).lower()\n",
    "        if ordinal_word in ordinals:\n",
    "            return ordinals[ordinal_word]\n",
    "\n",
    "    # Match standalone ordinal words (e.g., 'second')\n",
    "    for word, number in ordinals.items():\n",
    "        if word in text.lower():\n",
    "            return number\n",
    "\n",
    "    return None\n",
    "\n",
    "def ovs_port_exists(switch_name, port_name, sudo_password=sudo_password):\n",
    "    cmd = f\"sudo -S ovs-vsctl list-ports {switch_name}\"\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, input=sudo_password + \"\\n\")\n",
    "    ports = result.stdout.split()\n",
    "    return port_name in ports\n",
    "\n",
    "def get_switch_name_from_device_id(dpid: int):\n",
    "    # dpid is now a simple integer, e.g., 2\n",
    "    return f\"s{dpid}\"\n",
    "\n",
    "def create_two_queue_for_switch(device_id, port, max_rate=10000000, queue_configs=None):\n",
    "    \"\"\"\n",
    "    Creates queues dynamically for a specific switch and port.\n",
    "    \n",
    "    Parameters:\n",
    "        switch (str): The name of the switch in 'openflow:X' format (e.g., 'openflow:4').\n",
    "        port (int): The port number on the switch (e.g., 2).\n",
    "        max_rate (int): Maximum rate for the QoS (default is 10000000).\n",
    "        queue_configs (list): List of tuples specifying min-rate and max-rate for each queue (default is 2 queues).\n",
    "    \"\"\"\n",
    "    if queue_configs is None:\n",
    "        # Default to 2 queues with these configurations\n",
    "        queue_configs = [\n",
    "            (6000000, 6000000),  # Queue 0: min-rate and max-rate\n",
    "            (4000000, 4000000)   # Queue 1: min-rate and max-rate\n",
    "        ]\n",
    "\n",
    "    # Construct the port name from the input\n",
    "    #port_name = f\"{switch.replace('openflow:', 's')}-eth{port}\"\n",
    "    switch_name = get_switch_name_from_device_id(device_id)\n",
    "    port_name = f\"{switch_name}-eth{port}\"\n",
    "\n",
    "    if not ovs_port_exists(switch_name, port_name):\n",
    "        print(f\"Port {port_name} does not exist on bridge {switch_name}!\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # Construct the QoS command for the specific switch and port\n",
    "    qos_command = f\"sudo -S ovs-vsctl -- set port {port_name} qos=@newqos -- --id=@newqos create qos type=linux-htb other-config:max-rate={max_rate}\"\n",
    "    for i, (min_rate, max_rate) in enumerate(queue_configs):\n",
    "        qos_command += f\" queues:{i}=@q{i}\"\n",
    "    for i, (min_rate, max_rate) in enumerate(queue_configs):\n",
    "        qos_command += f\" -- --id=@q{i} create queue other-config:min-rate={min_rate} other-config:max-rate={max_rate}\"\n",
    "\n",
    "    # Execute the command\n",
    "    print(f\"Running: {qos_command}\")\n",
    "    process = subprocess.Popen(qos_command, shell=True, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    stdout, stderr = process.communicate(input=f\"{sudo_password}\\n\")\n",
    "    if process.returncode == 0:\n",
    "        print(f\"Success:\\n{stdout}\")\n",
    "    else:\n",
    "        print(f\"Error:\\n{stderr}\")\n",
    "\n",
    "\n",
    "def create_two_queue_for_switch_handler(slicing_info):\n",
    "\n",
    "    if 'use_queue' in slicing_info:         \n",
    "            slicing_status = slicing_info['use_queue']\n",
    "            slicing_switch_id = slicing_info['switch_id']\n",
    "            slicing_queue_id = slicing_info['queue_id']\n",
    "            slicing_port_id = slicing_info['port_id']\n",
    "\n",
    "            if(slicing_status == 1):\n",
    "                #openflow_id = extract_switch_id(slicing_switch_id)\n",
    "                dpid = extract_dpid_for_ryu(slicing_switch_id)\n",
    "                if dpid is None:\n",
    "                    print(\"Error: Could not extract DPID for queue creation.\")\n",
    "                    return\n",
    "                \n",
    "                openflow_id_str = f\"openflow:{dpid}\"\n",
    "\n",
    "                switch_port_mapping = get_switch_port_mapping()\n",
    "\n",
    "                print(\"\\nCheckpoint*******Entering Slice/Queue Management\\n\\n******\")\n",
    "\n",
    "                port_number = extract_port_number(slicing_port_id)\n",
    "                \n",
    "                if openflow_id_str not in switch_port_mapping :\n",
    "                    \n",
    "                        print(\"\\n\\nQueue was not installed in \",openflow_id, \"\\nInstalling now on interface: \", port_number,\"\\n\")\n",
    "                        print(\"\\nCheckpoint*******Entering queue creation\\n\\n******\")\n",
    "\n",
    "                        create_two_queue_for_switch(\n",
    "                            device_id=dpid,  port=port_number,\n",
    "                            queue_configs=[\n",
    "                                (6000000, 6000000),  # Queue 0\n",
    "                                (4000000, 4000000)   # Queue 1\n",
    "                            ]\n",
    "                            )   \n",
    "                else:\n",
    "                    if str(port_number) not in switch_port_mapping[openflow_id_str]:\n",
    "\n",
    "                        print(\"\\n\\nQueue was not installed in \",openflow_id_str, \" interface: \", port_number, \"\\nInstalling now...\\n\")\n",
    "                        print(\"\\nCheckpoint*******Entering queue creation\\n\\n******\")\n",
    "\n",
    "                        create_two_queue_for_switch(\n",
    "                            device_id=dpid,  port=port_number,\n",
    "                            queue_configs=[\n",
    "                                (6000000, 6000000),  # Queue 0\n",
    "                                (4000000, 4000000)   # Queue 1\n",
    "                            ]\n",
    "                            )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corrective_action_prompt_ryu(intent_nl, operational_flow_rule, dpid, ping_count,\n",
    "                                          candidate_src_ip, candidate_dst_ip, ping_output):\n",
    "    \"\"\"\n",
    "    Generates a prompt for the LLM to suggest corrective actions for a FAILED security intent.\n",
    "    This version is adapted for Ryu's flow format.\n",
    "    \"\"\"\n",
    "    # Get existing flows from the switch using our Ryu helper\n",
    "    base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "    existing_flows_dict = get_flows(base_url, dpid)\n",
    "    existing_flows = existing_flows_dict.get(str(dpid), [])\n",
    "\n",
    "    prompt_sections = []\n",
    "    prompt_sections.append(\n",
    "        \"You are an SDN network assistant. Your task is to recommend a ranked list of corrective actions \"\n",
    "        \"to enforce a **security intent** that failed during assurance testing.\"\n",
    "    )\n",
    "    prompt_sections.append(f\"1. **Security Intent (in Natural Language)**:\\n{intent_nl}\")\n",
    "\n",
    "    # Use Ryu's flow format\n",
    "    prompt_sections.append(\"2. **Ryu Flow Rule for the Security Intent (Operational in Switch)**:\")\n",
    "    prompt_sections.append(json.dumps(operational_flow_rule, indent=2))\n",
    "\n",
    "    prompt_sections.append(\"3. **Existing Flow Rules in the Same Switch (DPID)**:\")\n",
    "    prompt_sections.append(json.dumps(existing_flows, separators=(\",\", \":\")))\n",
    "\n",
    "    prompt_sections.append(\n",
    "        f\"\"\"4. **Assurance Test Result**:\n",
    "        - Ping Source IP: {candidate_src_ip}\n",
    "        - Ping Destination IP: {candidate_dst_ip}\n",
    "        - Ping Count: {ping_count}\n",
    "        - Ping Output:\n",
    "        {ping_output}\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Instructions updated for Ryu's JSON structure\n",
    "    prompt_sections.append(\n",
    "        \"---\\nNow, based on the above data, generate a ranked list of corrective actions. \"\n",
    "        \"You must use only the following predefined corrective actions:\\n\"\n",
    "        \"1. Correct Match Fields\\n\"\n",
    "        \"2. Increase Priority\\n\"\n",
    "        \"3. Fix Action Field\\n\\n\"\n",
    "        \"For each action:\\n\"\n",
    "        \"- **For 'Correct Match Fields'**: Specify *exactly* which key-value pairs should be set in the 'match' object (e.g., \\\"ipv4_src\\\": \\\"10.0.1.1\\\", \\\"eth_type\\\": 2048), and which keys (if any) should be removed from the 'match' object.\\n\"\n",
    "        \"- **For 'Increase Priority'**: Indicate which existing rule(s) (by cookie or match) are overshadowing the candidate rule, their current priority value(s), and the exact priority value the candidate rule should be set to.\\n\"\n",
    "        \"- **For 'Fix Action Field'**: Only include this if the 'actions' list is NOT empty ([]). The suggestion should be to set 'actions' to [].\\n\\n\"\n",
    "        \"Rank the actions and explain your reasoning.\\n\"\n",
    "        \"Return your answer ONLY in the following strict JSON format:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"recommended_actions\\\": [\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"rank\\\": 1,\\n\"\n",
    "        \"      \\\"action\\\": \\\"Correct Match Fields\\\",\\n\"\n",
    "        \"      \\\"suggestion\\\": {\\n\"\n",
    "        \"        \\\"set_fields\\\": {\\\"ipv4_src\\\": \\\"10.0.0.1\\\", \\\"ipv4_dst\\\": \\\"10.0.0.2\\\"},\\n\"\n",
    "        \"        \\\"remove_fields\\\": [\\\"tcp_dst\\\"],\\n\"\n",
    "        \"        \\\"reasoning\\\": \\\"The match fields do not match the intended source and destination.\\\"\\n\"\n",
    "        \"      }\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"rank\\\": 2,\\n\"\n",
    "        \"      \\\"action\\\": \\\"Increase Priority\\\",\\n\"\n",
    "        \"      \\\"suggestion\\\": {\\n\"\n",
    "        \"        \\\"conflicting_rules\\\": [ {\\\"cookie\\\": \\\"123...\\\", \\\"priority\\\": 400} ],\\n\"\n",
    "        \"        \\\"recommended_priority\\\": 410,\\n\"\n",
    "        \"        \\\"reasoning\\\": \\\"The candidate rule is overshadowed by a rule with higher priority.\\\"\\n\"\n",
    "        \"      }\\n\"\n",
    "        \"    }\\n\"\n",
    "        \"  ]\\n\"\n",
    "        \"}\\n\"\n",
    "        \"Omit any action that is not relevant.\"\n",
    "    )\n",
    "    return \"\\n\\n\".join(prompt_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW [cell 21] ---\n",
    "def Run_assurance_LLM (assurance_prompt):\n",
    "    \"\"\"\n",
    "    (This function is controller-agnostic and does not need changes.)\n",
    "    Sends the prompt to the LLM and gets a JSON response.\n",
    "    \"\"\"\n",
    "    for model in my_models_conflict_real:    \n",
    "        try:\n",
    "            time.sleep(0.1)             \n",
    "            response = client.generate(model=model,\n",
    "                options={'temperature': 0.3, 'num_ctx': 8192, 'top_p': 0.5, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                stream=False,\n",
    "                system=\"\",\n",
    "                prompt=assurance_prompt,\n",
    "                format='json'\n",
    "            )\n",
    "            output = response['response'].strip()\n",
    "            response_json = json.loads(output)\n",
    "            return response_json            \n",
    "        except Exception as e:\n",
    "            print(f\"Exception found at assurance LLM for corrective action generation: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "    return None # Return None if all models fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_command_full(command, timeout=15, with_sudo=False):\n",
    "    # Run the command as-is or under sudo (single layer)\n",
    "    cmd = command\n",
    "    if with_sudo:\n",
    "        # -S: read password from stdin; -p '' suppresses the prompt text\n",
    "        cmd = f\"sudo -S -p '' {command}\"\n",
    "        cmd = f\"printf '%s\\n' {shlex.quote(sudo_password)} | {cmd}\"\n",
    "    try:\n",
    "        res = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n",
    "        return res.stdout, res.stderr\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        print(f\"Command timed out: {command}\")\n",
    "        stdout_str = e.stdout if isinstance(e.stdout, str) else (e.stdout.decode('utf-8', 'ignore') if e.stdout else \"\")\n",
    "        stderr_str = e.stderr if isinstance(e.stderr, str) else (e.stderr.decode('utf-8', 'ignore') if e.stderr else \"Timeout\")\n",
    "        return stdout_str, stderr_str\n",
    "\n",
    "\n",
    "def execute_command_full_old(command, sudo_password=sudo_password, timeout=15):\n",
    "    \"\"\"\n",
    "    Runs a shell command with sudo, providing the password.\n",
    "    Returns TUPLE: (stdout_str, stderr_str)\n",
    "    \"\"\"\n",
    "    cmd = f\"echo {shlex.quote(sudo_password)} | sudo -S {command}\"\n",
    "    try:\n",
    "        # text=True decodes stdout/stderr as strings on success\n",
    "        result = subprocess.run(cmd, shell=True, \n",
    "                                capture_output=True, text=True, \n",
    "                                timeout=timeout)\n",
    "        return result.stdout, result.stderr\n",
    "    \n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        print(f\"Command timed out: {command}\")\n",
    "        # --- FIX ---\n",
    "        # e.stdout and e.stderr are BYTES, so we must decode them to STR\n",
    "        stdout_str = e.stdout.decode('utf-8', errors='ignore') if e.stdout else \"\"\n",
    "        stderr_str = e.stderr.decode('utf-8', errors='ignore') if e.stderr else \"Timeout\"\n",
    "        return stdout_str, stderr_str\n",
    "        # --- END FIX ---\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in execute_command_full: {e}\")\n",
    "        return \"\", str(e)\n",
    "\n",
    "def get_mininet_host_pid(src_host):\n",
    "    \"\"\"\n",
    "    Robustly get the PID of a Mininet host process (e.g., 'h1' or 'h1onos') regardless of user.\n",
    "    Looks for a process with command containing 'mininet:<src_host>'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ps_output = subprocess.check_output([\"ps\", \"-eo\", \"pid,args\"], text=True).strip().splitlines()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(\"Failed to run 'ps -eo pid,args'\") from e\n",
    "\n",
    "    for line in ps_output:\n",
    "        if f\"mininet:{src_host}\" in line and \"grep\" not in line:\n",
    "            parts = line.strip().split(None, 1)\n",
    "            if len(parts) == 2:\n",
    "                pid_str, cmd = parts\n",
    "                try:\n",
    "                    pid = int(pid_str)\n",
    "                    #print(f\"[DEBUG] Matched host '{src_host}' → PID {pid}\")\n",
    "                    return pid\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "    raise RuntimeError(f\"No Mininet host process found for '{src_host}'.\")\n",
    "\n",
    "# --- NEW [cell 22] ---\n",
    "\n",
    "def extract_host_and_ip_ryu(flow_data: dict):\n",
    "    \"\"\"\n",
    "    Extracts source and destination IPs from a Ryu flow rule.\n",
    "    Returns: (src_host, dst_host, src_ip, dst_ip)\n",
    "    \"\"\"\n",
    "    src_ip = dst_ip = None\n",
    "    match = flow_data.get(\"match\", {})\n",
    "\n",
    "    # Extract IPs, stripping any /mask\n",
    "    if \"ipv4_src\" in match:\n",
    "        src_ip = match[\"ipv4_src\"].split(\"/\")[0]\n",
    "    if \"ipv4_dst\" in match:\n",
    "        dst_ip = match[\"ipv4_dst\"].split(\"/\")[0]\n",
    "\n",
    "    # Use the ip_to_host map from [cell 3]\n",
    "    src_host = ip_to_host.get(src_ip) if src_ip else None\n",
    "    dst_host = ip_to_host.get(dst_ip) if dst_ip else None\n",
    "\n",
    "    return src_host, dst_host, src_ip, dst_ip\n",
    "\n",
    "#def ryu_assurance_for_security_intent(src_ip, dst_ip, ping_count=2):\n",
    "def ryu_assurance_for_security_intent(src_ip, dst_ip, rule_dpid, ping_count=2):\n",
    "    \"\"\"\n",
    "    (This function is mostly controller-agnostic, as it tests the data plane.)\n",
    "    (The 'get_mininet_host_pid' and 'execute_command_full' helpers are assumed to be in [cell 21] or [cell 22])\n",
    "    \"\"\"\n",
    "    all_ips = [\"10.0.1.1\", \"10.0.1.2\", \"10.0.1.3\", \"10.0.1.4\"]\n",
    "    global llm_caller_flag\n",
    "    llm_caller_flag = 0\n",
    "\n",
    "    def perform_ping(source_ip, target_ip):\n",
    "        source_host = ip_to_host.get(source_ip)\n",
    "        if not source_host:\n",
    "            print(f\"[WARNING] Unknown source host for IP: {source_ip}\")\n",
    "            return \"\"\n",
    "        try:\n",
    "            host_pid = get_mininet_host_pid(source_host)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Cannot get PID for host {source_host}: {e}\")\n",
    "            return \"\"\n",
    "        ping_cmd = f\"mnexec -a {host_pid} ping -c {ping_count} {target_ip}\"\n",
    "        out, err = execute_command_full(ping_cmd, with_sudo=True)\n",
    "        return f\"{out or ''}{err or ''}\"\n",
    "\n",
    "    # def perform_ping(source_ip, target_ip):\n",
    "    #     source_host = ip_to_host.get(source_ip)\n",
    "    #     if not source_host:\n",
    "    #         print(f\"[WARNING] Unknown source host for IP: {source_ip}\")\n",
    "    #         return None\n",
    "    #     try:\n",
    "    #         host_pid = get_mininet_host_pid(source_host)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"[ERROR] Cannot get PID for host {source_host}: {e}\")\n",
    "    #         return None\n",
    "    #     ping_cmd = f\"echo {sudo_password} | sudo -S mnexec -a {host_pid} ping -c {ping_count} {target_ip}\"\n",
    "    #     #return execute_command_full(ping_cmd, timeout=15)\n",
    "    #     return execute_command_full(ping_cmd, with_sudo=True)\n",
    "\n",
    "\n",
    "    # (The rest of the ping logic from your original cell 22 is correct)\n",
    "    # Case 1: src and dst IP both present\n",
    "    if src_ip and dst_ip:\n",
    "        print(f\"[INFO] Testing {src_ip} to {dst_ip}\")\n",
    "        output = perform_ping(src_ip, dst_ip)\n",
    "        if output and \"100% packet loss\" in output:\n",
    "            print(f\"[PASS] Intent effective. Traffic from {src_ip} to {dst_ip} is blocked.\")\n",
    "        elif output and \"0% packet loss\" in output:\n",
    "            print(f\"[FAIL] Intent NOT effective. Traffic from {src_ip} to {dst_ip} is NOT blocked.\")\n",
    "            llm_caller_flag = 1\n",
    "        else:\n",
    "            print(\"[WARN] Inconclusive result. Ping output:\\n\", output, \"\\nPlease check first why ping is not working.\")\n",
    "            llm_caller_flag = 2\n",
    "        return ping_count, src_ip, dst_ip, output\n",
    "    \n",
    "    # Case 1: src and dst IP both present\n",
    "    if src_ip and dst_ip:\n",
    "        print(f\"[INFO] Testing {src_ip} to {dst_ip}\")\n",
    "        output = perform_ping(src_ip, dst_ip)\n",
    "        if output and \"100% packet loss\" in output:\n",
    "            print(f\"[PASS] Intent effective. Traffic from {src_ip} to {dst_ip} is blocked.\")\n",
    "        elif output and \"0% packet loss\" in output:\n",
    "            print(f\"[FAIL] Intent NOT effective. Traffic from {src_ip} to {dst_ip} is NOT blocked.\")\n",
    "            llm_caller_flag = 1\n",
    "        else:\n",
    "            print(\"[WARN] Inconclusive result. Ping output:\\n\", output, \"\\nPlease check first why ping is not working.\")\n",
    "            llm_caller_flag = 2\n",
    "        return ping_count, src_ip, dst_ip, output\n",
    "\n",
    "    # Case 2: src_ip is None → test all src IPs → dst_ip\n",
    "    elif src_ip is None and dst_ip:\n",
    "        print(f\"[INFO] Testing multiple sources to {dst_ip}\")\n",
    "        for candidate_src in all_ips:\n",
    "            if candidate_src == dst_ip:\n",
    "                continue\n",
    "            output = perform_ping(candidate_src, dst_ip)\n",
    "            if output and \"100% packet loss\" in output:\n",
    "                continue  # this one is blocked, good\n",
    "            elif output and \"0% packet loss\" in output:\n",
    "                print(f\"[FAIL] Intent NOT effective. {candidate_src} → {dst_ip} was NOT blocked.\")\n",
    "                llm_caller_flag = 1\n",
    "                return ping_count, candidate_src, dst_ip, output\n",
    "            else:\n",
    "                print(f\"[WARN] Inconclusive result from {candidate_src} → {dst_ip}:\\n{output}\", \"\\nPlease check first why ping is not working.\")\n",
    "                llm_caller_flag = 2\n",
    "                return ping_count, candidate_src, dst_ip, output\n",
    "        print(f\"[PASS] Intent effective. All sources blocked from reaching {dst_ip}.\")\n",
    "        return ping_count, candidate_src, dst_ip, output\n",
    "\n",
    "    # Case 3: dst_ip is None → test src_ip → all destinations\n",
    "    elif dst_ip is None and src_ip:\n",
    "        print(f\"[INFO] Testing {src_ip} for multiple destinations\")\n",
    "        for candidate_dst in all_ips:\n",
    "            if candidate_dst == src_ip:\n",
    "                continue\n",
    "\n",
    "            # --- START NEW LOGIC ---\n",
    "            # (Make sure HOST_ATTACH and SW_OF from cell 2023 are accessible)\n",
    "            try:\n",
    "                src_attach_dev_str, _ = HOST_ATTACH.get(src_ip, (None, None))\n",
    "                dst_attach_dev_str, _ = HOST_ATTACH.get(candidate_dst, (None, None))\n",
    "                \n",
    "                # Convert the integer rule_dpid (e.g., 2) to its 'of:...' string\n",
    "                rule_dev_str = SW_OF.get(str(rule_dpid))\n",
    "\n",
    "                # If hosts are on the same switch, AND that switch is NOT where the rule is,\n",
    "                # this path is irrelevant. Skip the test.\n",
    "                if (src_attach_dev_str and \n",
    "                    src_attach_dev_str == dst_attach_dev_str and \n",
    "                    src_attach_dev_str != rule_dev_str):\n",
    "                    \n",
    "                    print(f\"[INFO] Skipping test {src_ip} -> {candidate_dst} (local on {src_attach_dev_str}, rule is on {rule_dev_str})\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Topology check failed: {e}. Proceeding with test.\")\n",
    "            # --- END NEW LOGIC ---\n",
    "\n",
    "            output = perform_ping(src_ip, candidate_dst)\n",
    "            if output and \"100% packet loss\" in output:\n",
    "                continue  # this one is blocked, good\n",
    "            elif output and \"0% packet loss\" in output:\n",
    "                print(f\"[FAIL] Intent NOT effective. {src_ip} → {candidate_dst} was NOT blocked.\")\n",
    "                llm_caller_flag = 1\n",
    "                return ping_count, src_ip, candidate_dst, output\n",
    "            else:\n",
    "                print(f\"[WARN] Inconclusive result from {src_ip} → {candidate_dst}:\\n{output}\", \"\\nPlease check first why ping is not working.\")\n",
    "                llm_caller_flag = 2\n",
    "                return ping_count, src_ip, candidate_dst, output\n",
    "        print(f\"[PASS] Intent effective.\")\n",
    "        return ping_count, src_ip, candidate_dst, output\n",
    "    \n",
    "    else:\n",
    "        print(\"[ERROR] Both source and destination IPs are missing. Cannot evaluate intent.\")\n",
    "        llm_caller_flag = 2\n",
    "        return ping_count, None, None, None\n",
    "\n",
    "\n",
    "def correct_match_fields_ryu(candidate_flow, set_fields, remove_fields):\n",
    "    \"\"\"\n",
    "    Update candidate_flow's match fields for Ryu.\n",
    "    set_fields: dict, e.g. {'ipv4_src': '10.0.2.2'}\n",
    "    remove_fields: list of keys, e.g. ['tcp_dst']\n",
    "    \"\"\"\n",
    "    match = candidate_flow.get(\"match\", {})\n",
    "    \n",
    "    # Remove fields\n",
    "    for field in remove_fields:\n",
    "        match.pop(field, None) # Safely remove\n",
    "    \n",
    "    # Set/update fields\n",
    "    match.update(set_fields)\n",
    "    \n",
    "    candidate_flow['match'] = match\n",
    "    return candidate_flow\n",
    "\n",
    "def increase_priority_ryu(candidate_flow, recommended_priority):\n",
    "    candidate_flow['priority'] = recommended_priority\n",
    "    return candidate_flow\n",
    "\n",
    "def fix_action_field_ryu(candidate_flow):\n",
    "    \"\"\"\n",
    "    Make sure actions list is empty (for a block rule).\n",
    "    \"\"\"\n",
    "    candidate_flow['actions'] = []\n",
    "    return candidate_flow\n",
    "\n",
    "def parse_and_execute_corrective_actions_ryu(candidate_flow, llm_response, dpid):\n",
    "    \"\"\"\n",
    "    Applies corrective actions suggested by the LLM, one by one, and re-verifies.\n",
    "    'candidate_flow' is the Ryu flow rule that failed verification.\n",
    "    \"\"\"\n",
    "    if not llm_response or \"recommended_actions\" not in llm_response:\n",
    "        print(\"LLM provided no valid corrective actions.\")\n",
    "        return False\n",
    "        \n",
    "    actions = sorted(llm_response[\"recommended_actions\"], key=lambda x: x[\"rank\"])\n",
    "    base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "    \n",
    "    # We need the original src/dst for re-verification\n",
    "    _, _, src_ip, dst_ip = extract_host_and_ip_ryu(candidate_flow)\n",
    "\n",
    "    # We must delete the old (bad) flow before adding the corrected one\n",
    "    print(\"Deleting the original, failed flow rule...\")\n",
    "    delete_flow(base_url, candidate_flow)\n",
    "    \n",
    "    original_cookie = candidate_flow.get(\"cookie\", 0) # Keep track of original cookie\n",
    "    \n",
    "    for action_item in actions:\n",
    "        action = action_item[\"action\"]\n",
    "        suggestion = action_item.get(\"suggestion\", {})\n",
    "        print(f\"Triggering action: {action} (rank {action_item['rank']})\")\n",
    "\n",
    "        # Make a copy to modify\n",
    "        corrected_flow = candidate_flow.copy()\n",
    "        \n",
    "        if action == \"Correct Match Fields\":\n",
    "            set_fields = suggestion.get(\"set_fields\", {})\n",
    "            remove_fields = suggestion.get(\"remove_fields\", [])\n",
    "            corrected_flow = correct_match_fields_ryu(corrected_flow, set_fields, remove_fields)\n",
    "        \n",
    "        elif action == \"Increase Priority\":\n",
    "            recommended_priority = suggestion.get(\"recommended_priority\")\n",
    "            if recommended_priority:\n",
    "                corrected_flow = increase_priority_ryu(corrected_flow, recommended_priority)\n",
    "            else:\n",
    "                print(\"Warning: LLM suggested priority increase but gave no value.\")\n",
    "                continue\n",
    "                \n",
    "        elif action == \"Fix Action Field\":\n",
    "            corrected_flow = fix_action_field_ryu(corrected_flow)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unknown action: {action}\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the cookie remains the same for tracking\n",
    "        corrected_flow[\"cookie\"] = original_cookie\n",
    "        \n",
    "        # --- 2. Push corrected flow ---\n",
    "        print(f\"Pushing corrected flow: {json.dumps(corrected_flow)}\")\n",
    "        try:\n",
    "            add_flow(base_url, corrected_flow)\n",
    "            time.sleep(0.5) # Settle time\n",
    "        except Exception as e:\n",
    "            print(f\"Exception while pushing corrected flow: {e}\")\n",
    "            continue # Try next action\n",
    "\n",
    "        # --- 3. Re-verify ---\n",
    "        print(\"Re-verifying intent...\")\n",
    "        global llm_caller_flag\n",
    "        # This will ping and set llm_caller_flag (0=PASS, 1=FAIL)\n",
    "        ryu_assurance_for_security_intent(src_ip, dst_ip) \n",
    "        \n",
    "        if llm_caller_flag == 0:\n",
    "            print(f\"Intent deviation resolved after action: {action}\")\n",
    "            return True  # Deviation fixed\n",
    "        else:\n",
    "            print(f\"Deviation not fixed after action: {action}. Deleting flow and trying next action...\")\n",
    "            delete_flow(base_url, corrected_flow) # Clean up before next loop\n",
    "\n",
    "    print(\"\\nAll suggested corrective actions exhausted, but deviation remains. Escalate to operator.\")\n",
    "    return False  # Deviation not fixed\n",
    "\n",
    "def ONOS_assurance_for_qos_intent(src_host, dst_host, src_ip, dst_ip, ping_count=2):\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "def ONOS_assurance_for_forwarding_intent(src_host, dst_host, src_ip, dst_ip, ping_count=2):\n",
    "\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_match_fields(candidate_flow, set_fields, remove_fields):\n",
    "    \"\"\"\n",
    "    Update candidate_flow's match fields: set specified, remove specified.\n",
    "    set_fields: dict, e.g. {'IPV4_SRC': '10.0.2.2/32'}\n",
    "    remove_fields: list of type strings\n",
    "    \"\"\"\n",
    "    selector = candidate_flow['selector']\n",
    "    # Remove fields\n",
    "    selector['criteria'] = [\n",
    "        crit for crit in selector['criteria'] if crit['type'] not in remove_fields\n",
    "    ]\n",
    "    # Set/update fields\n",
    "    for k, v in set_fields.items():\n",
    "        # Remove any existing\n",
    "        selector['criteria'] = [crit for crit in selector['criteria'] if crit['type'] != k]\n",
    "        # Correct value key based on ONOS spec\n",
    "        if k in [\"IPV4_SRC\", \"IPV4_DST\"]:\n",
    "            selector['criteria'].append({'type': k, 'ip': v})\n",
    "        elif k == \"ETH_TYPE\":\n",
    "            selector['criteria'].append({'type': k, 'ethType': v})\n",
    "        else:\n",
    "            selector['criteria'].append({'type': k, 'value': v})\n",
    "    candidate_flow['selector'] = selector\n",
    "    return candidate_flow\n",
    "\n",
    "def increase_priority(candidate_flow, recommended_priority):\n",
    "    candidate_flow['priority'] = recommended_priority\n",
    "    return candidate_flow\n",
    "\n",
    "def fix_action_field(candidate_flow):\n",
    "    \"\"\"\n",
    "    Make sure treatment.instructions is only [{'type': 'NOACTION'}]\n",
    "    \"\"\"\n",
    "    candidate_flow['treatment'] = {\n",
    "        \"instructions\": [{\"type\": \"NOACTION\"}]\n",
    "    }\n",
    "    return candidate_flow\n",
    "\n",
    "def parse_and_execute_corrective_actions_onos(candidate_flow, llm_response, device_id, flow_id, src_ip, dst_ip):\n",
    "    \"\"\"\n",
    "    candidate_flow: dict, the ONOS flow rule you want to fix\n",
    "    existing_flows: list of dicts, flows on the same device\n",
    "    llm_response: dict, output from LLM with 'recommended_actions'\n",
    "    intent_drift_fn: function that checks for intent drift, returns True if drift still exists, False if resolved\n",
    "    push_flow_fn: function to push the modified flow to ONOS\n",
    "    \"\"\"\n",
    "    actions = sorted(llm_response[\"recommended_actions\"], key=lambda x: x[\"rank\"])\n",
    "    for action_item in actions:\n",
    "        action = action_item[\"action\"]\n",
    "        suggestion = action_item[\"suggestion\"]\n",
    "        print(f\"Triggering action: {action} (rank {action_item['rank']})\")\n",
    "\n",
    "        if action == \"Correct Match Fields\":\n",
    "            set_fields = suggestion.get(\"set_fields\", {})\n",
    "            remove_fields = suggestion.get(\"remove_fields\", [])\n",
    "            candidate_flow = correct_match_fields(candidate_flow, set_fields, remove_fields)\n",
    "        elif action == \"Increase Priority\":\n",
    "            recommended_priority = suggestion.get(\"recommended_priority\")\n",
    "            candidate_flow = increase_priority(candidate_flow, recommended_priority)\n",
    "        elif action == \"Fix Action Field\":\n",
    "            candidate_flow = fix_action_field(candidate_flow)\n",
    "        else:\n",
    "            print(f\"Unknown action: {action}\")\n",
    "\n",
    "        # Push the flow to ONOS\n",
    "        # First remove the old one\n",
    "        delete_flow_rule_ONOS(device_id, flow_id)\n",
    "\n",
    "        try:\n",
    "            flow_id = push_flow_rule(device_id, candidate_flow)\n",
    "        except Exception as e:\n",
    "                print(\"Exception found while installing flow rule: \", e)\n",
    "                sys.stdout.flush()\n",
    "        try:\n",
    "            verification_status, operational_flow_rule = verify_flow_rule(device_id, flow_id)\n",
    "            if(verification_status == True):\n",
    "                print(\"\\nCorrected Flow Rules Installed Successfully\\n\")\n",
    "        except Exception as e:\n",
    "                print(\"Corrected Flow Rules Failed to be Installed Successfully. Exception found while verifying flow rule: \", e)\n",
    "                sys.stdout.flush()\n",
    "\n",
    "        global llm_caller_flag\n",
    "\n",
    "        ryu_assurance_for_security_intent(src_ip, dst_ip)\n",
    "\n",
    "        # Check intent drift (assurance) after each action\n",
    "        \n",
    "        if llm_caller_flag == 0:\n",
    "            print(f\"Intent deviation resolved after action: {action}\")\n",
    "            return True  # Deviation fixed\n",
    "\n",
    "        print(f\"Deviation not fixed after action: {action}, proceeding to next action...\")\n",
    "\n",
    "    print(\"\\n\\nAll suggested corrective actions exhausted, but deviation remains. Escalate the issue to the Operator.\\n\")\n",
    "    return False  # Deviation not fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2537,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QoS verification helper functions\n",
    "\n",
    "_UNITS_BYTES = {\"KBytes\":1024, \"MBytes\":1024**2, \"GBytes\":1024**3, \"TBytes\":1024**4, \"Bytes\":1}\n",
    "_UNITS_BITS  = {\"Kbits/sec\":1e3, \"Mbits/sec\":1e6, \"Gbits/sec\":1e9, \"bits/sec\":1.0}\n",
    "\n",
    "def build_pin_selector_forward(src_ip: str, dst_ip: str, dport: int, proto: str = \"tcp\"):\n",
    "    proto = proto.lower()\n",
    "    criteria = [\n",
    "        {\"type\": \"ETH_TYPE\", \"ethType\": \"0x800\"},\n",
    "        {\"type\": \"IP_PROTO\", \"protocol\": 6 if proto == \"tcp\" else 17},\n",
    "        {\"type\": \"IPV4_SRC\", \"ip\": f\"{src_ip}/32\"},\n",
    "        {\"type\": \"IPV4_DST\", \"ip\": f\"{dst_ip}/32\"},\n",
    "    ]\n",
    "    if proto == \"tcp\":\n",
    "        criteria.append({\"type\": \"TCP_DST\", \"tcpPort\": dport})\n",
    "    else:\n",
    "        criteria.append({\"type\": \"UDP_DST\", \"udpPort\": dport})\n",
    "    return {\"criteria\": criteria}\n",
    "\n",
    "def build_pin_selector_reverse(src_ip: str, dst_ip: str, sport: int, proto: str = \"tcp\"):\n",
    "    # reverse direction (dst->src); for TCP we match TCP_SRC=port, for UDP we match UDP_SRC=port\n",
    "    proto = proto.lower()\n",
    "    criteria = [\n",
    "        {\"type\": \"ETH_TYPE\", \"ethType\": \"0x800\"},\n",
    "        {\"type\": \"IP_PROTO\", \"protocol\": 6 if proto == \"tcp\" else 17},\n",
    "        {\"type\": \"IPV4_SRC\", \"ip\": f\"{dst_ip}/32\"},\n",
    "        {\"type\": \"IPV4_DST\", \"ip\": f\"{src_ip}/32\"},\n",
    "    ]\n",
    "    if proto == \"tcp\":\n",
    "        criteria.append({\"type\": \"TCP_SRC\", \"tcpPort\": sport})\n",
    "    else:\n",
    "        criteria.append({\"type\": \"UDP_SRC\", \"udpPort\": sport})\n",
    "    return {\"criteria\": criteria}\n",
    "\n",
    "def build_output_only_treatment(out_port: int | str):\n",
    "    return {\"instructions\": [{\"type\": \"OUTPUT\", \"port\": str(out_port)}]}\n",
    "\n",
    "def build_queue_and_output_treatment(queue_id: int, out_port: int | str):\n",
    "    return {\"instructions\": [{\"type\": \"QUEUE\", \"queueId\": int(queue_id)}, {\"type\": \"OUTPUT\", \"port\": str(out_port)}]}\n",
    "\n",
    "def build_pin_flow_body(device_id: str, selector: dict, treatment: dict, priority: int = 65000):\n",
    "    # ONOS expects an array under \"flows\"\n",
    "    return {\n",
    "        \"flows\": [{\n",
    "            \"deviceId\": device_id,\n",
    "            \"isPermanent\": True,\n",
    "            \"priority\": priority,\n",
    "            \"selector\": selector,\n",
    "            \"treatment\": treatment\n",
    "        }]\n",
    "    }\n",
    "\n",
    "# def delete_flow_rule(device_id: str, flow_id: str):\n",
    "#     url = f\"{ONOS_BASE_URL}/{device_id}/{flow_id}\"\n",
    "#     HEADERS = {\"Accept\": \"application/json\"}\n",
    "#     r = requests.delete(url, headers=HEADERS, auth=(USERNAME, PASSWORD), timeout=15)\n",
    "#     # 204/200/202 = fine; 404 -> already gone is OK\n",
    "#     if r.status_code not in (200, 202, 204, 404):\n",
    "#         print(f\"[WARN] DELETE {device_id}/{flow_id} failed {r.status_code}: {r.text}\")\n",
    "\n",
    "# ---------- PIN PATH VIA S3 (place below the helpers) ----------\n",
    "def pin_path_via_s3(src_ip=\"10.0.1.1\", dst_ip=\"10.0.1.3\", dst_port=80, proto=\"tcp\",\n",
    "                    include_s4_forward_qos=False, s4_queue_id=1):\n",
    "    \"\"\"\n",
    "    Installs high-priority OUTPUT-only pins on s1 and s3 for forward,\n",
    "    OUTPUT-only pins on s4,s3,s1 for reverse ACK path.\n",
    "    Optionally installs the s4 forward QoS rule (QUEUE+OUTPUT) if needed.\n",
    "    Returns dict of {label: (deviceId, flowId)} for cleanup.\n",
    "    \"\"\"\n",
    "    S1, S3, S4 = \"of:0000000000000001\", \"of:0000000000000003\", \"of:0000000000000004\"\n",
    "    pins = {}\n",
    "\n",
    "    # Selectors\n",
    "    sel_fwd = build_pin_selector_forward(src_ip, dst_ip, dport=dst_port, proto=proto)\n",
    "    sel_rev = build_pin_selector_reverse(src_ip, dst_ip, sport=dst_port, proto=proto)\n",
    "\n",
    "    # ---- Forward pins (s1->s3->s4). s4 forward QoS rule optional. ----\n",
    "    # s1 out to s3 (port 2)\n",
    "    body = build_pin_flow_body(S1, sel_fwd, build_output_only_treatment(2), priority=65000)\n",
    "    fid = push_flow_rule(S1, body); pins[\"s1_fwd\"] = (S1, fid)\n",
    "\n",
    "    # s3 out to s4 (port 2)\n",
    "    body = build_pin_flow_body(S3, sel_fwd, build_output_only_treatment(2), priority=65000)\n",
    "    fid = push_flow_rule(S3, body); pins[\"s3_fwd\"] = (S3, fid)\n",
    "\n",
    "    if include_s4_forward_qos:\n",
    "        # s4 set-queue then out to h3 (port 3)\n",
    "        body = build_pin_flow_body(S4, sel_fwd, build_queue_and_output_treatment(s4_queue_id, 3), priority=65000)\n",
    "        fid = push_flow_rule(S4, body); pins[\"s4_qos_fwd\"] = (S4, fid)\n",
    "\n",
    "    # ---- Reverse (ACK) pins (h3->h1) along s4->s3->s1 ----\n",
    "    # s4 out to s3 (port 2)\n",
    "    body = build_pin_flow_body(S4, sel_rev, build_output_only_treatment(2), priority=65000)\n",
    "    fid = push_flow_rule(S4, body); pins[\"s4_rev\"] = (S4, fid)\n",
    "\n",
    "    # s3 out to s1 (port 1)\n",
    "    body = build_pin_flow_body(S3, sel_rev, build_output_only_treatment(1), priority=65000)\n",
    "    fid = push_flow_rule(S3, body); pins[\"s3_rev\"] = (S3, fid)\n",
    "\n",
    "    # s1 out to h1 (port 3)\n",
    "    body = build_pin_flow_body(S1, sel_rev, build_output_only_treatment(3), priority=65000)\n",
    "    fid = push_flow_rule(S1, body); pins[\"s1_rev\"] = (S1, fid)\n",
    "\n",
    "    return pins\n",
    "\n",
    "def run_in_host(host_pid: int, cmd: str, timeout: int = 30) -> tuple[str, str]:\n",
    "    # Enter the host namespace; sudo is applied by execute_command_full\n",
    "    base = f\"mnexec -a {host_pid} {cmd}\"\n",
    "    return execute_command_full(base, timeout=timeout, with_sudo=True)\n",
    "\n",
    "def run_in_host_old(host_pid: int, cmd: str, timeout: int = 30, require_sudo: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Run a command inside a Mininet host namespace using mnexec -a <PID>.\n",
    "    Always uses sudo here to avoid 'Permission denied' when entering namespaces.\n",
    "    \"\"\"\n",
    "    #prefix = f\"echo {shlex.quote(sudo_password)} | sudo -S \"\n",
    "    #full_cmd = f\"{prefix}mnexec -a {host_pid} {cmd}\"\n",
    "\n",
    "    full_cmd = f\"mnexec -a {host_pid} {cmd}\"\n",
    "    #return execute_command_full(full_cmd, timeout=timeout)\n",
    "    return execute_command_full(full_cmd, with_sudo=True)\n",
    "\n",
    "def ensure_no_iperf_server_old2(host_pid: int, port: int) -> None:\n",
    "    # Fast attempt\n",
    "    run_in_host(host_pid, f\"pkill -f {shlex.quote(f'iperf3 -s -p {int(port)}')} || true\", timeout=5)\n",
    "    # Fallback: precise kill (note doubled braces in awk)\n",
    "    run_in_host(\n",
    "        host_pid,\n",
    "        f\"bash -lc \\\"pgrep -af 'iperf3.*-s.*-p {int(port)}' | awk '{{{{print $1}}}}' | xargs -r kill -9\\\"\",\n",
    "        timeout=5\n",
    "    )\n",
    "\n",
    "def ensure_no_iperf_server(host_pid: int, port: int) -> None:\n",
    "    # Fast attempt\n",
    "    run_in_host(host_pid, f\"/usr/bin/pkill -f {shlex.quote(f'iperf3 -s -p {int(port)}')} || true\", timeout=5)\n",
    "    # Fallback: precise kill (note doubled braces in awk)\n",
    "    run_in_host(\n",
    "        host_pid,\n",
    "        # FIX: Changed \\\\\" to \\\" to correctly escape the quotes\n",
    "        f\"bash -lc \\\"/usr/bin/pgrep -af 'iperf3.*-s.*-p {int(port)}' | /usr/bin/awk '{{{{print $1}}}}' | /usr/bin/xargs -r /bin/kill -9\\\"\",\n",
    "        timeout=5\n",
    "    )\n",
    "\n",
    "\n",
    "def ensure_no_iperf_server_old(host_pid: int, port: int) -> None:\n",
    "    \"\"\"\n",
    "    Attempt to kill any existing iperf3 server on that port in the host namespace.\n",
    "    \"\"\"\n",
    "    # Try pkill first, then kill by pgrep if needed.\n",
    "    run_in_host(host_pid, f\"pkill -f {shlex.quote(f'iperf3 -s -p {port}')} || true\", timeout=5, require_sudo=True)\n",
    "    run_in_host(host_pid, f\"bash -lc \\\"pgrep -af 'iperf3.*-s.*-p {port}' | awk '{{print $1}}' | xargs -r kill -9\\\"\", timeout=5, require_sudo=True)\n",
    "\n",
    "\n",
    "def start_iperf_server(host_pid: int, port: int, extra_args: str = \"\") -> None:\n",
    "    ensure_no_iperf_server(host_pid, port)\n",
    "\n",
    "    run_in_host(\n",
    "            host_pid,\n",
    "            f\"bash -lc 'nohup /usr/bin/iperf3 -s -p {int(port)} --one-off {extra_args} \"\n",
    "            f\">/tmp/iperf3_s_{int(port)}.log 2>&1 & echo $!'\",\n",
    "            timeout=5\n",
    "        )\n",
    "    \n",
    "    deadline = time.time() + 4.0\n",
    "    while time.time() < deadline:\n",
    "        out, err = run_in_host(\n",
    "            host_pid,\n",
    "            f\"bash -lc \\\"/usr/bin/ss -ltnp | grep ':{int(port)} ' || true\\\"\",\n",
    "            timeout=3\n",
    "        )\n",
    "        if out.strip():\n",
    "            return  # Server is listening on port\n",
    "\n",
    "    # Not listening: surface the server log to explain why\n",
    "    log, _ = run_in_host(\n",
    "        host_pid,\n",
    "        f\"bash -lc 'tail -n +200 /tmp/iperf3_s_{int(port)}.log 2>/dev/null || true'\",\n",
    "        timeout=3\n",
    "    )\n",
    "    raise RuntimeError(f\"iperf3 server failed to bind/listen on port {port}. Server log:\\n{log}\")\n",
    "\n",
    "\n",
    "def start_iperf_server_old2(host_pid: int, port: int, extra_args: str = \"\") -> None:\n",
    "    ensure_no_iperf_server(host_pid, port)\n",
    "\n",
    "    run_in_host(\n",
    "            host_pid,\n",
    "            f\"bash -lc 'nohup iperf3 -s -p {int(port)} --one-off {extra_args} \"\n",
    "            f\">/tmp/iperf3_s_{int(port)}.log 2>&1 & echo $!'\",\n",
    "            timeout=5\n",
    "        )\n",
    "    \n",
    "    deadline = time.time() + 4.0\n",
    "    while time.time() < deadline:\n",
    "        out, err = run_in_host(\n",
    "            host_pid,\n",
    "            f\"bash -lc \\\"ss -ltnp | grep ':{int(port)} ' || true\\\"\",\n",
    "            timeout=3\n",
    "        )\n",
    "        if out.strip():\n",
    "            return  # Server is listening on port\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Not listening: surface the server log to explain why\n",
    "    log, _ = run_in_host(\n",
    "        host_pid,\n",
    "        f\"bash -lc 'tail -n +200 /tmp/iperf3_s_{int(port)}.log 2>/dev/null || true'\",\n",
    "        timeout=3\n",
    "    )\n",
    "    raise RuntimeError(f\"iperf3 server failed to bind/listen on port {port}. Server log:\\n{log}\")\n",
    "\n",
    "    #cmd = f\"bash -lc 'nohup iperf3 -s -p {int(port)} --one-off {extra_args} >/tmp/iperf3_s_{int(port)}.log 2>&1 & echo $!'\"\n",
    "    #run_in_host(host_pid, cmd, timeout=5)\n",
    "    #time.sleep(0.6)  # small readiness delay\n",
    "\n",
    "\n",
    "def start_iperf_server_old(host_pid: int, port: int, extra_args: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Start iperf3 server (-s) on port (may require sudo for <1024). Run in background.\n",
    "    Use --one-off to auto-exit after one test.\n",
    "    \"\"\"\n",
    "    ensure_no_iperf_server(host_pid, port)\n",
    "    cmd = f\"bash -lc 'nohup iperf3 -s -p {int(port)} --one-off {extra_args} >/tmp/iperf3_s_{port}.log 2>&1 & echo $!'\"\n",
    "    out = run_in_host(host_pid, cmd, timeout=5, require_sudo=True)\n",
    "    # Best-effort small delay for readiness\n",
    "    time.sleep(0.6)\n",
    "\n",
    "def stop_iperf_server(host_pid: int, port: int) -> None:\n",
    "    ensure_no_iperf_server(host_pid, port)\n",
    "\n",
    "def sudo_sh(cmd, timeout=20):\n",
    "    # Pass ONLY the command. execute_command_full will add the sudo prefix.\n",
    "    return execute_command_full(f\"bash -lc {shlex.quote(cmd)}\", timeout=timeout, with_sudo=True)\n",
    "\n",
    "def sudo_sh_old(cmd, timeout=20):\n",
    "    return execute_command_full(f\"echo {shlex.quote(sudo_password)} | sudo -S bash -lc {shlex.quote(cmd)}\", timeout=timeout, with_sudo=True)\n",
    "\n",
    "def find_bridge_for_device(device_id: str) -> str:\n",
    "    js = sudo_sh(\"ovs-vsctl -f json list Bridge\", timeout=10)\n",
    "    data = json.loads(js)\n",
    "    dpid_hex = device_id.replace(\"of:\",\"\").lower()\n",
    "    head = data[\"headings\"]\n",
    "    for row in data[\"data\"]:\n",
    "        obj = dict(zip(head, row))\n",
    "        oc = dict(obj[\"other_config\"][1]) if isinstance(obj.get(\"other_config\"), list) else {}\n",
    "        oc_dpid = (oc.get(\"datapath-id\") or oc.get(\"datapath_id\") or \"\").lower().replace(\":\",\"\")\n",
    "        if oc_dpid == dpid_hex:\n",
    "            name = obj[\"name\"]\n",
    "            return name[-1] if isinstance(name, list) else name\n",
    "    raise RuntimeError(\"Bridge not found\")\n",
    "\n",
    "def parse_qos_show(iface: str) -> dict:\n",
    "    \"\"\"\n",
    "    Runs `ovs-appctl qos/show` and parses its output into a dict.\n",
    "    This version correctly uses the execute_command_full helper.\n",
    "    \"\"\"\n",
    "    # (Assumes 'sudo_password' is a global variable)\n",
    "    cmd = f\"ovs-appctl -t ovs-vswitchd qos/show {shlex.quote(iface)}\"\n",
    "    \n",
    "    # Call the helper that returns a (stdout, stderr) tuple\n",
    "    #stdout_text, stderr_text = execute_command_full(cmd, sudo_password, with_sudo=True)\n",
    "    stdout_text, stderr_text = execute_command_full(cmd, with_sudo=True)\n",
    "\n",
    "    if \"No QoS configured\" in stdout_text or \"No QoS configured\" in stderr_text:\n",
    "        return {\"raw\": stdout_text, \"queues\": {}}\n",
    "    if not stdout_text and \"Timeout\" in stderr_text:\n",
    "        print(f\"[WARN] Command timed out: {cmd}\")\n",
    "        return {\"raw\": \"Timeout\", \"queues\": {}}\n",
    "    \n",
    "    # --- This is the new parsing logic ---\n",
    "    raw = stdout_text\n",
    "    queues = {}\n",
    "    qos_meta = {\"raw\": raw}\n",
    "    current_obj = None # Can be 'qos_meta' or a specific queue dict\n",
    "    \n",
    "    for raw_line in raw.splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Check for a new section header (e.g., \"QoS:\", \"Queue 0:\", \"Default:\")\n",
    "        header_match = re.match(r\"^(QoS|Queue|Default):\\s*(.*)\", line)\n",
    "        if header_match:\n",
    "            obj_type = header_match.group(1)\n",
    "            if obj_type == \"QoS\":\n",
    "                current_obj = qos_meta\n",
    "            elif obj_type == \"Queue\":\n",
    "                q_id = line.split()[1].replace(\":\", \"\") # Get \"0\" from \"Queue 0:\"\n",
    "                queues[q_id] = {}\n",
    "                current_obj = queues[q_id]\n",
    "            elif obj_type == \"Default\":\n",
    "                queues[\"default\"] = {}\n",
    "                current_obj = queues[\"default\"]\n",
    "            continue\n",
    "        \n",
    "        # Parse key-value pairs\n",
    "        if current_obj is not None:\n",
    "            kv_match = re.match(r\"^([\\w-]+):\\s*(\\d+)\", line)\n",
    "            if kv_match:\n",
    "                key = kv_match.group(1)\n",
    "                try:\n",
    "                    value = int(kv_match.group(2))\n",
    "                except ValueError:\n",
    "                    value = kv_match.group(2)\n",
    "                current_obj[key] = value\n",
    "\n",
    "    qos_meta[\"queues\"] = queues\n",
    "    return qos_meta\n",
    "\n",
    "def parse_qos_show_old(iface: str):\n",
    "    \"\"\"\n",
    "    Parse `ovs-appctl -t ovs-vswitchd qos/show IFACE` that looks like:\n",
    "\n",
    "      QoS: s2onos-eth2 linux-htb\n",
    "      max-rate: 10000000\n",
    "\n",
    "      Default:\n",
    "        burst: 12512\n",
    "        min-rate: 6000000\n",
    "        tx_packets: 1868919\n",
    "        tx_bytes: 267187192\n",
    "        tx_errors: 0\n",
    "\n",
    "      Queue 1:\n",
    "        burst: 12512\n",
    "        min-rate: 4000000\n",
    "        tx_packets: 3429\n",
    "        tx_bytes: 5078775\n",
    "        tx_errors: 0\n",
    "\n",
    "    Returns: {\"queues\": {\"default\": {...}, \"1\": {...}}, \"raw\": <text>, \"_qos\": {\"max-rate\": ...}}\n",
    "    \"\"\"\n",
    "    txt = sudo_sh(f\"ovs-appctl -t ovs-vswitchd qos/show {shlex.quote(iface)} 2>&1\", timeout=8)\n",
    "\n",
    "    queues = {}\n",
    "    current = None\n",
    "    qos_meta = {}\n",
    "\n",
    "    for raw in txt.splitlines():\n",
    "        line = raw.strip()\n",
    "\n",
    "        # Top-level QoS meta (e.g., max-rate)\n",
    "        m_qos_max = re.search(r'\\bmax-rate:\\s*(\\d+)', line, re.IGNORECASE)\n",
    "        if m_qos_max and current is None:\n",
    "            qos_meta[\"max-rate\"] = int(m_qos_max.group(1))\n",
    "\n",
    "        # Block headers\n",
    "        if re.match(r'^(Default)\\s*:\\s*$', line, re.IGNORECASE):\n",
    "            current = \"default\"\n",
    "            queues.setdefault(current, {})\n",
    "            continue\n",
    "        m_q = re.match(r'^(?:Queue|queue)\\s+(\\d+)\\s*:\\s*$', line)\n",
    "        if m_q:\n",
    "            current = m_q.group(1)\n",
    "            queues.setdefault(current, {})\n",
    "            continue\n",
    "\n",
    "        # Inside a block, pick out key:value lines we care about\n",
    "        if current:\n",
    "            for key, pat in [\n",
    "                (\"min-rate\",   r'\\bmin-rate:\\s*(\\d+)'),\n",
    "                (\"max-rate\",   r'\\bmax-rate:\\s*(\\d+)'),\n",
    "                (\"tx_bytes\",   r'\\btx_bytes:\\s*(\\d+)'),\n",
    "                (\"tx_packets\", r'\\btx_packets:\\s*(\\d+)'),\n",
    "            ]:\n",
    "                m = re.search(pat, line, re.IGNORECASE)\n",
    "                if m:\n",
    "                    queues[current][key] = int(m.group(1))\n",
    "\n",
    "    return {\"queues\": queues, \"raw\": txt, \"_qos\": qos_meta}\n",
    "\n",
    "# # keep snapshot_queue using the patched parser\n",
    "def snapshot_queue(iface: str, queue_id: int | str):\n",
    "    parsed = parse_qos_show(iface)\n",
    "    q = parsed[\"queues\"].get(str(queue_id)) or parsed[\"queues\"].get(\"default\") or {}\n",
    "    return {\n",
    "        \"min_rate\": q.get(\"min-rate\"),\n",
    "        \"max_rate\": q.get(\"max-rate\"),\n",
    "        \"tx_bytes\": q.get(\"tx_bytes\"),\n",
    "        \"tx_packets\": q.get(\"tx_packets\"),\n",
    "        \"_raw\": parsed[\"raw\"][:1200]\n",
    "    }\n",
    "\n",
    "def _sudo(cmd, timeout=30):\n",
    "    return execute_command_full(f\"bash -lc {shlex.quote(cmd)}\", timeout=timeout, with_sudo=True)\n",
    "    #return execute_command_full(f\"echo {shlex.quote(sudo_password)} | sudo -S bash -lc {shlex.quote(cmd)}\", timeout=timeout)\n",
    "\n",
    "def _ns(pid, cmd, timeout=90):\n",
    "    return execute_command_full(f\"mnexec -a {pid} bash -lc {shlex.quote(cmd)}\", timeout=timeout, with_sudo=True)\n",
    "    #return execute_command_full(f\"echo {shlex.quote(sudo_password)} | sudo -S mnexec -a {pid} bash -lc {shlex.quote(cmd)}\", timeout=timeout)\n",
    "\n",
    "def _parse_iperf_sender(line: str) -> tuple[float, int] | tuple[None, None]:\n",
    "    \"\"\"\n",
    "    Parses a single iperf sender summary line.\n",
    "    Line format: [ 3] 0.0-10.0 sec  112 MBytes  93.8 Mbits/sec\n",
    "    Line format: [SUM] 0.0-10.0 sec  112 MBytes  93.8 Mbits/sec\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Regex to find the transfer and bandwidth\n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+MBytes\\s+(\\d+(?:\\.\\d+)?)\\s+Mbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024 * 1024)\n",
    "            mbps = float(match.group(2))\n",
    "            return mbps, bytes_sent\n",
    "            \n",
    "        # Fallback for Kbits\n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+KBytes\\s+(\\d+(?:\\.\\d+)?)\\s+Kbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024)\n",
    "            mbps = float(match.group(2)) / 1024.0\n",
    "            return mbps, bytes_sent\n",
    "            \n",
    "        # Fallback for bits\n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+Bytes\\s+(\\d+(?:\\.\\d+)?)\\s+bits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)))\n",
    "            mbps = float(match.group(2)) / 1_000_000.0\n",
    "            return mbps, bytes_sent\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse iperf line '{line}': {e}\")\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def _iperf_text_summary(client_pid, dst_ip, dst_port, duration, extra_args=\"\") -> dict:\n",
    "    \"\"\"\n",
    "    Runs iperf and returns the parsed summary text.\n",
    "    This version correctly finds the [SUM] line for parallel tests.\n",
    "    \"\"\"\n",
    "    # Build the command *without* the password\n",
    "    #cmd = f\"mnexec -a {client_pid} iperf -c {dst_ip} -p {dst_port} -t {duration} {extra_args}\"\n",
    "    #cmd = f\"mnexec -a {client_pid} iperf3 -c {dst_ip} -p {dst_port} -t {duration} {extra_args}\" # <-- FIX\n",
    "    iperf_cmd_str = f'/usr/bin/iperf3 -c {dst_ip} -p {dst_port} -t {duration} {extra_args}'\n",
    "    cmd = f\"mnexec -a {client_pid} bash -lc {shlex.quote(iperf_cmd_str)}\"\n",
    "\n",
    "    #cmd = f\"mnexec -a {client_pid} iperf3 -c {dst_ip} -p {dst_port} -t {duration} {extra_args}\"\n",
    "    # stdout_text, stderr_text = execute_command_full(cmd, timeout=max(15, duration + 10), with_sudo=True)\n",
    "\n",
    "    \n",
    "    # # Call the new helper, which handles sudo and returns two values\n",
    "    # # (Assumes 'sudo_password' is a global variable)\n",
    "    # stdout_text, stderr_text = execute_command_full(cmd, sudo_password, timeout=max(15, duration + 10), with_sudo=True)\n",
    "\n",
    "    stdout_text, stderr_text = execute_command_full(\n",
    "        cmd,\n",
    "        timeout=max(15, duration + 10),\n",
    "        with_sudo=True\n",
    "    )\n",
    "    \n",
    "    if \"Connection refused\" in stderr_text:\n",
    "        raise RuntimeError(f\"Iperf connection refused. Is server running on {dst_ip}:{dst_port}?\")\n",
    "    if \"No route to host\" in stderr_text:\n",
    "        raise RuntimeError(f\"Iperf 'No route to host' for {dst_ip}. Check pinning flows.\")\n",
    "\n",
    "    stdout_lines = stdout_text.splitlines()\n",
    "    \n",
    "    target_line = None\n",
    "    for ln in reversed(stdout_lines): # Search from the bottom up\n",
    "        if (\"bits/sec\" in ln and \n",
    "           (f\" 0.0-{duration:.1f}\" in ln or f\" 0.00-{duration:.02f}\" in ln or \" 0.0-\" in ln)):\n",
    "            \n",
    "            if ln.strip().startswith(\"[SUM]\"):\n",
    "                target_line = ln # Prefer SUM line\n",
    "                break\n",
    "            if ln.strip().startswith(\"[\"):\n",
    "                target_line = ln # Fallback to any summary line\n",
    "    \n",
    "    if not target_line:\n",
    "        print(\"[DEBUG] iperf stdout:\", stdout_text)\n",
    "        print(\"[DEBUG] iperf stderr:\", stderr_text)\n",
    "        raise RuntimeError(\"Could not find iperf sender summary line. iperf output was not as expected.\")\n",
    "\n",
    "    mbps, bytes_sent = _parse_iperf_sender(target_line)\n",
    "    return {\"sender_mbps\": mbps, \"bytes_sent\": bytes_sent, \"sender_line\": target_line}\n",
    "\n",
    "def _parse_iperf_sender_old(line: str):\n",
    "    m = re.search(\n",
    "        r'\\s(\\d+(?:\\.\\d+)?)\\s*(KBytes|MBytes|GBytes|TBytes|Bytes)\\s+'\n",
    "        r'(\\d+(?:\\.\\d+)?)\\s*(Kbits/sec|Mbits/sec|Gbits/sec|bits/sec)\\s+.*sender', line)\n",
    "    if not m:\n",
    "        raise RuntimeError(\"iperf sender parse failed: \" + line)\n",
    "    bytes_val = float(m.group(1)) * _UNITS_BYTES[m.group(2)]\n",
    "    bps_val   = float(m.group(3)) * _UNITS_BITS[m.group(4)]\n",
    "    return bps_val/1e6, int(bytes_val)\n",
    "\n",
    "def _iperf_text_summary_old(client_pid, dst_ip, dst_port, duration, extra_args=\"\"):\n",
    "    out = _ns(client_pid, f\"iperf3 -c {shlex.quote(dst_ip)} -p {int(dst_port)} -t {int(duration)} -f m {extra_args}\", \n",
    "              timeout=duration+60)\n",
    "    # prefer final [SUM] sender line; fallback to last 'sender'\n",
    "    lines = out.strip().splitlines()\n",
    "    target = None\n",
    "    for ln in lines:\n",
    "        if \"[SUM]\" in ln and \"sender\" in ln:\n",
    "            target = ln\n",
    "    if not target:\n",
    "        for ln in reversed(lines):\n",
    "            if \"sender\" in ln:\n",
    "                target = ln; break\n",
    "    # after failing to find 'sender', fallback to last line with 'sec' and 'bits/sec'\n",
    "    if not target:\n",
    "        for ln in reversed(lines):\n",
    "            if \"sec\" in ln and \"bits/sec\" in ln:\n",
    "                target = ln; break\n",
    "    if not target:\n",
    "        raise RuntimeError(\"Could not find iperf sender summary line.\")\n",
    "    mbps, bytes_sent = _parse_iperf_sender(target)\n",
    "    return {\"sender_mbps\": mbps, \"bytes_sent\": bytes_sent, \"sender_line\": target}\n",
    "\n",
    "# ---- ONOS flow counters helper (expects your get_a_flow_rule_ONOS) ----\n",
    "def _flow_counters(flow_json):\n",
    "    try:\n",
    "        f = flow_json[\"flows\"][0]\n",
    "        return int(f.get(\"packets\", 0)), int(f.get(\"bytes\", 0))\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "# def verify_qos_flow_with_iperf(\n",
    "#     flow_device_id: str, flow_id: str,\n",
    "#     queue_device_id: str, queue_port_no: int, queue_id: int,\n",
    "#     src_ip: str, dst_ip: str, dst_port: int,\n",
    "#     target_mbps: float,\n",
    "#     duration_sec: int = 8, parallel: int = 8, tcp_mss: int = 1200,\n",
    "#     tolerance_pct: float = 10.0,\n",
    "#     pin_path_flows: list | None = None,   # optional: list of (deviceId, out_port, direction) dicts you push via your push_flow_rule\n",
    "#     protocol: str = \"tcp\",                # <-- NEW\n",
    "#     udp_bw_mbps: float = 50.0,            # <-- NEW (target send rate for UDP)\n",
    "#     udp_len_bytes: int = 1200,            # <-- NEW (datagram size)\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Returns a dict result and prints a human summary.\n",
    "#     \"\"\"\n",
    "#     # Resolve actors\n",
    "#     iface = get_iface_for_port(queue_device_id, queue_port_no)\n",
    "#     client_pid = get_mininet_host_pid(ip_to_host[src_ip])\n",
    "#     server_pid = get_mininet_host_pid(ip_to_host[dst_ip])\n",
    "\n",
    "#     proto = protocol.lower()\n",
    "#     if proto == \"tcp\":\n",
    "#         extra_args = f\"-P {parallel} -M {tcp_mss}\"\n",
    "#     else:\n",
    "#         # For UDP you must specify a target rate with -b (pick higher than your cap so the shaper clamps it)\n",
    "#         extra_args = f\"-u -b {udp_bw_mbps}M -l {udp_len_bytes} -P {max(1, parallel)}\"\n",
    "\n",
    "#     # Optional: push pinning flows if provided (expects caller to craft correct selectors)\n",
    "#     pushed_ids = []\n",
    "#     if pin_path_flows:\n",
    "#         for item in pin_path_flows:\n",
    "#             dev, outp, direction = item[\"deviceId\"], item[\"out_port\"], item[\"direction\"]\n",
    "#             if direction == \"forward\":\n",
    "#                 sel = build_pin_selector_forward(src_ip, dst_ip, dst_port, proto=proto)\n",
    "#             else:\n",
    "#                 sel = build_pin_selector_reverse(src_ip, dst_ip, dst_port, proto=proto)\n",
    "#             flow_body = {\n",
    "#                 \"flows\": [{\n",
    "#                     \"priority\": 65000,\n",
    "#                     \"isPermanent\": True,\n",
    "#                     \"deviceId\": dev,\n",
    "#                     \"treatment\": {\"instructions\":[ {\"type\":\"OUTPUT\",\"port\": str(outp)} ]},\n",
    "#                     \"selector\": sel\n",
    "#                 }]}\n",
    "#             fid = push_flow_rule(dev, flow_body)\n",
    "#             pushed_ids.append((dev, fid))\n",
    "\n",
    "#     # Best-effort offload guard at egress\n",
    "#     try: _sudo(f\"ethtool -K {shlex.quote(iface)} gro off gso off tso off\", timeout=6)\n",
    "#     except Exception: pass\n",
    "\n",
    "#     # 1) Snapshots before\n",
    "#     #print(\"\\nSnapshot 1: before\\n\")\n",
    "#     flow_before = get_a_flow_rule_ONOS(flow_device_id, flow_id) or {}\n",
    "#     f_pkts0, f_bytes0 = _flow_counters(flow_before)\n",
    "#     q0 = snapshot_queue(iface, queue_id)\n",
    "\n",
    "#     # 2) Run iperf\n",
    "#     start_iperf_server(server_pid, dst_port)\n",
    "#     time.sleep(0.6)\n",
    "#     t0 = time.time()\n",
    "#     iptxt = _iperf_text_summary(client_pid, dst_ip, dst_port, duration_sec, extra_args=extra_args)\n",
    "#     t1 = time.time()\n",
    "#     stop_iperf_server(server_pid, dst_port)\n",
    "#     time.sleep(0.8)\n",
    "\n",
    "#     # 3) Snapshots after\n",
    "#     #print(\"\\nSnapshot 1: before\\n\")\n",
    "#     flow_after = get_a_flow_rule_ONOS(flow_device_id, flow_id) or {}\n",
    "#     f_pkts1, f_bytes1 = _flow_counters(flow_after)\n",
    "#     q1 = snapshot_queue(iface, queue_id)\n",
    "\n",
    "#     # 4) Deltas\n",
    "#     elapsed = max(0.001, t1 - t0)\n",
    "#     q_bytes = None if (q0[\"tx_bytes\"] is None or q1[\"tx_bytes\"] is None) else (q1[\"tx_bytes\"] - q0[\"tx_bytes\"])\n",
    "#     q_mbps  = (q_bytes * 8 / elapsed / 1e6) if q_bytes is not None else None\n",
    "#     f_pkts_delta = (f_pkts1 - f_pkts0) if (f_pkts0 is not None and f_pkts1 is not None) else None\n",
    "#     f_bytes_delta = (f_bytes1 - f_bytes0) if (f_bytes0 is not None and f_bytes1 is not None) else None\n",
    "#     f_mbps = (f_bytes_delta * 8 / elapsed / 1e6) if (f_bytes_delta is not None) else None\n",
    "\n",
    "#     # 5) Decision\n",
    "#     rate_ok   = (q_mbps is not None) and (abs(q_mbps - target_mbps) <= (tolerance_pct/100.0)*target_mbps)\n",
    "#     packets_ok= (f_pkts_delta or 0) > 0\n",
    "#     verdict   = \"PASS\" if (rate_ok and packets_ok) else \"FAIL\"\n",
    "\n",
    "#     # Print summary\n",
    "#     print(\"\\n=== QoS FLOW VERIFICATION ===\")\n",
    "#     print(f\"Flow {flow_id} @ {flow_device_id}  → selector should match {protocol} dst {dst_port} to {dst_ip}\")\n",
    "#     print(f\"Egress iface={iface} queue={queue_id}  target≈{target_mbps:.3f} Mbps  tol=±{tolerance_pct:.0f}%\")\n",
    "#     print(f\"iperf sender: {iptxt['sender_mbps']:.3f} Mbps  bytes≈{iptxt['bytes_sent']}\")\n",
    "#     print(f\"Queue Δbytes={q_bytes} over {elapsed:.3f}s  → queue_measured≈{(q_mbps or 0):.3f} Mbps\")\n",
    "#     print(f\"Flow Δ: packets={f_pkts_delta} bytes={f_bytes_delta}  (flow_measured≈{(f_mbps or 0):.3f} Mbps)\")\n",
    "#     print(f\"Queue caps (min/max): {q0['min_rate']} / {q0['max_rate']}  →  {q1['min_rate']} / {q1['max_rate']}\")\n",
    "#     print(\"VERDICT:\", verdict)\n",
    "\n",
    "#     return {\n",
    "#         \"elapsed_sec\": elapsed,\n",
    "#         \"iperf_sender_mbps\": iptxt[\"sender_mbps\"],\n",
    "#         \"queue_measured_mbps\": q_mbps,\n",
    "#         \"queue_delta_bytes\": q_bytes,\n",
    "#         \"flow_delta_packets\": f_pkts_delta,\n",
    "#         \"flow_delta_bytes\": f_bytes_delta,\n",
    "#         \"flow_measured_mbps\": f_mbps,\n",
    "#         \"verdict\": verdict\n",
    "#     }\n",
    "\n",
    "def ensure_qos_cap(device_id: str, port_no: int, qid: int, min_bps: int, max_bps: int, port_cap_bps: int = 100_000_000):\n",
    "    iface = get_iface_for_port(device_id, port_no)\n",
    "    print(f\"[INFO] Setting QoS on {iface}: q{qid} min={min_bps} max={max_bps}, root max-rate={port_cap_bps}bps\")\n",
    "    sudo_sh(f\"ovs-vsctl --if-exists clear Port {shlex.quote(iface)} qos\")\n",
    "    cmd = (\n",
    "        \"ovs-vsctl \"\n",
    "        f\"-- --id=@q create Queue other-config:min-rate={min_bps} other-config:max-rate={max_bps} \"\n",
    "        f\"-- --id=@qos create QoS type=linux-htb other-config:max-rate={port_cap_bps} queues:{qid}=@q \"\n",
    "        f\"-- set Port {shlex.quote(iface)} qos=@qos\"\n",
    "    )\n",
    "    print(sudo_sh(cmd))\n",
    "    print(sudo_sh(f\"ovs-appctl -t ovs-vswitchd qos/show {shlex.quote(iface)}\"))\n",
    "\n",
    "\n",
    "# --- NEW [cell 23] ---\n",
    "# QoS verification helper functions (Ryu-compatible)\n",
    "\n",
    "# (These OVS/iperf helpers from your original cell 23 are controller-agnostic and correct)\n",
    "# _UNITS_BYTES, _UNITS_BITS, _sudo, _ns, _parse_iperf_sender, _iperf_text_summary\n",
    "# (Assume they are copied here)\n",
    "\n",
    "# --- NEW Helper: Get flow by cookie ---\n",
    "def get_flow_by_cookie(base, dpid, cookie):\n",
    "    \"\"\"\n",
    "    Fetches all flows for a dpid and returns the one matching the cookie.\n",
    "    \"\"\"\n",
    "    all_flows_dict = get_flows(base, dpid)\n",
    "    flows_list = all_flows_dict.get(str(dpid), [])\n",
    "    \n",
    "    # Use the find_by_cookie helper from [cell 5]\n",
    "    return find_by_cookie(flows_list, cookie)\n",
    "\n",
    "# --- NEW Helper: Get flow counters ---\n",
    "def _ryu_flow_counters(flow_json):\n",
    "    \"\"\"\n",
    "    Extracts packet and byte counters from a Ryu flow rule.\n",
    "    \"\"\"\n",
    "    if flow_json:\n",
    "        return int(flow_json.get(\"packet_count\", 0)), int(flow_json.get(\"byte_count\", 0))\n",
    "    return None, None\n",
    "\n",
    "# --- NEW [cell 24] ---\n",
    "# (This cell contains helpers from your original topology)\n",
    "# (HOSTS, SW_OF, HOST_ATTACH are fine)\n",
    "# (The helpers like choose_dst_for_port, fill_missing_endpoints, etc. are also fine)\n",
    "\n",
    "# --- NEW [cell 25] ---\n",
    "# (Contains the main QoS verifier and pinning logic)\n",
    "\n",
    "def verify_qos_flow_with_iperf(\n",
    "    flow_device_id: int, flow_cookie: int, # <-- Changed to cookie\n",
    "    queue_device_id: int, queue_port_no: int, queue_id: int,\n",
    "    src_ip: str, dst_ip: str, dst_port: int,\n",
    "    target_mbps: float,\n",
    "    duration_sec: int = 8, parallel: int = 8, tcp_mss: int = 1200,\n",
    "    tolerance_pct: float = 10.0,\n",
    "    pin_path_flows: list | None = None, # This will be built by our new Ryu helper\n",
    "    protocol: str = \"tcp\",\n",
    "    udp_bw_mbps: float = 50.0,\n",
    "    udp_len_bytes: int = 1200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Verifies a QoS flow using iperf.\n",
    "    This version is adapted for Ryu:\n",
    "    - Uses dpid (int) and flow_cookie (int) to find the flow.\n",
    "    - Uses Ryu-compatible pinning flows.\n",
    "    \"\"\"\n",
    "    base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "    \n",
    "    # Resolve actors (This logic is OVS/Mininet based, no change needed)\n",
    "    #iface = get_iface_for_port(f\"of:{queue_device_id:016x}\", queue_port_no) # get_iface still needs ONOS format\n",
    "    iface = get_iface_for_port(queue_device_id, queue_port_no)\n",
    "    client_pid = get_mininet_host_pid(ip_to_host[src_ip])\n",
    "    server_pid = get_mininet_host_pid(ip_to_host[dst_ip])\n",
    "    proto = protocol.lower()\n",
    "    \n",
    "    if proto == \"tcp\":\n",
    "        extra_args = f\"-P {parallel} -M {tcp_mss}\"\n",
    "    else:\n",
    "        extra_args = f\"-u -b {udp_bw_mbps}M -l {udp_len_bytes} -P {max(1, parallel)}\"\n",
    "\n",
    "    # --- 1. Install Pinning Flows ---\n",
    "    # pin_path_flows is the *plan* (a list of dicts)\n",
    "    # We need to install them and get their cookies for cleanup\n",
    "    pushed_pin_flows = [] # This will store the actual flow dicts\n",
    "    if pin_path_flows:\n",
    "        pushed_pin_flows = install_pins_from_plan_ryu(\n",
    "            pins=pin_path_flows,\n",
    "            src_ip=src_ip, dst_ip=dst_ip,\n",
    "            dst_port=dst_port, protocol=protocol\n",
    "        )\n",
    "        print(f\"Installed {len(pushed_pin_flows)} pinning flows.\")\n",
    "\n",
    "    try:\n",
    "        # (Offload guard, iperf server start... all fine)\n",
    "        try: _sudo(f\"ethtool -K {shlex.quote(iface)} gro off gso off tso off\", timeout=6)\n",
    "        except Exception: pass\n",
    "\n",
    "        # 2) Snapshots before\n",
    "        flow_before = get_flow_by_cookie(base_url, flow_device_id, flow_cookie)\n",
    "        f_pkts0, f_bytes0 = _ryu_flow_counters(flow_before)\n",
    "        q0 = snapshot_queue(iface, queue_id) # OVS command, no change\n",
    "\n",
    "        # 3) Run iperf\n",
    "        start_iperf_server(server_pid, dst_port)\n",
    "        time.sleep(0.6)\n",
    "        t0 = time.time()\n",
    "        iptxt = _iperf_text_summary(client_pid, dst_ip, dst_port, duration_sec, extra_args=extra_args)\n",
    "        t1 = time.time()\n",
    "        stop_iperf_server(server_pid, dst_port)\n",
    "        time.sleep(0.8)\n",
    "\n",
    "        # 4) Snapshots after\n",
    "        flow_after = get_flow_by_cookie(base_url, flow_device_id, flow_cookie)\n",
    "        f_pkts1, f_bytes1 = _ryu_flow_counters(flow_after)\n",
    "        q1 = snapshot_queue(iface, queue_id) # OVS command, no change\n",
    "\n",
    "        # 5) Deltas (This logic is unchanged)\n",
    "        elapsed = max(0.001, t1 - t0)\n",
    "        q_bytes = None if (q0.get(\"tx_bytes\") is None or q1.get(\"tx_bytes\") is None) else (q1[\"tx_bytes\"] - q0[\"tx_bytes\"])\n",
    "        q_mbps  = (q_bytes * 8 / elapsed / 1e6) if q_bytes is not None else None\n",
    "        f_pkts_delta = (f_pkts1 - f_pkts0) if (f_pkts0 is not None and f_pkts1 is not None) else None\n",
    "        f_bytes_delta = (f_bytes1 - f_bytes0) if (f_bytes0 is not None and f_bytes1 is not None) else None\n",
    "        f_mbps = (f_bytes_delta * 8 / elapsed / 1e6) if (f_bytes_delta is not None) else None\n",
    "\n",
    "        # 6) Decision (This logic is unchanged)\n",
    "        rate_ok   = (q_mbps is not None) and (abs(q_mbps - target_mbps) <= (tolerance_pct/100.0)*target_mbps)\n",
    "        packets_ok= (f_pkts_delta or 0) > 0\n",
    "        verdict   = \"PASS\" if (rate_ok and packets_ok) else \"FAIL\"\n",
    "\n",
    "        # (Print summary is unchanged)\n",
    "        print(\"\\n=== QoS FLOW VERIFICATION ===\")\n",
    "        print(f\"Flow (cookie:{flow_cookie}) @ DPID {flow_device_id}  → selector should match {protocol} dst {dst_port} to {dst_ip}\")\n",
    "        print(f\"Egress iface={iface} queue={queue_id}  target≈{target_mbps:.3f} Mbps  tol=±{tolerance_pct:.0f}%\")\n",
    "        iperf_mbps_str = f\"{iptxt['sender_mbps']:.3f}\" if iptxt['sender_mbps'] is not None else \"N/A\"\n",
    "        iperf_bytes_str = f\"{iptxt['bytes_sent']}\" if iptxt['bytes_sent'] is not None else \"N/A\"\n",
    "        print(f\"iperf sender: {iperf_mbps_str} Mbps  bytes≈{iperf_bytes_str}\")\n",
    "        #print(f\"iperf sender: {iptxt['sender_mbps']:.3f} Mbps  bytes≈{iptxt['bytes_sent']}\")\n",
    "        print(f\"Queue Δbytes={q_bytes} over {elapsed:.3f}s  → queue_measured≈{(q_mbps or 0):.3f} Mbps\")\n",
    "        print(f\"Flow Δ: packets={f_pkts_delta} bytes={f_bytes_delta}  (flow_measured≈{(f_mbps or 0):.3f} Mbps)\")\n",
    "        print(f\"Queue caps (min/max): {q0['min_rate']} / {q0['max_rate']}  →  {q1['min_rate']} / {q1['max_rate']}\")\n",
    "        print(\"VERDICT:\", verdict)\n",
    "        \n",
    "        return {\n",
    "        \"elapsed_sec\": elapsed,\n",
    "        \"iperf_sender_mbps\": iptxt[\"sender_mbps\"],\n",
    "        \"queue_measured_mbps\": q_mbps,\n",
    "        \"queue_delta_bytes\": q_bytes,\n",
    "        \"flow_delta_packets\": f_pkts_delta,\n",
    "        \"flow_delta_bytes\": f_bytes_delta,\n",
    "        \"flow_measured_mbps\": f_mbps,\n",
    "        \"verdict\": verdict\n",
    "    }\n",
    "    \n",
    "    finally:\n",
    "        # --- 7. Always Clean Up Pinning Flows ---\n",
    "        print(f\"Cleaning up {len(pushed_pin_flows)} pinning flows...\")\n",
    "        unpin_path_ryu(pushed_pin_flows)\n",
    "\n",
    "# --- Ryu-compatible Pinning Functions ---\n",
    "\n",
    "def _pin_selector_ryu(direction: str, protocol: str, src_ip: str, dst_ip: str, dst_port: int):\n",
    "    \"\"\"Builds a Ryu-compatible match dictionary.\"\"\"\n",
    "    is_udp = protocol.lower() == \"udp\"\n",
    "    match = {\n",
    "        \"eth_type\": 2048, # 0x0800\n",
    "        \"ip_proto\": 17 if is_udp else 6\n",
    "    }\n",
    "    \n",
    "    if direction == \"forward\":\n",
    "        match[\"ipv4_src\"] = src_ip\n",
    "        match[\"ipv4_dst\"] = dst_ip\n",
    "        if is_udp:\n",
    "            match[\"udp_dst\"] = int(dst_port)\n",
    "        else:\n",
    "            match[\"tcp_dst\"] = int(dst_port)\n",
    "    else: # reverse\n",
    "        match[\"ipv4_src\"] = dst_ip\n",
    "        match[\"ipv4_dst\"] = src_ip\n",
    "        if is_udp:\n",
    "            match[\"udp_src\"] = int(dst_port)\n",
    "        else:\n",
    "            match[\"tcp_src\"] = int(dst_port)\n",
    "    return match\n",
    "\n",
    "def install_pins_from_plan_ryu(pins: list, src_ip: str, dst_ip: str, dst_port: int, protocol: str) -> list:\n",
    "    \"\"\"\n",
    "    Install the pin flows (Ryu format) and return the list of installed flows.\n",
    "    \"\"\"\n",
    "    base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "    pushed_flows = []\n",
    "    \n",
    "    for p in pins:\n",
    "        # dpid is integer, e.g., 1\n",
    "        dev_dpid = int(p[\"deviceId\"].split(\":\")[-1]) \n",
    "        outp = str(p[\"out_port\"])\n",
    "        direction = p[\"direction\"]\n",
    "        \n",
    "        flow_body = {\n",
    "            \"dpid\": dev_dpid,\n",
    "            \"priority\": 65000, # High priority\n",
    "            \"cookie\": secrets.randbits(64), # Unique cookie for temp rules\n",
    "            \"match\": _pin_selector_ryu(direction, protocol, src_ip, dst_ip, dst_port),\n",
    "            \"actions\": [\n",
    "                {\"type\": \"OUTPUT\", \"port\": int(outp)}\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            add_flow(base_url, flow_body)\n",
    "            pushed_flows.append(flow_body) # Store the whole flow for deletion\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to install pin flow: {e}\")\n",
    "            \n",
    "    return pushed_flows\n",
    "\n",
    "def unpin_path_ryu(pushed_flows: list):\n",
    "    \"\"\"\n",
    "    Remove all pinned rules using the 'delete_flow' helper from [cell 5].\n",
    "    \"\"\"\n",
    "    base_url = f\"http://{RYU_CONTROLLER_IP}:{RYU_CONTROLLER_PORT}\"\n",
    "    for flow in pushed_flows:\n",
    "        delete_flow(base_url, flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2538,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New helpers to satisfy the “missing src/dst” rules ---\n",
    "\n",
    "def choose_dst_for_port(device_id: str, port_no: int) -> str:\n",
    "    \"\"\"\n",
    "    Pick a destination IP that makes forward traffic EXIT on (device_id, port_no).\n",
    "    Uses your diamond wiring.\n",
    "    \"\"\"\n",
    "    # Final-hop host ports first\n",
    "    if device_id == SW_OF[\"1\"] and port_no == 3:  # s1 -> h1\n",
    "        return HOSTS[\"h1\"]\n",
    "    if device_id == SW_OF[\"1\"] and port_no == 4:  # s1 -> h2\n",
    "        return HOSTS[\"h2\"]\n",
    "    if device_id == SW_OF[\"4\"] and port_no == 3:  # s4 -> h3\n",
    "        return HOSTS[\"h3\"]\n",
    "    if device_id == SW_OF[\"4\"] and port_no == 4:  # s4 -> h4\n",
    "        return HOSTS[\"h4\"]\n",
    "\n",
    "    # Inter-switch egress: choose a host \"behind\" the far side so packets must traverse this link\n",
    "    # s1:1->s2 (right), s1:2->s3 (right) → pick a right-side host (h3 default)\n",
    "    if device_id == SW_OF[\"1\"] and port_no in (1, 2):\n",
    "        return HOSTS[\"h3\"]\n",
    "    # s2:2->s4 (right) → pick right-side host; s2:1->s1 (left) → pick left-side host\n",
    "    if device_id == SW_OF[\"2\"] and port_no == 2:\n",
    "        return HOSTS[\"h3\"]\n",
    "    if device_id == SW_OF[\"2\"] and port_no == 1:\n",
    "        return HOSTS[\"h1\"]\n",
    "    # s3:2->s4 (right) → right host; s3:1->s1 (left) → left host\n",
    "    if device_id == SW_OF[\"3\"] and port_no == 2:\n",
    "        return HOSTS[\"h3\"]\n",
    "    if device_id == SW_OF[\"3\"] and port_no == 1:\n",
    "        return HOSTS[\"h1\"]\n",
    "    # s4 inter-switch (rare in your tests): port1->s2 (left) pick left host; port2->s3 (left) pick left host\n",
    "    if device_id == SW_OF[\"4\"] and port_no in (1, 2):\n",
    "        return HOSTS[\"h1\"]\n",
    "\n",
    "    # Fallback\n",
    "    return HOSTS[\"h3\"]\n",
    "\n",
    "def fill_missing_endpoints(plan: dict, device_id: str, port_no: int) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Apply your policy:\n",
    "      - If only dst is present → choose src that forces egress at (device,port)\n",
    "      - If only src is present → choose dst that forces egress at (device,port)\n",
    "      - If both missing → pick both to force egress at (device,port)\n",
    "      - Ensure src != dst\n",
    "    \"\"\"\n",
    "    src = plan.get(\"src_ip\")\n",
    "    dst = plan.get(\"dst_ip\")\n",
    "\n",
    "    # Normalize empty strings to None\n",
    "    src = src or None\n",
    "    dst = dst or None\n",
    "\n",
    "    if dst is None:\n",
    "        dst = choose_dst_for_port(device_id, port_no)\n",
    "    if src is None:\n",
    "        src = choose_src_for_port(device_id, port_no)\n",
    "\n",
    "    # If they accidentally collide, flip src to the opposite side\n",
    "    if src == dst:\n",
    "        # If dst is on right side, move src to left; else move to right\n",
    "        try:\n",
    "            dst_edge, _ = _edge_for_ip(dst)\n",
    "        except Exception:\n",
    "            dst_edge = SW_OF[\"4\"]  # assume right if unknown\n",
    "        src = HOSTS[\"h1\"] if dst_edge in (SW_OF[\"3\"], SW_OF[\"4\"]) else HOSTS[\"h3\"]\n",
    "\n",
    "    return src, dst\n",
    "\n",
    "def _strip32(ip: str) -> str:\n",
    "    return ip.split(\"/\", 1)[0]\n",
    "\n",
    "def find_edge_for_ip(dst_ip: str):\n",
    "    \"\"\"Return (deviceId, portNo) for the host that owns dst_ip.\n",
    "       Prefer static map; fall back to ONOS /hosts if needed.\"\"\"\n",
    "    ip = _strip32(dst_ip)\n",
    "    if ip in HOST_ATTACH:\n",
    "        return HOST_ATTACH[ip]\n",
    "    # Optional fallback via ONOS:\n",
    "    try:\n",
    "        js = onos_get(f\"hosts?ip={ip}\")\n",
    "        lst = js.get(\"hosts\", js if isinstance(js, list) else [])\n",
    "        if lst:\n",
    "            locs = lst[0].get(\"locations\") or lst[0].get(\"location\") or []\n",
    "            if locs:\n",
    "                dev = locs[0].get(\"elementId\") or locs[0].get(\"device\") or locs[0].get(\"element\")\n",
    "                port = int(locs[0].get(\"port\"))\n",
    "                return dev, port\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise RuntimeError(f\"Could not infer edge device/port for dst_ip={dst_ip}\")\n",
    "\n",
    "def infer_enforcement_point(plan):\n",
    "    \"\"\"\n",
    "    Decide where to enforce if device/port not fully specified.\n",
    "    Preference:\n",
    "      1) If plan.device_id & plan.port_no given → use those\n",
    "      2) Else enforce at the final hop to the destination host (works for h1..h4).\n",
    "    \"\"\"\n",
    "    device_id, port_no = plan.get(\"device_id\"), plan.get(\"port_no\")\n",
    "    if device_id and port_no:\n",
    "        return device_id, port_no\n",
    "    return find_edge_for_ip(plan[\"dst_ip\"])\n",
    "\n",
    "def _edge_for_ip(ip: str):\n",
    "    ip = ip.split(\"/\", 1)[0]\n",
    "    if ip in HOST_ATTACH:\n",
    "        return HOST_ATTACH[ip]\n",
    "    # Optional: try ONOS if not in static map (kept simple)\n",
    "    try:\n",
    "        js = onos_get(f\"hosts?ip={ip}\")\n",
    "        lst = js.get(\"hosts\", js if isinstance(js, list) else [])\n",
    "        if lst:\n",
    "            loc = (lst[0].get(\"locations\") or lst[0].get(\"location\") or [])[0]\n",
    "            return loc[\"elementId\"], int(loc[\"port\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    raise RuntimeError(f\"No edge mapping for IP {ip}\")\n",
    "\n",
    "def _is(dev: str, n: str) -> bool:\n",
    "    \"\"\"dev == SW_OF[n]?\"\"\"\n",
    "    return dev == SW_OF[n]\n",
    "\n",
    "def choose_src_for_port(device_id: str, port_no: int) -> str:\n",
    "    \"\"\"\n",
    "    Pick a source IP that will make *forward* traffic egress on (device_id, port_no).\n",
    "    This aligns the traffic direction with your enforcement point so your queue is actually exercised.\n",
    "    \"\"\"\n",
    "    # s1: final-host ports 3/4 → pick a source on the right (h3)\n",
    "    if _is(device_id, \"1\"):\n",
    "        if port_no in (3, 4):        # s1 -> h1/h2\n",
    "            return HOSTS[\"h3\"]\n",
    "        elif port_no in (1, 2):      # s1 -> s2/s3 (inter-switch)\n",
    "            return HOSTS[\"h1\"]       # left host causes egress at s1\n",
    "    # s2: port2 to s4 → src on left; port1 to s1 → src on right\n",
    "    if _is(device_id, \"2\"):\n",
    "        return HOSTS[\"h1\"] if port_no == 2 else HOSTS[\"h3\"]\n",
    "    # s3: port2 to s4 → src on left; port1 to s1 → src on right\n",
    "    if _is(device_id, \"3\"):\n",
    "        return HOSTS[\"h1\"] if port_no == 2 else HOSTS[\"h3\"]\n",
    "    # s4: final-host ports 3/4 → src on left; inter-switch (1/2) → src on right\n",
    "    if _is(device_id, \"4\"):\n",
    "        if port_no in (3, 4):        # s4 -> h3/h4\n",
    "            return HOSTS[\"h1\"]\n",
    "        elif port_no in (1, 2):      # s4 -> s2/s3\n",
    "            return HOSTS[\"h3\"]\n",
    "    # Fallback\n",
    "    return HOSTS[\"h1\"]\n",
    "\n",
    "def make_pin_path_flows(device_id: str, port_no: int,\n",
    "                        src_ip: str, dst_ip: str, dst_port: int,\n",
    "                        protocol: str = \"tcp\"):\n",
    "    \"\"\"\n",
    "    Build a pinning plan so src_ip -> dst_ip forward traffic *must* traverse (device_id, port_no).\n",
    "\n",
    "    Returns a list of dicts: [{\"deviceId\":..., \"out_port\":..., \"direction\":\"forward|reverse\"}, ...]\n",
    "    - For TCP: includes reverse pins (ACK path) to reduce variability.\n",
    "    - For UDP: forward-only pins (reverse is optional, skipped here).\n",
    "    \"\"\"\n",
    "    proto = protocol.lower()\n",
    "    is_udp = (proto == \"udp\")\n",
    "\n",
    "    pins = []\n",
    "    src_edge_dev, src_edge_port = _edge_for_ip(src_ip)\n",
    "    dst_edge_dev, dst_edge_port = _edge_for_ip(dst_ip)\n",
    "\n",
    "    def add(dev, outp, direction):\n",
    "        pins.append({\"deviceId\": dev, \"out_port\": int(outp), \"direction\": direction})\n",
    "\n",
    "    # ----- cases by enforcement point -----\n",
    "\n",
    "    # s4 final hop to host (port 3 or 4: h3/h4)\n",
    "    if _is(device_id, \"4\") and port_no in (3, 4):\n",
    "        # Forward: steer from the left toward s4 via s1->s3->s4\n",
    "        # If source is already on the right (h3/h4), nothing to pin on the way in.\n",
    "        if _is(src_edge_dev, \"1\"):\n",
    "            #add(SW_OF[\"1\"], 2, \"forward\")   # s1 -> s3\n",
    "            #add(SW_OF[\"3\"], 2, \"forward\")   # s3 -> s4\n",
    "            add(SW_OF[\"1\"], 1, \"forward\")   # s1 -> s2 (port 1)\n",
    "            add(SW_OF[\"2\"], 2, \"forward\")   # s2 -> s4 (port 2)\n",
    "        elif _is(src_edge_dev, \"4\"):\n",
    "            pass  # already on s4 side\n",
    "        elif _is(src_edge_dev, \"2\"):\n",
    "            add(SW_OF[\"2\"], 2, \"forward\")   # s2 -> s4\n",
    "        elif _is(src_edge_dev, \"3\"):\n",
    "            add(SW_OF[\"3\"], 2, \"forward\")   # s3 -> s4\n",
    "\n",
    "        # Reverse (TCP): s4 -> s3 -> s1 -> host(src)\n",
    "        if not is_udp:\n",
    "            if _is(src_edge_dev, \"1\"):\n",
    "                #add(SW_OF[\"4\"], 2, \"reverse\")   # s4 -> s3\n",
    "                #add(SW_OF[\"3\"], 1, \"reverse\")   # s3 -> s1\n",
    "                #add(SW_OF[\"1\"], src_edge_port, \"reverse\")  # s1 -> h1/h2\n",
    "                add(SW_OF[\"4\"], 1, \"reverse\")   # s4 -> s2 (port 1)\n",
    "                add(SW_OF[\"2\"], 1, \"reverse\")   # s2 -> s1 (port 1)\n",
    "                add(SW_OF[\"1\"], src_edge_port, \"reverse\")  # s1 -> h1/h2\n",
    "            elif _is(src_edge_dev, \"4\"):\n",
    "                add(SW_OF[\"4\"], src_edge_port, \"reverse\")  # local s4 -> h3/h4\n",
    "\n",
    "    # s1 final hop to host (port 3 or 4: h1/h2)\n",
    "    elif _is(device_id, \"1\") and port_no in (3, 4):\n",
    "        # Forward: steer from the right toward s1 via s4->s3->s1\n",
    "        if _is(src_edge_dev, \"4\"):\n",
    "            add(SW_OF[\"4\"], 2, \"forward\")   # s4 -> s3\n",
    "            add(SW_OF[\"3\"], 1, \"forward\")   # s3 -> s1\n",
    "        elif _is(src_edge_dev, \"1\"):\n",
    "            pass  # already on s1 side\n",
    "\n",
    "        # Reverse (TCP): s1 -> s3 -> s4 -> host(src)\n",
    "        if not is_udp:\n",
    "            if _is(src_edge_dev, \"4\"):\n",
    "                add(SW_OF[\"1\"], 2, \"reverse\")   # s1 -> s3\n",
    "                add(SW_OF[\"3\"], 2, \"reverse\")   # s3 -> s4\n",
    "                add(SW_OF[\"4\"], src_edge_port, \"reverse\")  # s4 -> h3/h4\n",
    "            elif _is(src_edge_dev, \"1\"):\n",
    "                add(SW_OF[\"1\"], src_edge_port, \"reverse\")  # local s1 -> h1/h2\n",
    "\n",
    "    # s1 inter-switch egress\n",
    "    elif _is(device_id, \"1\") and port_no == 2:\n",
    "        # Forward: use s1->s3->s4 path\n",
    "        add(SW_OF[\"1\"], 2, \"forward\")\n",
    "        add(SW_OF[\"3\"], 2, \"forward\")\n",
    "        # Reverse (TCP): s4 -> s3 -> s1 -> host(src)\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"4\"], 2, \"reverse\")\n",
    "            add(SW_OF[\"3\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "    elif _is(device_id, \"1\") and port_no == 1:\n",
    "        # Forward: use s1->s2->s4 path\n",
    "        add(SW_OF[\"1\"], 1, \"forward\")\n",
    "        add(SW_OF[\"2\"], 2, \"forward\")\n",
    "        # Reverse (TCP): s4 -> s2 -> s1 -> host(src)\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"4\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"2\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "\n",
    "    # s2 inter-switch egress\n",
    "    elif _is(device_id, \"2\") and port_no == 2:\n",
    "        # Forward: s1 -> s2 -> s4\n",
    "        add(SW_OF[\"1\"], 1, \"forward\")\n",
    "        add(SW_OF[\"2\"], 2, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"4\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"2\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "    elif _is(device_id, \"2\") and port_no == 1:\n",
    "        # Forward: s4 -> s2 -> s1\n",
    "        add(SW_OF[\"4\"], 1, \"forward\")\n",
    "        add(SW_OF[\"2\"], 1, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"1\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"2\"], 2, \"reverse\")\n",
    "            add(SW_OF[\"4\"], src_edge_port, \"reverse\")\n",
    "\n",
    "    # s3 inter-switch egress\n",
    "    elif _is(device_id, \"3\") and port_no == 2:\n",
    "        # Forward: s1 -> s3 -> s4\n",
    "        add(SW_OF[\"1\"], 2, \"forward\")\n",
    "        add(SW_OF[\"3\"], 2, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"4\"], 2, \"reverse\")\n",
    "            add(SW_OF[\"3\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "    elif _is(device_id, \"3\") and port_no == 1:\n",
    "        # Forward: s4 -> s3 -> s1\n",
    "        add(SW_OF[\"4\"], 2, \"forward\")\n",
    "        add(SW_OF[\"3\"], 1, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"1\"], 2, \"reverse\")\n",
    "            add(SW_OF[\"3\"], 2, \"reverse\")\n",
    "            add(SW_OF[\"4\"], src_edge_port, \"reverse\")\n",
    "\n",
    "    # s4 inter-switch egress (rare in your tests but supported)\n",
    "    elif _is(device_id, \"4\") and port_no in (1, 2):\n",
    "        # To use s4->(2?3?) link in *forward* direction, src should be on the right.\n",
    "        # Forward: none if src is already on s4; else steer toward s4 via right path\n",
    "        if _is(src_edge_dev, \"1\"):\n",
    "            add(SW_OF[\"1\"], 2, \"forward\")  # prefer via s3\n",
    "            add(SW_OF[\"3\"], 2, \"forward\")\n",
    "        # Reverse (TCP): symmetric back to src\n",
    "        if not is_udp:\n",
    "            if port_no == 1:\n",
    "                add(SW_OF[\"4\"], 1, \"reverse\"); add(SW_OF[\"2\"], 1, \"reverse\")\n",
    "                add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "            else:\n",
    "                add(SW_OF[\"4\"], 2, \"reverse\"); add(SW_OF[\"3\"], 1, \"reverse\")\n",
    "                add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "\n",
    "    else:\n",
    "        # Fallback: route via s1->s3->s4 with TCP reverse symmetry\n",
    "        add(SW_OF[\"1\"], 2, \"forward\")\n",
    "        add(SW_OF[\"3\"], 2, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(SW_OF[\"4\"], 2, \"reverse\")\n",
    "            add(SW_OF[\"3\"], 1, \"reverse\")\n",
    "            add(SW_OF[\"1\"], src_edge_port, \"reverse\")\n",
    "\n",
    "    # For UDP, we intentionally omit reverse pins (ACK-less).\n",
    "    # if is_udp:\n",
    "    #     pins = [p for p in pins if p[\"direction\"] == \"forward\"]\n",
    "\n",
    "    # --- Never overshadow the QoS rule on the enforcement port ---\n",
    "    # Drop any FORWARD pin that sits on the enforcement device+port.\n",
    "    pins = [p for p in pins\n",
    "            if not (p[\"direction\"] == \"forward\"\n",
    "                    and p[\"deviceId\"] == device_id\n",
    "                    and int(p[\"out_port\"]) == int(port_no))]\n",
    "\n",
    "    # For UDP, keep forward-only (already done above)\n",
    "    if is_udp:\n",
    "        pins = [p for p in pins if p[\"direction\"] == \"forward\"]\n",
    "\n",
    "    # De-duplicate [(dev,port,direction)] triples, keep first\n",
    "    seen, uniq = set(), []\n",
    "    for p in pins:\n",
    "        key = (p[\"deviceId\"], int(p[\"out_port\"]), p[\"direction\"])\n",
    "        if key in seen: \n",
    "            continue\n",
    "        seen.add(key); uniq.append(p)\n",
    "    return uniq\n",
    "    #return pins\n",
    "\n",
    "def normalize_device_id(dev: str | None) -> str | None:\n",
    "    if not dev:\n",
    "        return dev\n",
    "    dev = dev.strip()\n",
    "\n",
    "    # openflow:<decimal>\n",
    "    m = re.fullmatch(r'openflow:(\\d+)', dev, flags=re.I)\n",
    "    if m:\n",
    "        return f\"of:{int(m.group(1)):016x}\"\n",
    "\n",
    "    # openflow:<hex> or openflow:0x<hex>\n",
    "    m = re.fullmatch(r'openflow:(?:0x)?([0-9a-fA-F]+)', dev, flags=re.I)\n",
    "    if m:\n",
    "        return f\"of:{int(m.group(1), 16):016x}\"\n",
    "\n",
    "    # of:<decimal>\n",
    "    m = re.fullmatch(r'of:(\\d+)', dev, flags=re.I)\n",
    "    if m:\n",
    "        return f\"of:{int(m.group(1)):016x}\"\n",
    "\n",
    "    # of:<hex already>\n",
    "    if dev.lower().startswith(\"of:\"):\n",
    "        return \"of:\" + dev[3:].lower()\n",
    "\n",
    "    return dev\n",
    "\n",
    "def parse_intent_text(slicing_info, src_ip, dst_ip):\n",
    "\n",
    "                slicing_queue_id = slicing_info['queue_id'] if (slicing_info['queue_id']) != \"\" else 1\n",
    "                proto = slicing_info['traffic_type'] if \"udp\" in slicing_info['traffic_type'] else (\"tcp\" if \"tcp\" in slicing_info['traffic_type'] or \"http\" in slicing_info['traffic_type'] else \"tcp\")\n",
    "                slicing_l4_port = slicing_info['l4_port'] if slicing_info['l4_port'] != \"\" else 80\n",
    "                port_no = extract_port_number(slicing_info['port_id'])\n",
    "                device_id = extract_switch_id(slicing_info['switch_id'])\n",
    "                device_id = normalize_device_id (device_id)\n",
    "\n",
    "                return {\n",
    "                    \"protocol\": proto,\n",
    "                    \"dst_port\": slicing_l4_port,\n",
    "                    \"device_id\": device_id,   # may be None\n",
    "                    \"port_no\": port_no,       # may be None, it means output interface number\n",
    "                    \"queue_id\": slicing_queue_id,\n",
    "                    \"src_ip\": src_ip,         # may be None\n",
    "                    \"dst_ip\": dst_ip,         # may be None\n",
    "                }\n",
    "\n",
    "# [cell 2023]\n",
    "# PASTE THIS ENTIRE FUNCTION, REPLACING THE OLD ONE\n",
    "\n",
    "def intent_to_verifier_args(slicing_info, old_src_ip, old_dst_ip, main_flow_cookie: int, target_mbps: float = 4.0, translated_flow_rule: dict = None):\n",
    "    \"\"\"\n",
    "    Parses the slicing info and the main flow rule to create the\n",
    "    correct arguments for the verify_qos_flow_with_iperf function.\n",
    "    \n",
    "    This version includes the logic to detect 'in_port' from the \n",
    "    translated_flow_rule and lock the test's src_ip to the correct host.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Parse the text intent\n",
    "    plan = parse_intent_text(slicing_info, old_src_ip, old_dst_ip)\n",
    "    \n",
    "    # 2. Find the enforcement point (device/port for the queue)\n",
    "    # (infer_enforcement_point is fine)\n",
    "    device_id, port_no = infer_enforcement_point(plan)\n",
    "    plan[\"device_id\"], plan[\"port_no\"] = device_id, port_no\n",
    "\n",
    "    # --- START CORRECTED LOGIC ---\n",
    "    # Check if the main rule has an 'in_port' and use it to find the *correct* source host\n",
    "    if translated_flow_rule:\n",
    "        match = translated_flow_rule.get(\"match\", {})\n",
    "        in_port = match.get(\"in_port\")\n",
    "        \n",
    "        if in_port:\n",
    "            # We have an in_port. Find which host connects to this (device, in_port).\n",
    "            # HOST_ATTACH is in {ip: (dev_str, port_num)} format\n",
    "            # We need to build a reverse map: {(dev_str, port_num): ip}\n",
    "            \n",
    "            # Build the reverse map (HOST_ATTACH is defined in this cell)\n",
    "            port_to_ip = {v: k for k, v in HOST_ATTACH.items()}\n",
    "            \n",
    "            # Check if this (device, in_port) corresponds to a known host\n",
    "            # The 'device_id' variable IS the correct 'of:...' string (e.g., \"of:0000000000000001\")\n",
    "            host_ip = port_to_ip.get( (device_id, int(in_port)) )\n",
    "            \n",
    "            if host_ip:\n",
    "                print(f\"[INFO] Rule has in_port:{in_port}, locking source host to {host_ip}\")\n",
    "                plan[\"src_ip\"] = host_ip # Set the source IP\n",
    "            else:\n",
    "                # This can happen if in_port is an inter-switch port\n",
    "                print(f\"[WARN] Rule has in_port:{in_port}, but no host is mapped to ({device_id}, {in_port}).\")\n",
    "    # --- END CORRECTED LOGIC ---\n",
    "\n",
    "    # 3. Fill any remaining missing endpoints\n",
    "    # This will now use plan[\"src_ip\"] if we set it, or guess if we didn't.\n",
    "    src_ip, dst_ip = fill_missing_endpoints(plan, device_id, port_no)\n",
    "    plan[\"src_ip\"], plan[\"dst_ip\"] = src_ip, dst_ip\n",
    "\n",
    "    # 4. Convert 'of:...' string to integer dpid for Ryu functions\n",
    "    # (split on ':', take last part, convert from hex)\n",
    "    dpid = int(device_id.split(\":\")[-1], 16) \n",
    "\n",
    "    # 5. Build verifier args\n",
    "    args = {\n",
    "        \"flow_device_id\":  dpid,\n",
    "        \"flow_cookie\":     main_flow_cookie,\n",
    "        \"queue_device_id\": dpid,\n",
    "        \"queue_port_no\":   port_no,\n",
    "        \"queue_id\":        plan[\"queue_id\"],\n",
    "        \"src_ip\":          src_ip,\n",
    "        \"dst_ip\":          dst_ip,\n",
    "        \"dst_port\":        plan[\"dst_port\"],\n",
    "        \"target_mbps\":     target_mbps,\n",
    "        \"protocol\":        plan[\"protocol\"]\n",
    "    }\n",
    "\n",
    "    # 6. Build Pin plan\n",
    "    pins_plan = make_pin_path_flows(\n",
    "        device_id, \n",
    "        port_no, \n",
    "        src_ip, \n",
    "        dst_ip, \n",
    "        plan[\"dst_port\"],  # <-- Fixed potential typo here too\n",
    "        plan[\"protocol\"]\n",
    "    )\n",
    "    \n",
    "    return args, pins_plan, plan\n",
    "\n",
    "def intent_to_verifier_args_old2(slicing_info, old_src_ip, old_dst_ip, main_flow_cookie: int, target_mbps: float = 4.0, translated_flow_rule: dict = None):\n",
    "    \n",
    "    # (parse_intent_text is fine)\n",
    "    plan = parse_intent_text(slicing_info, old_src_ip, old_dst_ip)\n",
    "    \n",
    "    # (infer_enforcement_point is fine)\n",
    "    device_id, port_no = infer_enforcement_point(plan)\n",
    "    plan[\"device_id\"], plan[\"port_no\"] = device_id, port_no\n",
    "\n",
    "    # --- START NEW LOGIC ---\n",
    "    # Check if the main rule has an 'in_port' and use it to find the *correct* source host\n",
    "    if translated_flow_rule:\n",
    "        match = translated_flow_rule.get(\"match\", {})\n",
    "        in_port = match.get(\"in_port\")\n",
    "        \n",
    "        if in_port:\n",
    "            # We have an in_port. Find which host connects to this (device, in_port).\n",
    "            # HOST_ATTACH is in {ip: (dev_str, port_num)} format\n",
    "            # We need to build a reverse map: {(dev_str, port_num): ip}\n",
    "            \n",
    "            # (SW_OF and HOST_ATTACH are defined earlier in this cell)\n",
    "            rule_device_str = SW_OF.get(str(device_id.split(\":\")[-1])) # \"of:...\"\n",
    "            \n",
    "            # Build the reverse map\n",
    "            port_to_ip = {v: k for k, v in HOST_ATTACH.items()}\n",
    "            \n",
    "            # Check if this (device, in_port) corresponds to a known host\n",
    "            host_ip = port_to_ip.get( (rule_device_str, int(in_port)) )\n",
    "            \n",
    "            if host_ip:\n",
    "                print(f\"[INFO] Rule has in_port:{in_port}, locking source host to {host_ip}\")\n",
    "                plan[\"src_ip\"] = host_ip # Set the source IP\n",
    "    \n",
    "    # --- END NEW LOGIC ---\n",
    "\n",
    "    # (fill_missing_endpoints is fine)\n",
    "    # This will now use plan[\"src_ip\"] if we set it, or guess if we didn't.\n",
    "    src_ip, dst_ip = fill_missing_endpoints(plan, device_id, port_no)\n",
    "    plan[\"src_ip\"], plan[\"dst_ip\"] = src_ip, dst_ip\n",
    "\n",
    "    # Convert 'of:...' string to integer dpid\n",
    "    dpid = int(device_id.split(\":\")[-1])\n",
    "\n",
    "    # 3) Build verifier args\n",
    "    # (Rest of the function is unchanged)\n",
    "    args = {\n",
    "        \"flow_device_id\":  dpid,\n",
    "        \"flow_cookie\":     main_flow_cookie, # <-- PASS COOKIE\n",
    "        \"queue_device_id\": dpid,\n",
    "        \"queue_port_no\":   port_no,\n",
    "        \"queue_id\":        plan[\"queue_id\"],\n",
    "        \"src_ip\":          src_ip,\n",
    "        \"dst_ip\":          dst_ip,\n",
    "        \"dst_port\":        plan[\"dst_port\"],\n",
    "        \"target_mbps\":     target_mbps,\n",
    "        \"protocol\":        plan[\"protocol\"]\n",
    "    }\n",
    "\n",
    "    # 4) Build Pin plan (This logic is OVS-agnostic and fine)\n",
    "    pins_plan = make_pin_path_flows(device_id, port_no, src_ip, dst_ip, plan[\"dst_port\"], plan[\"protocol\"])                                                                            \n",
    "    return args, pins_plan, plan\n",
    "    \n",
    "\n",
    "def intent_to_verifier_args_old(slicing_info, old_src_ip, old_dst_ip, main_flow_cookie: int, target_mbps: float = 4.0):\n",
    "    \n",
    "    # (parse_intent_text is fine)\n",
    "    plan = parse_intent_text(slicing_info, old_src_ip, old_dst_ip)\n",
    "\n",
    "    # (infer_enforcement_point is fine)\n",
    "    device_id, port_no = infer_enforcement_point(plan)\n",
    "    plan[\"device_id\"], plan[\"port_no\"] = device_id, port_no\n",
    "    \n",
    "    # (fill_missing_endpoints is fine)\n",
    "    src_ip, dst_ip = fill_missing_endpoints(plan, device_id, port_no)\n",
    "    plan[\"src_ip\"], plan[\"dst_ip\"] = src_ip, dst_ip\n",
    "\n",
    "    # Convert 'of:...' string to integer dpid\n",
    "    dpid = int(device_id.split(\":\")[-1])\n",
    "\n",
    "    # 3) Build verifier args\n",
    "    args = {\n",
    "        \"flow_device_id\":  dpid,\n",
    "        \"flow_cookie\":     main_flow_cookie, # <-- PASS COOKIE\n",
    "        \"queue_device_id\": dpid,\n",
    "        \"queue_port_no\":   port_no,\n",
    "        \"queue_id\":        plan[\"queue_id\"],\n",
    "        \"src_ip\":          src_ip,\n",
    "        \"dst_ip\":          dst_ip,\n",
    "        \"dst_port\":        plan[\"dst_port\"],\n",
    "        \"target_mbps\":     target_mbps,\n",
    "        \"protocol\":        plan[\"protocol\"]\n",
    "    }\n",
    "\n",
    "    # 4) Build Pin plan (This logic is OVS-agnostic and fine)\n",
    "    pins_plan = make_pin_path_flows(device_id, port_no, src_ip, dst_ip, plan[\"dst_port\"], plan[\"protocol\"])\n",
    "    return args, pins_plan, plan\n",
    "\n",
    "def _pin_selector(direction: str, protocol: str, src_ip: str, dst_ip: str, dst_port: int):\n",
    "    \"\"\"Build protocol-aware 5-tuple for forward/reverse.\"\"\"\n",
    "    is_udp = protocol.lower() == \"udp\"\n",
    "    crit = [\n",
    "        {\"type\":\"ETH_TYPE\",\"ethType\":\"0x800\"},\n",
    "        {\"type\":\"IP_PROTO\",\"protocol\": 17 if is_udp else 6},\n",
    "    ]\n",
    "    if direction == \"forward\":\n",
    "        crit += [\n",
    "            {\"type\":\"IPV4_SRC\",\"ip\": f\"{src_ip}/32\"},\n",
    "            {\"type\":\"IPV4_DST\",\"ip\": f\"{dst_ip}/32\"},\n",
    "            ({\"type\":\"UDP_DST\",\"udpPort\": dst_port} if is_udp else {\"type\":\"TCP_DST\",\"tcpPort\": dst_port}),\n",
    "        ]\n",
    "    else:  # reverse\n",
    "        crit += [\n",
    "            {\"type\":\"IPV4_SRC\",\"ip\": f\"{dst_ip}/32\"},\n",
    "            {\"type\":\"IPV4_DST\",\"ip\": f\"{src_ip}/32\"},\n",
    "            ({\"type\":\"UDP_SRC\",\"udpPort\": dst_port} if is_udp else {\"type\":\"TCP_SRC\",\"tcpPort\": dst_port}),\n",
    "        ]\n",
    "    return {\"criteria\": crit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent = \"Forward TCP traffic on port 80 destined for 10.0.1.3 via interface 3, assigning it to queue 1 for prioritized handling in switch 4.\"\n",
    "# intent = \"In switch 4, traffic destined for 10.0.1.4 should use port 4.\"\n",
    "# intent =  \"In switch 4, block all IPv4 traffic from 10.0.1.1 to 10.0.1.4 with a high priority, ensuring the switch operates as a firewall.\"\n",
    "# intent = \"Drop all traffic from 10.0.1.3 on switch 2 while forwarding all other traffic normally.\"\n",
    "# intent = \"If interface 1 on node 2 receives UDP to port 80, pass via port 2, queue 1.\"\n",
    "# intent = \"If switch 1 receives TCP on port 4 to port 80, pass via interface 1, queue 0.\"\n",
    "# intent = \"In node 1, traffic destined for 10.0.1.2 should use port 4.\"\n",
    "# intent = \"In switch 2, traffic from port 1 should pass through port 2.\"\n",
    "# intent = \"Traffic from port 2 of switch 2 to 10.0.1.1 should use interface 1.\"\n",
    "intent = \"In switch 4, traffic from 10.0.1.1 to 10.0.1.4 should use output interface 4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = time.time()\n",
    "\n",
    "slicing_info = run_LLM_Slice(intent)\n",
    "\n",
    "#print(slicing_info)\n",
    "\n",
    "#create_two_queue_for_switch_handler(slicing_info)\n",
    "\n",
    "deployment_status, flow_id, device_id, translated_flow_rule, operational_flow_rule = end_to_end_IBN(intent)\n",
    "\n",
    "if 'use_queue' in slicing_info and slicing_info['use_queue'] == 1:\n",
    "    #Creating one queue\n",
    "    expected_queue_rate_mbps = 4.0\n",
    "    port_max = 100_000_000  # 100 Mbps cap for the port QoS root\n",
    "    min_rate = max_rate = int(expected_queue_rate_mbps * 1_000_000)\n",
    "    slicing_queue_id = slicing_info['queue_id'] if (slicing_info['queue_id']) != \"\" else 1\n",
    "    proto = slicing_info['traffic_type'] if \"udp\" in slicing_info['traffic_type'] else (\"tcp\" if \"tcp\" in slicing_info['traffic_type'] or \"http\" in slicing_info['traffic_type'] else \"tcp\")\n",
    "    slicing_l4_port = slicing_info['l4_port'] if slicing_info['l4_port'] != \"\" else 80\n",
    "    port_no = extract_port_number(slicing_info['port_id'])\n",
    "    \n",
    "    ensure_qos_cap(device_id, port_no, slicing_queue_id, min_bps=min_rate, max_bps=max_rate, port_cap_bps=100_000_000)\n",
    "\n",
    "global llm_caller_flag\n",
    "\n",
    "if (deployment_status == True):\n",
    "        proc_time_s = (time.time() - current_time)\n",
    "        print(\"\\n\\nSuccessfully translated and installed the rule in Ryu SDN Controller. Time taken: \", proc_time_s)\n",
    "        src_host, dst_host, src_ip, dst_ip = extract_host_and_ip_ryu(translated_flow_rule)\n",
    "        flow_rule_type, flow_rule_specificity = classify_ryu_flow_rule(translated_flow_rule)\n",
    "        \n",
    "        # Example of appending an intent to IntentStore\n",
    "        append_intent_to_store(\n",
    "            \"IntentStore_Ryu.jsonl\",\n",
    "            nl_intent=intent,\n",
    "            json_flow_rule=translated_flow_rule,\n",
    "            device_id=device_id,\n",
    "            flow_id=flow_id,\n",
    "            intent_type=flow_rule_type,\n",
    "            intent_specificity=flow_rule_specificity\n",
    "            )\n",
    "        \n",
    "        if (flow_rule_type== \"security\"):\n",
    "            #ping_count, candidate_src_ip, candidate_dst_ip, ping_output = ryu_assurance_for_security_intent(src_ip, dst_ip)\n",
    "            ping_count, candidate_src_ip, candidate_dst_ip, ping_output = ryu_assurance_for_security_intent(src_ip, dst_ip, device_id)\n",
    "            if (llm_caller_flag == 1):\n",
    "                print(\"\\nAsking LLM to generate corrective actions...\")\n",
    "                assurance_LLM_prompt = generate_corrective_action_prompt_ryu(intent, operational_flow_rule, device_id, ping_count,\n",
    "                                    candidate_src_ip, candidate_dst_ip, ping_output)\n",
    "                llm_response = Run_assurance_LLM (assurance_LLM_prompt)\n",
    "                print(llm_response)\n",
    "                parse_and_execute_corrective_actions_ryu(operational_flow_rule, llm_response, device_id)\n",
    "\n",
    "\n",
    "        elif (flow_rule_type== \"qos\"):\n",
    "            \n",
    "            target_mbps = 4.0 # Set your target\n",
    "            \n",
    "            # flow_id from end_to_end_IBN is the cookie\n",
    "            main_flow_cookie = flow_id \n",
    "            \n",
    "            # 1) Resolve intent\n",
    "            # Pass the cookie to the args builder\n",
    "            #args, pins_plan, plan = intent_to_verifier_args(slicing_info, src_ip, dst_ip, main_flow_cookie, target_mbps)\n",
    "            args, pins_plan, plan = intent_to_verifier_args(\n",
    "                slicing_info, \n",
    "                src_ip, \n",
    "                dst_ip, \n",
    "                main_flow_cookie, \n",
    "                target_mbps,\n",
    "                translated_flow_rule  # <-- ADD THIS ARGUMENT\n",
    "            )\n",
    "            protocol = plan[\"protocol\"]\n",
    "            dst_port = args[\"dst_port\"]\n",
    "\n",
    "            # (No need to find flow_id, we already have it)\n",
    "\n",
    "            # 3) Install pin flows (this is now done *inside* the verifier)\n",
    "\n",
    "            #################################################################\n",
    "            # --- START FINAL DEBUGGING BLOCK (v2 - Robust Filter) ---\n",
    "            #################################################################\n",
    "            # print(\"\\n[DEBUG] --- STARTING FINAL DEBUGGING (v2) ---\")\n",
    "            \n",
    "            # # --- Get parameters ---\n",
    "            # debug_src_ip = args[\"src_ip\"]\n",
    "            # debug_dst_ip = args[\"dst_ip\"]\n",
    "            # debug_dst_port = args[\"dst_port\"]\n",
    "            # debug_client_pid = get_mininet_host_pid(ip_to_host[debug_src_ip])\n",
    "            # debug_server_pid = get_mininet_host_pid(ip_to_host[debug_dst_ip])\n",
    "\n",
    "            # print(f\"[DEBUG] Client: h1 (PID {debug_client_pid}) -> Server: h3 (PID {debug_server_pid})\")\n",
    "            # print(f\"[DEBUG] Pinning plan has {len(pins_plan)} flows.\")\n",
    "\n",
    "            # pushed_pins = []\n",
    "            # tcpdump_running = False\n",
    "            # try:\n",
    "            #     # --- STEP 1: Install Pins ---\n",
    "            #     pushed_pins = install_pins_from_plan_ryu(\n",
    "            #         pins_plan, debug_src_ip, debug_dst_ip, debug_dst_port, protocol\n",
    "            #     )\n",
    "            #     print(f\"[DEBUG] STEP 1: Successfully installed {len(pushed_pins)} pinning flows.\")\n",
    "                \n",
    "            #     # --- STEP 2: L3 Ping Test ---\n",
    "            #     print(\"[DEBUG] STEP 2: Testing L3 connectivity (ping h1 -> h3)...\")\n",
    "            #     ping_out, ping_err = run_in_host(debug_client_pid, f\"ping -c 3 {debug_dst_ip}\", timeout=5)\n",
    "            #     if \"0% packet loss\" in ping_out:\n",
    "            #         print(\"[DEBUG] PING (h1->h3) SUCCESS: Default path is working.\")\n",
    "            #     else:\n",
    "            #         print(f\"[DEBUG] PING (h1->h3) FAILED. Basic connectivity is down.\")\n",
    "\n",
    "            #     # --- STEP 3: Start Server ---\n",
    "            #     print(\"[DEBUG] STEP 3: Starting iperf3 server on h3...\")\n",
    "            #     start_iperf_server(debug_server_pid, debug_dst_port)\n",
    "            #     print(\"[DEBUG] Server started.\")\n",
    "                \n",
    "            #     # --- STEP 4: Find and Start tcpdump ---\n",
    "            #     print(\"[DEBUG] STEP 4: Finding and starting tcpdump on h3...\")\n",
    "                \n",
    "            #     # 4a. Find the path for tcpdump using 'which'\n",
    "            #     which_out, which_err = run_in_host(debug_server_pid, \"/usr/bin/which tcpdump\", timeout=5)\n",
    "            #     tcpdump_path = which_out.strip()\n",
    "                \n",
    "            #     if not tcpdump_path or \"no tcpdump\" in tcpdump_path or \"not found\" in tcpdump_path:\n",
    "            #         print(\"[DEBUG] FATAL: 'tcpdump' not found on h3. Cannot complete L4 debugging.\")\n",
    "            #     else:\n",
    "            #         print(f\"[DEBUG] 'tcpdump' found at: {tcpdump_path}\")\n",
    "                    \n",
    "            #         # 4b. Start tcpdump with the ROBUST bit-masking filter\n",
    "            #         # This filter checks for 'tcp port 80' AND 'the SYN flag is set'\n",
    "                    \n",
    "            #         # --- *** THIS IS THE ONLY LINE THAT CHANGED *** ---\n",
    "            #         tcpdump_cmd = f\"nohup {tcpdump_path} -i h3-eth0 -c 3 '(tcp port 80) and (tcp[tcpflags] & tcp-syn != 0)' > /tmp/h3_tcpdump.log 2>&1 &\"\n",
    "                    \n",
    "            #         run_in_host(debug_server_pid, tcpdump_cmd, timeout=5)\n",
    "            #         tcpdump_running = True\n",
    "            #         time.sleep(1) # Give tcpdump time to start\n",
    "\n",
    "            #         # --- STEP 5: Run Netcat ---\n",
    "            #         print(\"[DEBUG] STEP 5: Running netcat (h1 -> h3:80)...\")\n",
    "            #         nc_cmd = f\"/bin/nc -zvw 4 {debug_dst_ip} {debug_dst_port}\"\n",
    "            #         nc_out, nc_err = run_in_host(debug_client_pid, nc_cmd, timeout=7)\n",
    "\n",
    "            #         if \"succeeded\" in nc_err or \"Connected\" in nc_err:\n",
    "            #             print(f\"[DEBUG] NETCAT SUCCESS: Port {debug_dst_port} is open.\")\n",
    "            #         else:\n",
    "            #             print(f\"[DEBUG] NETCAT FAILED: Port {debug_dst_port} is closed or unreachable.\")\n",
    "            #             print(f\"[DEBUG] NETCAT STDOUT: {nc_out}\")\n",
    "            #             print(f\"[DEBUG] NETCAT STDERR: {nc_err}\")\n",
    "\n",
    "            #         # --- STEP 6: Check tcpdump results ---\n",
    "            #         print(\"[DEBUG] STEP 6: Checking tcpdump results from h3...\")\n",
    "            #         time.sleep(1) # wait for logs\n",
    "            #         if tcpdump_running:\n",
    "            #             run_in_host(debug_server_pid, \"killall tcpdump\", timeout=5) # Stop the capture\n",
    "                    \n",
    "            #         dump_log, _ = run_in_host(debug_server_pid, \"cat /tmp/h3_tcpdump.log\", timeout=5)\n",
    "            #         print(f\"[DEBUG] H3 TCPDUMP LOG:\\n{dump_log}\")\n",
    "                    \n",
    "            #         # Check for the \"listening on...\" and \"packets captured\" lines\n",
    "            #         if (\"packets captured\" in dump_log and \"0 packets\" not in dump_log):\n",
    "            #              print(\"[DEBUG] ANALYSIS: SUCCESS! The SYN packet ARRIVED at h3.\")\n",
    "            #              print(\"[DEBUG] ROOT CAUSE: The problem is the REVERSE path (SYN-ACK from h3 to h1).\")\n",
    "            #         elif (\"listening on\" in dump_log):\n",
    "            #              print(\"[DEBUG] ANALYSIS: FAILED! The SYN packet did NOT ARRIVE at h3.\")\n",
    "            #              print(\"[DEBUG] (tcpdump ran, but 0 packets were captured)\")\n",
    "            #              print(\"[DEBUG] ROOT CAUSE: The FORWARD path (h1->s1->s3->s4) is broken.\")\n",
    "            #         else:\n",
    "            #              print(\"[DEBUG] ANALYSIS: FAILED! The tcpdump command failed to run correctly.\")\n",
    "            #              print(\"[DEBUG] (The log does not contain expected 'listening' or 'captured' output)\")\n",
    "\n",
    "            # except Exception as e:\n",
    "            #     print(f\"[DEBUG] An exception occurred during debugging: {e}\")\n",
    "            \n",
    "            # finally:\n",
    "            #     # --- Always cleanup ---\n",
    "            #     print(\"[DEBUG] STEP 7: Cleaning up...\")\n",
    "            #     unpin_path_ryu(pushed_pins)\n",
    "            #     stop_iperf_server(debug_server_pid, debug_dst_port)\n",
    "            #     if tcpdump_running:\n",
    "            #         run_in_host(debug_server_pid, \"killall tcpdump || true\", timeout=5)\n",
    "            #     print(\"[DEBUG] --- MANUAL DEBUGGING COMPLETE ---\")\n",
    "            #################################################################\n",
    "            # --- END FINAL DEBUGGING BLOCK (v2) ---\n",
    "            #################################################################\n",
    "\n",
    "\n",
    "            try:\n",
    "                # 4) Call the verifier\n",
    "                if protocol == \"udp\":\n",
    "                    verify_qos_flow_with_iperf(\n",
    "                        flow_device_id=args[\"flow_device_id\"],\n",
    "                        flow_cookie=args[\"flow_cookie\"], \n",
    "                        queue_device_id=args[\"queue_device_id\"],\n",
    "                        queue_port_no=args[\"queue_port_no\"],\n",
    "                        queue_id=args[\"queue_id\"],\n",
    "                        src_ip=args[\"src_ip\"],\n",
    "                        dst_ip=args[\"dst_ip\"],\n",
    "                        dst_port=dst_port,\n",
    "                        target_mbps=args[\"target_mbps\"],\n",
    "                        protocol=\"udp\",\n",
    "                        pin_path_flows=pins_plan,\n",
    "                        \n",
    "                        # --- Explicit iperf parameters for UDP ---\n",
    "                        duration_sec=8,\n",
    "                        parallel=1, # <-- Explicitly set to 1 for UDP\n",
    "                        tolerance_pct=10.0,\n",
    "                        udp_bw_mbps=50.0,\n",
    "                        udp_len_bytes=1200\n",
    "                    )\n",
    "                else:\n",
    "                    # TCP\n",
    "                    verify_qos_flow_with_iperf(\n",
    "                        flow_device_id=args[\"flow_device_id\"],\n",
    "                        flow_cookie=args[\"flow_cookie\"], \n",
    "                        queue_device_id=args[\"queue_device_id\"],\n",
    "                        queue_port_no=args[\"queue_port_no\"],\n",
    "                        queue_id=args[\"queue_id\"],\n",
    "                        src_ip=args[\"src_ip\"],\n",
    "                        dst_ip=args[\"dst_ip\"],\n",
    "                        dst_port=dst_port,\n",
    "                        target_mbps=args[\"target_mbps\"],\n",
    "                        protocol=\"tcp\",\n",
    "                        pin_path_flows=pins_plan,\n",
    "                        \n",
    "                        # --- Explicit iperf parameters for TCP ---\n",
    "                        duration_sec=8,\n",
    "                        parallel=8,\n",
    "                        tcp_mss=1200,\n",
    "                        tolerance_pct=10.0\n",
    "                    )\n",
    "            finally:\n",
    "                # The verifier function has its own 'finally'\n",
    "                # block to clean up pin flows.\n",
    "                pass    \n",
    "             \n",
    "        elif (flow_rule_type== \"forwarding\"):\n",
    "             ONOS_assurance_for_forwarding_intent(src_host, dst_host, src_ip, dst_ip)\n",
    "\n",
    "        elapsed_time = (time.time() - current_time)\n",
    "        print(\"\\nTime taken for end-to-end IBN: \",  round(elapsed_time,2))\n",
    "\n",
    "elif (flow_id == \"Tie\"):\n",
    "    print(\"\\n\\nReport to the operator about this conflict resolution issue. Need adjustment to conflict resolution policy.\\n\")\n",
    "\n",
    "elif (flow_id == \"existing_rule_win\"):\n",
    "    print(\"\\n\\nThe new intent conflicts with an existing one having a higher priority according to current policy, hence the new intent was not installed. See the existing flow rule that conflicts.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\\nLLM failed to produce meaningful response. Either update context example or model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dpid 1] delete(cookie=0x3b8678c981e2b5e7) via cookie(int) -> still present? NO\n"
     ]
    }
   ],
   "source": [
    "#manual delete. The cookie can be int or hex\n",
    "# !python3 -m ryu_flow_installer --dpid 1 --delete --cookie 4289248501979854311"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
