{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7307fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part of this code is adapted from \"https://github.com/tu-nv/ibn_llm/tree/master\". \n",
    "Particularly the usage of datasets, associated prompt, computation of accuracy and leveraging the MaxMarginalRelevanceExampleSelector tool. \n",
    "We acknowledge and thank the original authors.\n",
    "The datasets here (Formal specification and NFV configuration) are the existing datasets that we refered to in our paper.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector\n",
    "#from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from ollama import Client\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c36fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedding_url=\"http://localhost:11434\"\n",
    "ollama_server_url=\"http://localhost:11435\"\n",
    "client = Client(host=ollama_server_url , timeout=120)\n",
    "context_examples=[0, 1, 3, 6, 9]\n",
    "use_case = \"formal_spec\" #or \"nfv_conf\"\n",
    "\n",
    "if use_case == \"formal_spec\":\n",
    "    from formal_specification.dataset import trainset, testset\n",
    "    from formal_specification.prompts import SYSTEM_PROMPT\n",
    "    from formal_specification.utils import compare_result\n",
    "elif use_case == \"nfv_conf\":\n",
    "    from nfv_configuration.dataset import trainset, testset\n",
    "    from nfv_configuration.prompts import SYSTEM_PROMPT\n",
    "    from nfv_configuration.utils import compare_result\n",
    "else:\n",
    "    raise ValueError(\"Invalid use case\")\n",
    "\n",
    "my_models = [\n",
    "\"codegemma\",\n",
    "\"starcoder2\",\n",
    "\"dolphin-mistral\",\n",
    "\"wizardlm2\",\n",
    "\"phi\",\n",
    "\"yi\",\n",
    "\"command-r\",\n",
    "\"orca-mini\", \"llava-llama3\", \"zephyr\",\n",
    "\"starcoder\", \"codestral\",\n",
    "\"codellama:34b\",\n",
    "\"codellama\",\n",
    "\"llama2\",\n",
    "\"llama3\",\n",
    "\"llama3.1\",\n",
    "\"llama3.2\",\n",
    "\"qwen\",\n",
    "\"qwen2\",\n",
    "\"qwen2.5\",\n",
    "\"gemma2:27b\",\n",
    "\"openchat\",\n",
    "\"marco-o1\",\n",
    "\"mistral\",\n",
    "\"phi3\",\n",
    "\"huihui_ai/qwq-abliterated\",\n",
    "\"huihui_ai/qwq-fusion\",\n",
    "\"qwq\",\n",
    "\"mistral-nemo\",\n",
    "\"tinyllama\",\n",
    "\"deepseek-coder\"\n",
    "]\n",
    "\n",
    "#\"deepseek-coder-v2\" does not support kshift\n",
    "#\"mixtral\", \"qwen2.5-coder\",\"llava.\",\n",
    "#(does not support) \"mxbai-embed-large\", \"nomic-embed-text\", \"snowflake-arctic-embed\"\n",
    "\n",
    "my_models_large = [\"llama3.3\"]\n",
    "default_model = \"llama2\"\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=default_model,\n",
    "    base_url=ollama_embedding_url,\n",
    ")\n",
    "\n",
    "csv_file = \"translate_result_\"+use_case+\".csv\"\n",
    "pd.DataFrame(columns=[\"model\", \"num_examples\", \"accuracy\", \"avg_time\"]).to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465426b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in my_models:\n",
    "\n",
    "    for num_examples in context_examples:\n",
    "        \n",
    "        example_selector = MaxMarginalRelevanceExampleSelector.from_examples([trainset[0]], ollama_emb, Chroma, input_keys=[\"instruction\"], k=num_examples, vectorstore_kwargs={\"fetch_k\": min(num_examples, len(trainset))} )\n",
    "        example_selector.vectorstore.reset_collection()\n",
    "        for example in trainset:\n",
    "            example_selector.add_example(example)\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        processing_times = []\n",
    "\n",
    "        for testcase in testset:\n",
    "            intent = testcase[\"instruction\"]\n",
    "            expected_output = testcase[\"output\"]\n",
    "            system_prompt = SYSTEM_PROMPT\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    time.sleep(0.1)\n",
    "                    current_time = time.time()\n",
    "                    if num_examples > 0:\n",
    "                        examples = example_selector.select_examples({\"instruction\": intent})\n",
    "                        example_str = \"\\n\\n\\n\".join(map(lambda x: \"Input: \" + x[\"instruction\"] + \"\\n\\nOutput: \" + x[\"output\"], examples))\n",
    "                        system_prompt += example_str + \"\\n\\n\\n\"\n",
    "\n",
    "                    response = client.generate(model=model,\n",
    "                        options={\n",
    "                            'temperature': 0.6,\n",
    "                            'num_ctx': 8192,\n",
    "                            'top_p': 0.3,\n",
    "                            'num_predict': 1024,\n",
    "                            'num_gpu': 99,\n",
    "                            },\n",
    "                        stream=False,\n",
    "                        system=system_prompt,\n",
    "                        prompt=intent,\n",
    "                        format='json'\n",
    "                    )\n",
    "                    actual_output = response['response']\n",
    "                    #print(\"\\nGot Response\\n\")\n",
    "\n",
    "                    proc_time_s = (time.time() - current_time)\n",
    "                    processing_times.append(proc_time_s)\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(\"Exception on Input: \", e)\n",
    "                    sys.stdout.flush()\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                expected_output = json.loads(expected_output)\n",
    "                actual_output = json.loads(actual_output)\n",
    "                num_correct_translation, total_translation = compare_result(expected_output, actual_output)\n",
    "\n",
    "                #if num_correct_translation == 0:\n",
    "                    #print(f\"Input: {intent}\")\n",
    "                    #print(f\"Expected: {expected_output}\")\n",
    "                    #print(f\"Actual: {actual_output}\")\n",
    "                    #print(f\"Diff: {jsondiff.diff(expected_output, actual_output)}\")\n",
    "                correct += num_correct_translation\n",
    "                total += total_translation\n",
    "\n",
    "                #print(\"=====================================\")\n",
    "                #print(f\"Corrects: {correct}, total: {total}, percent: {(correct/total)*100}, proc time: {proc_time_s}\")\n",
    "                #sys.stdout.flush()\n",
    "            except Exception as e:\n",
    "                print(\"Exception on comparing result: \", e)\n",
    "\n",
    "        print(\"=====================================\")\n",
    "        print(f\"Finish eval on use case: {use_case}, model: {model}, num context examples: {num_examples}, testcases: {total}, accuracy: {round((correct/total)*100, 3)}  avg proc time: {round(np.average(processing_times), 1)}\")\n",
    "        \n",
    "        try:\n",
    "            # Assume this block runs after successful computation\n",
    "            my_result = {\n",
    "                        \"model\": model,\n",
    "                        \"num_examples\": num_examples,\n",
    "                        \"accuracy\": round((correct / total) * 100, 2),\n",
    "                        \"avg_time\": round(np.average(processing_times), 1)\n",
    "                    }\n",
    "\n",
    "            # Append the new result to the CSV file\n",
    "            pd.DataFrame([my_result]).to_csv(csv_file, mode='a', header=False, index=False)        \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing model {model} with {num_examples} examples: {e}\")             \n",
    "        \n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if(correct == total):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba187fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
