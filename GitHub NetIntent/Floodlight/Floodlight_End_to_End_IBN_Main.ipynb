{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import MaxMarginalRelevanceExampleSelector\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from ollama import Client\n",
    "import json\n",
    "import subprocess\n",
    "import time\n",
    "import ipaddress\n",
    "from datetime import datetime\n",
    "import shlex\n",
    "import math\n",
    "from typing import Optional, Dict, Any\n",
    "import argparse, secrets, hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Floodlight Controller Details\n",
    "FLOODLIGHT_CONTROLLER_IP = \"127.0.0.1\"\n",
    "FLOODLIGHT_CONTROLLER_PORT = 8080\n",
    "FLOODLIGHT_BASE_URL = f\"http://{FLOODLIGHT_CONTROLLER_IP}:{FLOODLIGHT_CONTROLLER_PORT}\"\n",
    "\n",
    "# API Paths\n",
    "STATIC_FLOW_PUSHER_URL = f\"{FLOODLIGHT_BASE_URL}/wm/staticflowpusher\"\n",
    "STATIC_FLOW_LIST_URL = f\"{STATIC_FLOW_PUSHER_URL}/list\"\n",
    "STATIC_FLOW_CLEAR_URL = f\"{STATIC_FLOW_PUSHER_URL}/clear\"\n",
    "SWITCH_STATS_FLOW_URL = f\"{FLOODLIGHT_BASE_URL}/wm/core/switch\"\n",
    "SWITCHES_URL = f\"{FLOODLIGHT_BASE_URL}/wm/core/controller/switches/json\"\n",
    "\n",
    "# Define sudo password\n",
    "sudo_password = \"test@irciss008\" #your localhost password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_to_host = {\n",
    "    \"10.0.1.1\": \"h1\",\n",
    "    \"10.0.1.2\": \"h2\",\n",
    "    \"10.0.1.3\": \"h3\",\n",
    "    \"10.0.1.4\": \"h4\"\n",
    "            }\n",
    "\n",
    "host_to_ip = {\n",
    "    \"h1\": \"10.0.1.1\",\n",
    "    \"h2\": \"10.0.1.2\",\n",
    "    \"h3\": \"10.0.1.3\",\n",
    "    \"h4\": \"10.0.1.4\",\n",
    "}\n",
    "\n",
    "# --- Diamond topology wiring helpers (Floodlight format) ---\n",
    "\n",
    "SW_OF = {\n",
    "    \"1\": \"00:00:00:00:00:00:00:01\",  # s1\n",
    "    \"2\": \"00:00:00:00:00:00:00:02\",  # s2\n",
    "    \"3\": \"00:00:00:00:00:00:00:03\",  # s3\n",
    "    \"4\": \"00:00:00:00:00:00:00:04\",  # s4\n",
    "}\n",
    "\n",
    "HOSTS = {\n",
    "    \"h1\": \"10.0.1.1\",\n",
    "    \"h2\": \"10.0.1.2\",\n",
    "    \"h3\": \"10.0.1.3\",\n",
    "    \"h4\": \"10.0.1.4\",\n",
    "}\n",
    "\n",
    "# Host attachment (edge switch, access port) from your Mininet build\n",
    "# This now uses the correct DPID format as keys\n",
    "HOST_ATTACH = {\n",
    "    HOSTS[\"h1\"]: (SW_OF[\"1\"], 3),  # h1 -> s1:3\n",
    "    HOSTS[\"h2\"]: (SW_OF[\"1\"], 4),  # h2 -> s1:4\n",
    "    HOSTS[\"h3\"]: (SW_OF[\"4\"], 3),  # h3 -> s4:3\n",
    "    HOSTS[\"h4\"]: (SW_OF[\"4\"], 4),  # h4 -> s4:4\n",
    "}\n",
    "\n",
    "switch_id_for_llm_assurance = None\n",
    "llm_caller_flag = 0\n",
    "gl_flow_name = \"\" # Use flow name for tracking, not cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_all_operational_flows(dpid_str: str) -> str:\n",
    "    \"\"\"Helper to get a formatted string of all operational flows on a switch.\"\"\"\n",
    "    try:\n",
    "        url = f\"{SWITCH_STATS_FLOW_URL}/{dpid_str}/flow/json\"\n",
    "        r = requests.get(url, timeout=5)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        # Pretty-print the JSON for readability\n",
    "        return json.dumps(data.get(\"flows\", []), indent=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error dumping flows for {dpid_str}: {e}\"\n",
    "\n",
    "def fl_get(url):\n",
    "    \"\"\"Generic GET request for Floodlight.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error in GET {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fl_post(url, json_payload):\n",
    "    \"\"\"Generic POST request for Floodlight.\"\"\"\n",
    "    try:\n",
    "        r = requests.post(url, json=json_payload, timeout=5)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error in POST {url}: {e} - Payload: {json_payload}\")\n",
    "        return None\n",
    "\n",
    "def add_flow_floodlight(flow):\n",
    "    \"\"\"\n",
    "    Pushes a single static flow rule to Floodlight.\n",
    "    'flow' must be a valid Floodlight static flow JSON object.\n",
    "    \"\"\"\n",
    "    response = fl_post(f\"{STATIC_FLOW_PUSHER_URL}/json\", flow)\n",
    "    if response and response.get(\"status\") == \"Entry pushed\":\n",
    "        return True\n",
    "    print(f\"Warning: Add flow failed: {response}\")\n",
    "    return False\n",
    "\n",
    "def delete_flow_floodlight_alternative(flow_name):\n",
    "    \"\"\"\n",
    "    Deletes a static flow rule from Floodlight by its 'name'.\n",
    "    \"\"\"\n",
    "    payload = {\"remove\": {\"name\": flow_name}}\n",
    "    response = fl_post(f\"{STATIC_FLOW_PUSHER_URL}/json\", payload)\n",
    "    if response and \"Entry removed\" in response.get(\"status\", \"\"):\n",
    "        return True\n",
    "    # It might also be a DELETE request, but POST with \"remove\" is common\n",
    "    # Alternative:\n",
    "    # r = requests.delete(f\"{STATIC_FLOW_PUSHER_URL}/json\", json={\"name\": flow_name})\n",
    "    # if r.status_code == 200: return True\n",
    "    \n",
    "    print(f\"Warning: Delete flow failed for name {flow_name}: {response}\")\n",
    "    return False\n",
    "\n",
    "def delete_flow_floodlight(flow_name):\n",
    "    url = f\"{STATIC_FLOW_PUSHER_URL}/json\"\n",
    "    try:\n",
    "        r = requests.delete(url, json={\"name\": flow_name}, timeout=5)\n",
    "        if r.ok: return True\n",
    "        # fallback(s)\n",
    "        r = requests.post(url, json={\"name\": flow_name, \"command\": \"delete\"}, timeout=5)\n",
    "        if r.ok: return True\n",
    "        r = requests.post(url, json={\"name\": flow_name, \"action\": \"remove\"}, timeout=5)\n",
    "        return r.ok\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error deleting {flow_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_static_flows_floodlight(dpid_str):\n",
    "    \"\"\"\n",
    "    Gets all *static* flows for a given switch DPID.\n",
    "    \"\"\"\n",
    "    data = fl_get(f\"{STATIC_FLOW_LIST_URL}/{dpid_str}/json\")\n",
    "    return data.get(dpid_str, []) if data else []\n",
    "\n",
    "def get_operational_flows_floodlight(dpid_str):\n",
    "    \"\"\"\n",
    "    Gets all *operational* flows (with stats) from a switch.\n",
    "    \"\"\"\n",
    "    data = fl_get(f\"{SWITCH_STATS_FLOW_URL}/{dpid_str}/flow/json\")\n",
    "    return data.get(\"flows\", []) if data else []\n",
    "\n",
    "def wait_for_dpids(expect_dpids, timeout=30):\n",
    "    \"\"\"\n",
    "    Waits for a set of DPIDs (colon-hex) to connect to Floodlight.\n",
    "    \"\"\"\n",
    "    want = set(expect_dpids)\n",
    "    start = time.time()\n",
    "    seen_dpids = set()\n",
    "    while time.time() - start < timeout:\n",
    "        switches_data = fl_get(SWITCHES_URL)\n",
    "        if switches_data:\n",
    "            seen_dpids = {s.get(\"dpid\") for s in switches_data}\n",
    "            if want.issubset(seen_dpids):\n",
    "                return sorted(seen_dpids)\n",
    "        time.sleep(1)\n",
    "    return sorted(seen_dpids)\n",
    "\n",
    "def find_flow_by_name(flow_dump, name_to_find):\n",
    "    \"\"\"Finds a flow in a list of static flows by its 'name'.\"\"\"\n",
    "    for flow_entry_dict in flow_dump:\n",
    "        if name_to_find in flow_entry_dict:\n",
    "            return flow_entry_dict[name_to_find]\n",
    "    return None\n",
    "\n",
    "def exists_floodlight(dpid_str, flow_name, retries=3, delay=0.5):\n",
    "    \"\"\"\n",
    "    Checks if a static flow exists on a switch by its 'name'.\n",
    "    Returns (True, found_flow) or (False, None).\n",
    "    \"\"\"\n",
    "    for _ in range(retries):\n",
    "        dump = get_static_flows_floodlight(dpid_str)\n",
    "        found = find_flow_by_name(dump, flow_name)\n",
    "        if found:\n",
    "            return True, found\n",
    "        time.sleep(delay)\n",
    "    return False, None\n",
    "\n",
    "def fingerprint(flow):\n",
    "    \"\"\"\n",
    "    (This helper is fine, but we prefer using 'name'.)\n",
    "    Generates a hash for a flow rule.\n",
    "    \"\"\"\n",
    "    # We must adapt this to the flat Floodlight JSON structure\n",
    "    # Let's fingerprint the *match* and *actions*\n",
    "    \n",
    "    # Extract match keys (anything not in the control set)\n",
    "    control_keys = {\"switch\", \"name\", \"active\", \"priority\", \"actions\"}\n",
    "    match_part = {k: v for k, v in flow.items() if k not in control_keys}\n",
    "    \n",
    "    h = hashlib.sha1(json.dumps({\n",
    "        \"priority\": flow.get(\"priority\"),\n",
    "        \"match\": match_part,\n",
    "        \"actions\": flow.get(\"actions\", \"\")\n",
    "    }, sort_keys=True).encode()).hexdigest()\n",
    "    return h[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATION_PROMPT_FLOODLIGHT = \"\"\"\n",
    "You are a meticulous network engineer. Convert the user's intent into a single Floodlight (Static Entry Pusher, OpenFlow 1.0) flow rule as a JSON object ONLY (no code fences, no comments, no extra text).\n",
    "\n",
    "Return exactly ONE JSON object with ONLY these keys.\n",
    "\n",
    "Required keys (all strings):\n",
    "- \"switch\": \"00:00:00:00:00:00:00:0X\"   (DPID; e.g., N → \"...:0N\")\n",
    "- \"name\": \"flow-<short-descriptor>\"\n",
    "- \"active\": \"true\"\n",
    "- \"priority\": \"<0-65535>\"\n",
    "\n",
    "Allowed match keys (strings; include ONLY those needed):\n",
    "- \"in_port\"\n",
    "- \"eth_type\"            (IPv4=0x0800, IPv6=0x86dd, ARP=0x0806)\n",
    "- \"eth_src\", \"eth_dst\"\n",
    "- \"ip_proto\"            (TCP=6, UDP=17, ICMP=1)\n",
    "- \"ipv4_src\", \"ipv4_dst\", \"ipv6_src\", \"ipv6_dst\"\n",
    "- \"tcp_src\", \"tcp_dst\", \"udp_src\", \"udp_dst\", \"sctp_src\", \"sctp_dst\"\n",
    "- \"icmpv4_type\", \"icmpv4_code\"\n",
    "- \"arp_opcode\", \"arp_spa\", \"arp_tpa\", \"arp_sha\", \"arp_tha\"\n",
    "- \"eth_vlan_vid\", \"eth_vlan_pcp\"\n",
    "\n",
    "Allowed action key:\n",
    "- \"actions\": \"<comma-separated actions>\" (omit this key entirely to DROP)\n",
    "\n",
    "Allowed action values (comma-separated in a single string):\n",
    "- \"output=X\"\n",
    "- \"set_queue=Y\"\n",
    "- \"push_vlan=0x8100\"\n",
    "- \"pop_vlan\"\n",
    "- \"set_vlan_vid=VID\"\n",
    "\n",
    "Interpretation rules:\n",
    "- “in/on switch N” → \"switch\":\"00:00:00:00:00:00:00:0N\".\n",
    "- If you use any IP/L4 fields and \"eth_type\" is unspecified, set \"eth_type\":\"0x0800\".\n",
    "- Protocol cues: TCP→\"ip_proto\":\"6\", UDP→\"17\", ICMP→\"1\".\n",
    "- Services: HTTP→\"tcp_dst\":\"80\", HTTPS→\"443\", SSH→\"22\", DNS→\"udp_dst\":\"53\", DHCP→\"udp_dst\":\"67\" (server) or \"68\" (client), ping→\"icmpv4_type\":\"8\".\n",
    "- “forward/send out/through port X” → \"actions\":\"output=X\".\n",
    "- QoS “queue Y” → \"actions\":\"set_queue=Y,output=X\".\n",
    "- “block/deny/drop” → OMIT the \"actions\" key.\n",
    "- Use ONLY fields implied by the intent (plus the defaults above). All keys and values are strings.\n",
    "\n",
    "Priority guidance (pick the most appropriate):\n",
    "- \"300\" → explicit block/deny\n",
    "- \"200\" → highly specific forwarding with QoS\n",
    "- \"100\" → specific L3/L4 matches\n",
    "- \"50\"  → less-specific catch-alls\n",
    "\n",
    "Constraints:\n",
    "- Output must be a single valid JSON object, nothing else.\n",
    "- Do not invent fields, do not include comments or markdown.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFLICT_PROMPT_FLOODLIGHT = \"\"\"\n",
    "You are an expert network engineer analyzing two Floodlight (OpenFlow) flow rules to decide whether they conflict.\n",
    "\n",
    "PRIMARY DECISION\n",
    "A conflict exists if BOTH are true:\n",
    "1) The rules apply to the same switch (equal \"switch\"; if both specify \"table\", tables must be equal too).\n",
    "2) Their match conditions overlap (there exists at least one packet that matches both),\n",
    "   AND either:\n",
    "   a) their actions differ, or\n",
    "   b) they are redundant (same action and one rule’s match is a subset of the other).\n",
    "\n",
    "DEFINITIONS & NORMALIZATION\n",
    "- Floodlight drop = the \"actions\" key is absent. (Empty string is treated as drop ONLY if present in the input.)\n",
    "- Actions are a comma-separated list in a single string (e.g., \"set_queue=1,output=2\"). Compare actions as an unordered set of tokens; ignore whitespace and token order.\n",
    "- Consider only top-level match keys (e.g., \"in_port\", \"eth_type\", \"ip_proto\", \"ipv4_src\", \"ipv4_dst\", \"tcp_dst\", \"udp_dst\", \"eth_vlan_vid\", etc.). Keys like \"name\", \"active\", \"priority\" do not affect matching.\n",
    "- Normalize values before comparison:\n",
    "  • eth_type: \"0x800\" == \"0x0800\" (IPv4).  \n",
    "  • ip_proto: accept \"6\"/\"tcp\", \"17\"/\"udp\", \"1\"/\"icmp\".  \n",
    "  • ports and priorities: compare numerically even if strings.  \n",
    "  • IPs may be host (/32) or CIDR; treat \"10.0.0.1\" as \"10.0.0.1/32\".\n",
    "- Match overlap rules:\n",
    "  • If \"eth_type\" differ (e.g., IPv4 vs IPv6), no overlap.  \n",
    "  • If \"ip_proto\" differ (e.g., TCP vs UDP), no overlap.  \n",
    "  • If both specify \"in_port\" with different values, no overlap.  \n",
    "  • IP prefixes overlap if their CIDRs intersect (e.g., 10.0.0.0/24 overlaps 10.0.0.1/32).  \n",
    "  • TCP/UDP port equality is required when both specify the same L4 field.  \n",
    "  • VLAN: \"eth_vlan_vid\" must be equal if both specify it; otherwise the more general one (no VLAN constraint) overlaps the specific one.  \n",
    "  • Absence of a field makes a rule more general on that dimension.\n",
    "\n",
    "GUIDANCE\n",
    "- Different \"switch\" → no conflict. If both specify \"table\" and they differ → no conflict.  \n",
    "- Overlap + different actions → conflict.  \n",
    "- Overlap + same actions:\n",
    "    • If one match ⊂ the other → conflict (Redundancy).  \n",
    "    • If partial overlap without subset and same actions → treat as no conflict for this binary decision.\n",
    "- Priority does not affect the YES/NO decision, but may be used in the explanation (e.g., “more general rule may shadow a specific rule if higher priority”).\n",
    "\n",
    "EXAMPLES\n",
    "\n",
    "Example A: Conflict (Different Action)\n",
    "F1: {\"switch\":\"...:01\",\"eth_type\":\"0x0800\",\"ipv4_dst\":\"10.0.0.1\",\"actions\":\"output=1\"}\n",
    "F2: {\"switch\":\"...:01\",\"eth_type\":\"0x0800\",\"ipv4_dst\":\"10.0.0.1\",\"actions\":\"output=2\"}\n",
    "Output: {\"conflict_status\":1,\"conflict_explanation\":\"Same switch and identical match; actions differ (output=1 vs output=2).\"}\n",
    "\n",
    "Example B: No Conflict (Different Switch)\n",
    "F1: {\"switch\":\"...:01\",\"eth_type\":\"0x0800\",\"ipv4_dst\":\"10.0.0.1\",\"actions\":\"output=3\"}\n",
    "F2: {\"switch\":\"...:02\",\"eth_type\":\"0x0800\",\"ipv4_dst\":\"10.0.0.1\",\"actions\":\"output=3\"}\n",
    "Output: {\"conflict_status\":0,\"conflict_explanation\":\"Different switches.\"}\n",
    "\n",
    "Example C: Conflict (Redundancy)\n",
    "F1: {\"switch\":\"...:04\",\"eth_type\":\"0x0800\",\"ip_proto\":\"6\",\"tcp_dst\":\"80\",\"actions\":\"output=2\"}\n",
    "F2: {\"switch\":\"...:04\",\"eth_type\":\"0x0800\",\"ip_proto\":\"6\",\"actions\":\"output=2\"}\n",
    "Output: {\"conflict_status\":1,\"conflict_explanation\":\"Specific rule is subset of a more general rule with the same action (redundancy).\"}\n",
    "\n",
    "Example D: No Conflict (Protocol Mismatch)\n",
    "F1: {\"switch\":\"...:01\",\"eth_type\":\"0x0800\",\"ip_proto\":\"6\",\"tcp_dst\":\"22\",\"actions\":\"output=1\"}\n",
    "F2: {\"switch\":\"...:01\",\"eth_type\":\"0x0800\",\"ip_proto\":\"17\",\"udp_dst\":\"53\",\"actions\":\"output=1\"}\n",
    "Output: {\"conflict_status\":0,\"conflict_explanation\":\"TCP vs UDP are mutually exclusive.\"}\n",
    "\n",
    "INPUT FORMAT\n",
    "You will be provided with two JSON flow rules:\n",
    "\n",
    "Flow 1:\n",
    "<JSON>\n",
    "\n",
    "Flow 2:\n",
    "<JSON>\n",
    "\n",
    "EXPECTED OUTPUT (strict JSON only):\n",
    "{\n",
    "  \"conflict_status\": <0 or 1>,\n",
    "  \"conflict_explanation\": \"<brief reason or empty string>\"\n",
    "}\n",
    "NO EXTRA TEXT.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICING_PROMPT = \"\"\"You are tasked with analyzing a natural language intent to determine if it contains a command to create or use a queue/slice in an OpenFlow switch. You should respond in JSON format.\n",
    "\n",
    "### Rules for Interpretation:\n",
    "1. **Queue/Slice Detection:**  \n",
    "   - The intent is considered related to queue/slice if it contains commands such as:\n",
    "     - \"create queue\", \"create slices\", \"slice the network\", \"implement slicing\", \"slice the flow\", \"make flowspace slicing\", \"do slicing\", \"slice\", \"implement queue\", \"do queuing\", \"assign queue\", \"assign slice\", or any similar phrasing.\n",
    "   - If the intent does not mention creating or using a queue/slice, set the field `\"use_queue\"` to `0`.\n",
    "\n",
    "2. **Switch, Queue, and Port Identification (Data-plane port/interface):**  \n",
    "   - If the intent specifies a **switch ID** (e.g., \"switch 4\" or \"openflow:4\" or \"node 4\" or \"openflow 4\"), populate the `\"switch_id\"` field with its value.  \n",
    "   - If the intent specifies a **Queue ID or slice ID** (e.g., \"queue 4\" or \"4th queue\" or \"fourth queue\" or \"slice 1\" or \"first slice\"), populate the `\"queue_id\"` field with its value.  \n",
    "   - If the intent specifies a **port ID** referring to the device interface/output (e.g., \"port 2\" or \"interface 2\" or \"ethernet 2\" or \"output node connector 2\" or \"second port\" or \"second interface\"), populate the `\"port_id\"` field with its value. If there are multiple instances of \"port_id\" present, take the one which indicates the **output port or outgoing interface**.\n",
    "   - If the intent does not specify a switch ID or queue ID or port ID, set the respective field to an empty string (`\"\"`).\n",
    "\n",
    "3. **Traffic Type and L4 Destination Port Extraction (Protocol/Service):**\n",
    "   - Detect the **Layer-4 protocol** if mentioned: `\"tcp\"` or `\"udp\"`. Populate this in `\"traffic_type\"`. If not specified, set `\"traffic_type\"` to `\"\"`.\n",
    "   - Detect the **destination application port number** (Layer-4 port), if specified as a number (e.g., \"port 80\", \"UDP port 53\") or implied via a service reference such as \"HTTP (TCP port 80)\". Populate this number (as a string) in `\"l4_port\"`.  \n",
    "   - Do **not** confuse the L4 port (e.g., 80/53) with the device/interface port (e.g., switch port 2). The former goes to `\"l4_port\"`, the latter to `\"port_id\"`.\n",
    "   - If multiple L4 ports are mentioned, prefer the **destination/service port** used by the traffic selector (e.g., \"traffic destined for port 80\"). If still ambiguous, choose the first explicit destination/service port mentioned.\n",
    "   - If the L4 destination port is not specified, set `\"l4_port\"` to `\"\"`.\n",
    "\n",
    "4. **Negative Constraint**: Intents that only contain commands like \"block\", \"drop\", \"deny\", or \"forward\" without any explicit mention of \"queue\" or \"slice\" are not queue-related, and use_queue must be 0.\n",
    "\n",
    "5. **Output Format:**  \n",
    "   - Respond strictly in valid JSON format adhering to the following schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"use_queue\": <integer>,\n",
    "  \"switch_id\": \"<string>\",\n",
    "  \"queue_id\": \"<string>\",\n",
    "  \"port_id\": \"<string>\",\n",
    "  \"traffic_type\": \"<string>\",\n",
    "  \"l4_port\": \"<string>\"\n",
    "}\n",
    "\n",
    "Field Description:\n",
    "use_queue: 1 if the intent commands to create or use a queue/slice, 0 otherwise.\n",
    "switch_id: Switch ID if specified in the intent, otherwise \"\".\n",
    "queue_id: Queue/Slice ID if specified in the intent, otherwise \"\".\n",
    "port_id: Device/interface port if specified (output/outgoing preferred), otherwise \"\".\n",
    "traffic_type: \"tcp\" or \"udp\" if specified, otherwise \"\".\n",
    "l4_port: Destination application port number (e.g., \"80\", \"53\") if specified, otherwise \"\".\n",
    "\n",
    "No Additional Text:\n",
    "Do not include any comments, explanations, or outputs outside the JSON format.\n",
    "\n",
    "Example Inputs and Outputs:\n",
    "\n",
    "Input Intent:\n",
    "\"Create a queue in switch 4 on port 3 for slicing the flow.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"switch 4\",\n",
    "\"queue_id\": \"\",\n",
    "\"port_id\": \"port 3\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Send all video traffic through queue 0 of openflow:2.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"openflow 2\",\n",
    "\"queue_id\": \"0\",\n",
    "\"port_id\": \"\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Configure switch 5 for traffic management.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 0,\n",
    "\"switch_id\": \"switch 5\",\n",
    "\"queue_id\": \"\",\n",
    "\"port_id\": \"\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Monitor traffic flow on port 1.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 0,\n",
    "\"switch_id\": \"\",\n",
    "\"queue_id\": \"\",\n",
    "\"port_id\": \"port 1\",\n",
    "\"traffic_type\": \"\",\n",
    "\"l4_port\": \"\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"In switch 3, if the incoming traffic in port 1 is TCP traffic destined for port 80, then pass it via interface 2, assigning it to queue 0 for prioritized handling.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"switch 3\",\n",
    "\"queue_id\": \"0\",\n",
    "\"port_id\": \"interface 2\",\n",
    "\"traffic_type\": \"tcp\",\n",
    "\"l4_port\": \"80\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"For node 1, route HTTP (TCP port 80) traffic targeting 10.0.0.3/32 through port 2 with traffic assigned to queue 0.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"node 1\",\n",
    "\"queue_id\": \"0\",\n",
    "\"port_id\": \"port 2\",\n",
    "\"traffic_type\": \"tcp\",\n",
    "\"l4_port\": \"80\"\n",
    "}\n",
    "\n",
    "Input Intent:\n",
    "\"Forward UDP traffic on port 53 destined for 10.0.0.9 via interface 5 of switch 2, assigning it to queue 3.\"\n",
    "Output:\n",
    "{\n",
    "\"use_queue\": 1,\n",
    "\"switch_id\": \"switch 2\",\n",
    "\"queue_id\": \"3\",\n",
    "\"port_id\": \"interface 5\",\n",
    "\"traffic_type\": \"udp\",\n",
    "\"l4_port\": \"53\"\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_models_translate_real = [\n",
    "\"codestral:22b\",\n",
    "\"command-r:35b\",\n",
    "\"huihui_ai/qwq-abliterated:latest\",\n",
    "]\n",
    "\n",
    "my_models_conflict_real = [  \n",
    "\"huihui_ai/qwq-fusion:latest\",\n",
    "\"qwq:latest\"\n",
    "]\n",
    "\n",
    "context_examples = [3, 6]\n",
    "\n",
    "default_model = \"llama2:7b\"\n",
    "\n",
    "ollama_embedding_url = \"http://localhost:11434\"\n",
    "ollama_server_url = \"http://localhost:11435\"  \n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=default_model,\n",
    "    base_url=ollama_embedding_url,\n",
    ")\n",
    "\n",
    "client = Client(host=ollama_server_url , timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom dataset from CSV\n",
    "custom_dataset = pd.read_csv('Intent2Flow-Floodlight.csv')\n",
    "\n",
    "# Ensure proper column names and format\n",
    "if not {'instruction', 'output'}.issubset(custom_dataset.columns):\n",
    "    raise ValueError(\"The dataset must have 'instruction' and 'output' columns.\")\n",
    "\n",
    "# Split into train and test (50/50 split for example)\n",
    "#trainset, testset = train_test_split(custom_dataset, test_size=0.5, random_state=42, shuffle=True)\n",
    "trainset = custom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_intent_to_store(\n",
    "    file_path,\n",
    "    nl_intent,\n",
    "    json_flow_rule,\n",
    "    device_id,\n",
    "    flow_id,\n",
    "    intent_type,\n",
    "    intent_specificity\n",
    "):\n",
    "    \"\"\"\n",
    "    Appends an intent record to the IntentStore file in JSONL format.\n",
    "    \"\"\"\n",
    "    record = {\n",
    "        \"nl_intent\": nl_intent,\n",
    "        \"json_flow_rule\": json_flow_rule,\n",
    "        \"device_id\": device_id,\n",
    "        \"flow_id\": flow_id,\n",
    "        \"intent_type\": intent_type,\n",
    "        \"intent_specificity\": intent_specificity\n",
    "    }\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "def read_intents_from_store(file_path):\n",
    "    \"\"\"\n",
    "    Yields each intent record from the IntentStore file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            intent = json.loads(line)\n",
    "            yield intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LLM_conflict(existing_intent_flow_json, new_intent_flow_json):\n",
    "    system_prompt = CONFLICT_PROMPT_FLOODLIGHT\n",
    "\n",
    "    for model in my_models_conflict_real:\n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            try:\n",
    "                time.sleep(0.1)\n",
    "\n",
    "                response = client.generate(model=model,\n",
    "                    options={'temperature': 0.3, 'num_ctx': 8192, 'top_p': 0.5, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                    stream=False,\n",
    "                    system=system_prompt,\n",
    "                    prompt=f\"Flow 1:\\n{json.dumps(existing_intent_flow_json, indent=2)}\\n\\nFlow 2:\\n{json.dumps(new_intent_flow_json, indent=2)}\",\n",
    "                    format='json'\n",
    "                )\n",
    "\n",
    "                output = response['response'].strip()\n",
    "\n",
    "                response_json = json.loads(output)\n",
    "\n",
    "                if 'conflict_status' not in response_json:\n",
    "                    #print(\"\\nWarning: 'conflict_status' key is missing in the response.\\n\")\n",
    "                    break\n",
    "                else:\n",
    "                    valid_conflict_response = True\n",
    "                    conflict_status = response_json.get('conflict_status', 0)\n",
    "                    # Ensure conflict_status is an integer\n",
    "                    if isinstance(conflict_status, str):\n",
    "                        conflict_status = int(conflict_status)\n",
    "\n",
    "                    return valid_conflict_response, conflict_status, response_json['conflict_explanation']             \n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"Exception found: \", e)\n",
    "                sys.stdout.flush()\n",
    "                if(count<15):\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"\\n\",model, \" failed to produce valid JSON for conflict info after 15 tries. Going to next model\\n\")\n",
    "                    break               \n",
    "    \n",
    "    return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conflict(device_dpid_str, new_intent_flow_json):\n",
    "    \"\"\"\n",
    "    Checks for conflicts against operational flows.\n",
    "    device_dpid_str must be in colon-hex format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use your new Floodlight helper function\n",
    "    #existing_flows = get_operational_flows_floodlight(device_dpid_str) # <--- NEW\n",
    "\n",
    "    # The rest of this function must be adapted for the operational flow format\n",
    "    # Floodlight's operational flow format is different from its static flow format.\n",
    "    # It has 'match' and 'actions' as sub-objects.\n",
    "    # The CONFLICT_PROMPT_FLOODLIGHT is designed for the STATIC format.\n",
    "    \n",
    "    # This is a problem. The conflict LLM is trained on the *static* format,\n",
    "    # but the 'existing_flows' are in *operational* format.\n",
    "    \n",
    "    # This is solved as the static flow rules can be fetched and used instaed of operational ones.\n",
    "    \n",
    "    # Let's try fetching STATIC flows instead for a true apples-to-apples comparison.\n",
    "    existing_static_flows = get_static_flows_floodlight(device_dpid_str)\n",
    "    \n",
    "    #\n",
    "    # *** DEBUGGING NOTE: ***\n",
    "    # If you want to conflict against ALL flows (including those from\n",
    "    # reactive apps), use get_operational_flows_floodlight() and update\n",
    "    # the conflict prompt to handle the { \"match\": {...}, \"actions\": [...] } format.\n",
    "    # For now, we conflict against other static flows.\n",
    "    #\n",
    "    \n",
    "    for existing_flow in existing_static_flows:\n",
    "\n",
    "        # --- START: ROBUST HARD-CODED EXCLUSION ---\n",
    "        # This filter needs to check the flat JSON structure\n",
    "        try:\n",
    "            # new_intent_flow_json is flat\n",
    "            match1 = new_intent_flow_json\n",
    "            # existing_flow is also flat\n",
    "            match2 = existing_flow\n",
    "\n",
    "            dl_dst_1 = str(match1.get('eth_dst', ''))\n",
    "            dl_dst_2 = str(match2.get('eth_dst', ''))\n",
    "            \n",
    "            is_m1_l2_special = dl_dst_1.startswith('01:80:c2:00:00:0') \n",
    "            is_m2_l2_special = dl_dst_2.startswith('01:80:c2:00:00:0')\n",
    "\n",
    "            eth_type_1 = str(match1.get('eth_type', '')).lower()\n",
    "            eth_type_2 = str(match2.get('eth_type', '')).lower()\n",
    "            \n",
    "            is_m1_ip_arp = eth_type_1 in ['2048', '0x0800', '2054', '0x0806'] or 'ipv4_dst' in match1 or 'ipv4_src' in match1\n",
    "            is_m2_ip_arp = eth_type_2 in ['2048', '0x0800', '2054', '0x0806'] or 'ipv4_dst' in match2 or 'ipv4_src' in match2\n",
    "\n",
    "            if (is_m1_l2_special and is_m2_ip_arp) or (is_m1_ip_arp and is_m2_l2_special):\n",
    "                continue\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Pre-filter check failed: {e}\")\n",
    "            pass\n",
    "        # --- END: ROBUST HARD-CODED EXCLUSION ---\n",
    "\n",
    "        valid_conflict_response, conflict_status, conflict_details = run_LLM_conflict(existing_flow, new_intent_flow_json)\n",
    "\n",
    "        if (valid_conflict_response == False):\n",
    "            return 2, None, None\n",
    "        elif (conflict_status == 1):\n",
    "            return conflict_status, conflict_details, existing_flow\n",
    "\n",
    "    return 0, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LLM_IBN(intent, device_id_str):\n",
    "    \n",
    "    # NOTE: 'device_id_str' is the DPID, but the LLM prompt\n",
    "    # is designed to extract this from the intent text itself.\n",
    "    # We pass it here in case we want to use it later.\n",
    "\n",
    "    for num_examples in context_examples:\n",
    "        for model in my_models_translate_real:\n",
    "            \n",
    "            # !! REMINDER !!\n",
    "            # The 'trainset' (from Intent2Flow-Floodlight.csv) MUST\n",
    "            # have examples that match the TRANSLATION_PROMPT_FLOODLIGHT format.\n",
    "            example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "                [{\"instruction\": trainset.iloc[0][\"instruction\"], \"output\": trainset.iloc[0][\"output\"]}],\n",
    "                ollama_emb,\n",
    "                Chroma,\n",
    "                input_keys=[\"instruction\"],\n",
    "                k=num_examples,\n",
    "                vectorstore_kwargs={\"fetch_k\": min(num_examples, len(trainset))}\n",
    "                )\n",
    "            # Clear and add all remaining examples from the trainset\n",
    "            example_selector.vectorstore.reset_collection()\n",
    "            \n",
    "            for _, row in trainset.iterrows():\n",
    "                example_selector.add_example({\n",
    "                    \"instruction\": row[\"instruction\"],\n",
    "                    \"output\": row[\"output\"]\n",
    "                })\n",
    "            \n",
    "            # Use the new Floodlight-compatible prompt\n",
    "            system_prompt = TRANSLATION_PROMPT_FLOODLIGHT\n",
    "            count = 0\n",
    "\n",
    "            while True:\n",
    "                count+=1\n",
    "                try:\n",
    "                    time.sleep(0.1)\n",
    "                    example_str = \"\" #\n",
    "                    if num_examples > 0:\n",
    "                        examples = example_selector.select_examples({\"instruction\": intent})\n",
    "                        example_str = \"\\n\\n\\n\".join(map(lambda x: \"Input: \" + x[\"instruction\"] + \"\\n\\nOutput: \" + x[\"output\"], examples))\n",
    "                        # Append examples to the system prompt\n",
    "                        full_system_prompt = system_prompt + \"\\n\\n\" + example_str + \"\\n\\n\\n\"\n",
    "                    else:\n",
    "                        full_system_prompt = system_prompt\n",
    "                        \n",
    "                    response = client.generate(model=model,\n",
    "                        options={'temperature': 0.6, 'num_ctx': 8192, 'top_p': 0.3, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                        stream=False,\n",
    "                        system=full_system_prompt,\n",
    "                        prompt=intent,\n",
    "                        format='json'\n",
    "                    )\n",
    "                    actual_output = response['response']\n",
    "                    break\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(\"Exception on Input: \", e)\n",
    "                    print(\"\\nCheck example_str same or not: \\n\",example_str)\n",
    "                    sys.stdout.flush()\n",
    "                    if(count<15):\n",
    "                        continue\n",
    "                    else:\n",
    "                        print(\"\\n\",model, \" failed to produce valid JSON for translation info after 15 tries. Going to next model\\n\")\n",
    "                        break \n",
    "            try:\n",
    "                flow_json = json.loads(actual_output)\n",
    "                \n",
    "                # The LLM should have generated the \"switch\" key.\n",
    "                # If not, we could inject it, but the prompt is strict.\n",
    "                if \"switch\" not in flow_json:\n",
    "                    print(f\"Warning: LLM output missing 'switch' key. Injecting DPID {device_id_str}.\")\n",
    "                    flow_json[\"switch\"] = device_id_str\n",
    "\n",
    "                return flow_json\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Exception found parsing LLM output: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LLM_Slice(intent):\n",
    "\n",
    "    system_prompt = SLICING_PROMPT\n",
    "    \n",
    "    for model in my_models_translate_real:     \n",
    "        try:\n",
    "            time.sleep(0.1)             \n",
    "            response = client.generate(model=model,\n",
    "                options={'temperature': 0.3, 'num_ctx': 8192, 'top_p': 0.5, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                #options={'device': 'cpu'},\n",
    "                stream=False,\n",
    "                system=system_prompt,\n",
    "                prompt=intent,\n",
    "                format='json'\n",
    "            )\n",
    "            \n",
    "            output = response['response'].strip()\n",
    "            response_json = json.loads(output)\n",
    "            \n",
    "            #print(\"\\nCheckpoint*******Exiting LLM Slicing detection\\n\\n******\")\n",
    "            return response_json            \n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"Exception found: \", e)\n",
    "            sys.stdout.flush()\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dpid_string_floodlight(intent: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extracts the integer datapath ID (dpid) and formats it\n",
    "    as a Floodlight-compatible colon-hex string.\n",
    "    \"\"\"\n",
    "    match = re.search(r'\\b(?:switch|dpid|node|device)(?:_id)?\\s*(\\d+)', intent, re.IGNORECASE)\n",
    "    if match:\n",
    "        dpid_int = int(match.group(1))\n",
    "        # Format as 00:00:00:00:00:00:00:0X\n",
    "        return f\"00:00:00:00:00:00:00:{dpid_int:02x}\"\n",
    "    return None\n",
    "\n",
    "def get_iface_for_port(device_id_int: int, port_no: int | str) -> str:\n",
    "    \"\"\"\n",
    "    Builds the Mininet/OVS interface name (e.g., s4-eth1) from an\n",
    "    integer DPID (device_id) and port number.\n",
    "    This is for OVS commands, so it's controller-agnostic.\n",
    "    \"\"\"\n",
    "    switch_name = f\"s{device_id_int}\"\n",
    "    return f\"{switch_name}-eth{port_no}\"\n",
    "\n",
    "def classify_floodlight_flow_rule(flow_rule: dict):\n",
    "    \"\"\"\n",
    "    Classify a Floodlight static flow rule into a type and compute its specificity.\n",
    "    This version handles the flat JSON structure.\n",
    "    \n",
    "    Returns: (rule_type: str, specificity: float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- 1. Rule Type Detection ---\n",
    "    # Actions are a single comma-separated string, or absent for DROP\n",
    "    actions_str = flow_rule.get(\"actions\")\n",
    "    rule_type = \"unknown\"\n",
    "\n",
    "    if actions_str is None:\n",
    "        # actions key is absent\n",
    "        rule_type = \"security\" \n",
    "    elif \"set_queue\" in actions_str:\n",
    "        rule_type = \"qos\"\n",
    "    elif \"output\" in actions_str:\n",
    "        rule_type = \"forwarding\"\n",
    "    else:\n",
    "        # Includes empty string \"\" which is also a drop\n",
    "        rule_type = \"security\"\n",
    "\n",
    "    # --- 2. Specificity Computation ---\n",
    "    specificity = 0.0\n",
    "    \n",
    "    # Define keys that are *not* match fields\n",
    "    control_keys = {\"switch\", \"name\", \"active\", \"priority\", \"actions\"}\n",
    "\n",
    "    for key, value in flow_rule.items():\n",
    "        if key in control_keys:\n",
    "            continue\n",
    "        \n",
    "        # This is a match key\n",
    "        specificity += 1.0 \n",
    "        \n",
    "        if key in (\"ipv4_src\", \"ipv4_dst\", \"ipv6_src\", \"ipv6_dst\") and value:\n",
    "            try:\n",
    "                ip_net = ipaddress.ip_network(value, strict=False) \n",
    "                if \"v6\" in key:\n",
    "                    specificity += (ip_net.prefixlen / 128.0)\n",
    "                else:\n",
    "                    specificity += (ip_net.prefixlen / 32.0)\n",
    "            except Exception:\n",
    "                pass \n",
    "                \n",
    "    return rule_type, specificity\n",
    "\n",
    "def resolve_floodlight_conflict(rule1, rule2):\n",
    "    \"\"\"\n",
    "    Resolve conflict between two Floodlight rules using Type > Specificity > Priority.\n",
    "    Returns: winner_rule, loser_rule\n",
    "    \"\"\"\n",
    "    type_priority = {\"security\": 3, \"qos\": 2, \"forwarding\": 1, \"unknown\": 0}\n",
    "\n",
    "    type1, spec1 = classify_floodlight_flow_rule(rule1)\n",
    "    type2, spec2 = classify_floodlight_flow_rule(rule2)\n",
    "\n",
    "    p1 = int(rule1.get(\"priority\", 0))\n",
    "    p2 = int(rule2.get(\"priority\", 0))\n",
    "\n",
    "    # --- Resolution Hierarchy ---\n",
    "    \n",
    "    # 1. Type\n",
    "    type1_score = type_priority.get(type1, 0)\n",
    "    type2_score = type_priority.get(type2, 0)\n",
    "    \n",
    "    if type1_score > type2_score:\n",
    "        return rule1, rule2\n",
    "    elif type2_score > type1_score:\n",
    "        return rule2, rule1\n",
    "\n",
    "    # 2. Specificity\n",
    "    if spec1 > spec2:\n",
    "        return rule1, rule2\n",
    "    elif spec2 > spec1:\n",
    "        return rule2, rule1\n",
    "\n",
    "    # 3. Priority\n",
    "    if p1 > p2:\n",
    "        return rule1, rule2\n",
    "    elif p2 > p1:\n",
    "        return rule2, rule1\n",
    "\n",
    "    # Tie\n",
    "    return None, None\n",
    "\n",
    "def adjust_priority_floodlight(winner_rule: dict, loser_rule: dict, step: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Adjusts the priority of the winning Floodlight flow rule.\n",
    "    Priority is a string in Floodlight, but we treat it as int.\n",
    "    \"\"\"\n",
    "    loser_priority = int(loser_rule.get(\"priority\", 0))\n",
    "    winner_priority = int(winner_rule.get(\"priority\", 0))\n",
    "\n",
    "    new_priority = max(winner_priority, loser_priority + step)\n",
    "    \n",
    "    if new_priority > 65535:\n",
    "        print(f\"Warning: Calculated priority ({new_priority}) exceeds max (65535). Capping at 65535.\")\n",
    "        new_priority = 65535\n",
    "\n",
    "    # Store it as a string, as required by the prompt\n",
    "    winner_rule[\"priority\"] = str(new_priority)\n",
    "    return winner_rule\n",
    "\n",
    "def extract_inner_flow(rule):\n",
    "    # This function is no longer needed for Floodlight's flat JSON,\n",
    "    # but we keep it to avoid breaking end_to_end_IBN\n",
    "    return rule\n",
    "\n",
    "def normalize_ip_prefix(ip_str):\n",
    "    \"\"\"Converts '10.0.1.1' to '10.0.1.1/32'.\"\"\"\n",
    "    if not ip_str:\n",
    "        return None\n",
    "    ip_str = str(ip_str)\n",
    "    if \"/\" not in ip_str:\n",
    "        try:\n",
    "            ipaddress.ip_address(ip_str)\n",
    "            return f\"{ip_str}/32\"\n",
    "        except ValueError:\n",
    "            return ip_str \n",
    "    return ip_str\n",
    "\n",
    "def normalize_numeric_field(field_str):\n",
    "    \"\"\"Converts '0x800', '0x11', '17', '80' to a standard integer.\"\"\"\n",
    "    if not field_str:\n",
    "        return None\n",
    "    try:\n",
    "        # int(str, 0) handles hex and decimal\n",
    "        return int(str(field_str), 0)\n",
    "    except (ValueError, TypeError):\n",
    "        return str(field_str) # Fallback\n",
    "\n",
    "def get_operational_flow_by_match(dpid_str: str, static_flow_rule: dict):\n",
    "    \"\"\"\n",
    "    Finds the *operational* flow (with stats) that matches the\n",
    "    match fields of a *static* flow rule.\n",
    "    \"\"\"\n",
    "    op_flows = get_operational_flows_floodlight(dpid_str)\n",
    "    \n",
    "    # --- 1. Build a NORMALIZED Target Match from Static Rule ---\n",
    "    control_keys = {\"switch\", \"name\", \"active\", \"priority\", \"actions\"}\n",
    "    target_match_normalized = {}\n",
    "    \n",
    "    for k, v in static_flow_rule.items():\n",
    "        if k in control_keys:\n",
    "            continue\n",
    "        \n",
    "        # Normalize the target values *before* comparison\n",
    "        if k in (\"ipv4_src\", \"ipv4_dst\", \"arp_spa\", \"arp_tpa\"):\n",
    "            target_match_normalized[k] = normalize_ip_prefix(v)\n",
    "        elif k in (\"eth_src\", \"eth_dst\", \"arp_sha\", \"arp_tha\"):\n",
    "            target_match_normalized[k] = str(v).lower()\n",
    "        else:\n",
    "            # This handles eth_type, ip_proto, ports, etc.\n",
    "            target_match_normalized[k] = normalize_numeric_field(v)\n",
    "            \n",
    "    target_prio = int(static_flow_rule.get(\"priority\", 0))\n",
    "\n",
    "    if not op_flows:\n",
    "        print(\"[WARN] get_operational_flow_by_match: Received no operational flows from switch.\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Iterate and Compare ---\n",
    "    for op_flow in op_flows:\n",
    "        \n",
    "        op_prio_int = int(op_flow.get(\"priority\", -1))\n",
    "        if op_prio_int != target_prio:\n",
    "            continue\n",
    "            \n",
    "        op_match = op_flow.get(\"match\", {})\n",
    "        \n",
    "        # 2a. Check for exact length match\n",
    "        if len(op_match) != len(target_match_normalized):\n",
    "            continue\n",
    "            \n",
    "        # 2b. Build a NORMALIZED Operational Match\n",
    "        op_match_normalized = {}\n",
    "        for k, v in op_match.items():\n",
    "            if k in (\"ipv4_src\", \"ipv4_dst\", \"arp_spa\", \"arp_tpa\"):\n",
    "                op_match_normalized[k] = normalize_ip_prefix(v)\n",
    "            elif k in (\"eth_src\", \"eth_dst\", \"arp_sha\", \"arp_tha\"):\n",
    "                op_match_normalized[k] = str(v).lower()\n",
    "            else:\n",
    "                op_match_normalized[k] = normalize_numeric_field(v)\n",
    "        \n",
    "        # 2c. Compare the two normalized dictionaries\n",
    "        if op_match_normalized == target_match_normalized:\n",
    "            return op_flow # Found it!\n",
    "            \n",
    "    # --- 3. No Match Found ---\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_to_end_IBN(intent):\n",
    "\n",
    "    # 1. Extract DPID\n",
    "    device_dpid_str = extract_dpid_string_floodlight(intent)\n",
    "\n",
    "    if device_dpid_str is None:\n",
    "        print(\"Error: Could not determine switch DPID from intent.\")\n",
    "        return False, None, None, None, None\n",
    "\n",
    "    global switch_id_for_llm_assurance \n",
    "    # Store the integer part for OVS helpers\n",
    "    switch_id_for_llm_assurance = int(device_dpid_str.split(\":\")[-1], 16)\n",
    "\n",
    "    # 2. Translate Intent\n",
    "    # The LLM will generate the full JSON, including \"switch\" and \"name\"\n",
    "    intent_JSON = run_LLM_IBN(intent, device_dpid_str)\n",
    "    \n",
    "    if not intent_JSON or \"switch\" not in intent_JSON or \"name\" not in intent_JSON:\n",
    "         print(\"Error: LLM translation failed or JSON is malformed (missing 'switch' or 'name').\")\n",
    "         return False, None, None, None, None\n",
    "         \n",
    "    # The DPID from the JSON is the source of truth\n",
    "    device_dpid_str = intent_JSON[\"switch\"]\n",
    "\n",
    "    # Get the base name from the LLM\n",
    "    base_name = intent_JSON.get(\"name\", \"flow-unnamed\")\n",
    "    # Create a unique suffix (e.g., 'a1b2c3d4')\n",
    "    unique_suffix = secrets.token_hex(4)\n",
    "    # Create the new, guaranteed-unique name\n",
    "    flow_name = f\"{base_name}-{unique_suffix}\"\n",
    "    \n",
    "    # Update the JSON object with the new unique name before proceeding\n",
    "    intent_JSON[\"name\"] = flow_name\n",
    "\n",
    "    global gl_flow_name\n",
    "    gl_flow_name = flow_name\n",
    "\n",
    "    # 3. Conflict Detection\n",
    "    conflict_status, conflict_details, which_flow_conflict = conflict(device_dpid_str, intent_JSON)\n",
    "\n",
    "    if((conflict_status != 1) and (conflict_status != 0)):\n",
    "        print(\"\\nCheck Conflict Detection Module, LLM did not produce a valid JSON for conflict detection.\\n\")\n",
    "        return False, None, None, None, None\n",
    "\n",
    "    elif (conflict_status == 1):\n",
    "        print(\"Conflict detected. Resolving...\")\n",
    "        winner, non_winner = resolve_floodlight_conflict(intent_JSON, which_flow_conflict)\n",
    "\n",
    "        if winner is None:\n",
    "            print(\"\\nConflict resolution resulted in a tie.\")\n",
    "            print(f\"\\nThe New flow rule:\\n{intent_JSON}\\nConflicts with existing rule:\\n{which_flow_conflict}\")\n",
    "            print(f\"\\nExisting Flow Rule Location: In switch: {device_dpid_str}\\nConflict Details: {conflict_details}\")\n",
    "            return False, \"Tie\", None, None, None\n",
    "            \n",
    "        elif winner.get(\"name\") != flow_name:\n",
    "            # The existing rule won\n",
    "            print(f\"\\nExisting Flow Rule that Conflicts (and won):\\n{winner}\")\n",
    "            print(f\"\\nThe New Flow Rule Attempted (and lost):\\n{non_winner}\")\n",
    "            print(f\"\\nConflict Details : {conflict_details}\")\n",
    "            return False, \"existing_rule_win\", None, None, None\n",
    "            \n",
    "        else:\n",
    "            # The new rule won, needs priority adjustment\n",
    "            print(f\"Conflict Resolved. Winner Flow Rule (New):\\n{winner}\")\n",
    "            print(f\"\\nShadowed Flow Rule (Existing):\\n{non_winner}\")\n",
    "            print(f\"\\nConflict Details : {conflict_details}\")\n",
    "            updated_flow_json = adjust_priority_floodlight(winner, non_winner)\n",
    "            intent_JSON = updated_flow_json   \n",
    "\n",
    "    # 4. Install Flow\n",
    "    try:\n",
    "        success = add_flow_floodlight(intent_JSON)\n",
    "        if not success:\n",
    "            raise Exception(\"add_flow_floodlight returned False\")\n",
    "        print(f\"Successfully pushed flow '{flow_name}' to dpid: {device_dpid_str}\")\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Exception found while installing flow rule: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            return False, None, None, None, None\n",
    "    \n",
    "    # 5. Verify Installation\n",
    "    try:\n",
    "        time.sleep(.5) # Give controller time to install\n",
    "        \n",
    "        # Use the new exists_floodlight() helper\n",
    "        verification_status, returned_static_flow = exists_floodlight(\n",
    "            device_dpid_str, \n",
    "            flow_name\n",
    "        )\n",
    "        \n",
    "        if(verification_status == True):\n",
    "            time.sleep(.5)\n",
    "            # --- NEW: Fetch the operational rule ---\n",
    "            #print(\"Now, fetching corresponding operational flow rule for assurance...\")\n",
    "            operational_rule_with_stats = get_operational_flow_by_match(\n",
    "                device_dpid_str, \n",
    "                intent_JSON # Use the rule we pushed to match against\n",
    "            )\n",
    "            \n",
    "            if not operational_rule_with_stats:\n",
    "                print(\"[WARN] Static rule found, but matching operational rule not found on switch yet.\")\n",
    "                # We'll return the static rule as a fallback\n",
    "                operational_rule_with_stats = intent_JSON \n",
    "            else:\n",
    "                print(\"Successfully fetched operational flow rule.\")\n",
    "            # --- END NEW ---\n",
    "\n",
    "            flow_id = flow_name\n",
    "            \n",
    "            # The 'operational_flow_rule' from static list is what we *pushed*.\n",
    "            # For assurance, we need the *operational* flow with stats.\n",
    "            # We'll fetch that in the assurance step.\n",
    "            # For now, return the *intended* flow rule.\n",
    "            \n",
    "            # Get the integer dpid for OVS helpers\n",
    "            dpid_int = int(device_dpid_str.split(\":\")[-1], 16)\n",
    "            \n",
    "            return True, flow_id, dpid_int, intent_JSON, operational_rule_with_stats\n",
    "        else:\n",
    "             print(\"Flow verification failed. Rule not found in switch's static flow list.\")\n",
    "             return False, None, None, None, None\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Exception found while verifying flow rule: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            return False, None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corrective_action_prompt_floodlight(intent_nl, operational_flow_rule, dpid_str, ping_count,\n",
    "                                                 candidate_src_ip, candidate_dst_ip, ping_output):\n",
    "    \"\"\"\n",
    "    Generates a prompt for the LLM to suggest corrective actions for a FAILED security intent.\n",
    "    This version is adapted for Floodlight's flat flow format.\n",
    "    'operational_flow_rule' is the rule that was pushed (static format).\n",
    "    \"\"\"\n",
    "    # Get existing operational flows from the switch\n",
    "    # Note: these are in OPERATIONAL format (with 'match' objects)\n",
    "    # This might be confusing for the LLM. Let's send the STATIC flows.\n",
    "    existing_flows = get_static_flows_floodlight(dpid_str)\n",
    "\n",
    "    prompt_sections = []\n",
    "    prompt_sections.append(\n",
    "        \"You are an SDN network assistant. Your task is to recommend a ranked list of corrective actions \"\n",
    "        \"to enforce a **security intent** that failed during assurance testing.\"\n",
    "    )\n",
    "    prompt_sections.append(f\"1. **Security Intent (in Natural Language)**:\\n{intent_nl}\")\n",
    "\n",
    "    # Use Floodlight's flow format\n",
    "    prompt_sections.append(\"2. **Floodlight Flow Rule for the Security Intent (Pushed to Switch)**:\")\n",
    "    prompt_sections.append(json.dumps(operational_flow_rule, indent=2))\n",
    "\n",
    "    prompt_sections.append(\"3. **Existing Static Flow Rules in the Same Switch (DPID)**:\")\n",
    "    prompt_sections.append(json.dumps(existing_flows, separators=(\",\", \":\")))\n",
    "\n",
    "    prompt_sections.append(\n",
    "        f\"\"\"4. **Assurance Test Result**:\n",
    "        - Ping Source IP: {candidate_src_ip}\n",
    "        - Ping Destination IP: {candidate_dst_ip}\n",
    "        - Ping Count: {ping_count}\n",
    "        - Ping Output:\n",
    "        {ping_output}\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Instructions updated for Floodlight's JSON structure\n",
    "    prompt_sections.append(\n",
    "        \"---\\nNow, based on the above data, generate a ranked list of corrective actions. \"\n",
    "        \"You must use only the following predefined corrective actions:\\n\"\n",
    "        \"1. Correct Match Fields\\n\"\n",
    "        \"2. Increase Priority\\n\"\n",
    "        \"3. Fix Action Field\\n\\n\"\n",
    "        \"For each action:\\n\"\n",
    "        \"- **For 'Correct Match Fields'**: Specify *exactly* which top-level key-value pairs should be set (e.g., \\\"ipv4_src\\\": \\\"10.0.1.1\\\", \\\"eth_type\\\": \\\"0x0800\\\"), and which keys (if any) should be removed.\\n\"\n",
    "        \"- **For 'Increase Priority'**: Indicate which existing rule(s) (by 'name') are overshadowing the candidate rule, their current priority value(s), and the exact priority value the candidate rule should be set to.\\n\"\n",
    "        \"- **For 'Fix Action Field'**: Only include this if the 'actions' key is present. The suggestion should be to REMOVE the 'actions' key entirely.\\n\\n\"\n",
    "        \"Rank the actions and explain your reasoning.\\n\"\n",
    "        \"Return your answer ONLY in the following strict JSON format:\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"recommended_actions\\\": [\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"rank\\\": 1,\\n\"\n",
    "        \"      \\\"action\\\": \\\"Correct Match Fields\\\",\\n\"\n",
    "        \"      \\\"suggestion\\\": {\\n\"\n",
    "        \"        \\\"set_fields\\\": {\\\"ipv4_src\\\": \\\"10.0.0.1\\\", \\\"ipv4_dst\\\": \\\"10.0.0.2\\\"},\\n\"\n",
    "        \"        \\\"remove_fields\\\": [\\\"tcp_dst\\\"],\\n\"\n",
    "        \"        \\\"reasoning\\\": \\\"The match fields do not match the intended source and destination.\\\"\\n\"\n",
    "        \"      }\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"      \\\"rank\\\": 2,\\n\"\n",
    "        \"      \\\"action\\\": \\\"Increase Priority\\\",\\n\"\n",
    "        \"      \\\"suggestion\\\": {\\n\"\n",
    "        \"        \\\"conflicting_rules\\\": [ {\\\"name\\\": \\\"existing-flow-1\\\", \\\"priority\\\": \\\"400\\\"} ],\\n\"\n",
    "        \"        \\\"recommended_priority\\\": 410,\\n\"\n",
    "        \"        \\\"reasoning\\\": \\\"The candidate rule is overshadowed by a rule with higher priority.\\\"\\n\"\n",
    "        \"      }\\n\"\n",
    "        \"    }\\n\"\n",
    "        \"  ]\\n\"\n",
    "        \"}\\n\"\n",
    "        \"Omit any action that is not relevant.\"\n",
    "    )\n",
    "    return \"\\n\\n\".join(prompt_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_assurance_LLM (assurance_prompt):\n",
    "    \"\"\"\n",
    "    (This function is controller-agnostic and does not need changes.)\n",
    "    Sends the prompt to the LLM and gets a JSON response.\n",
    "    \"\"\"\n",
    "    for model in my_models_conflict_real:    \n",
    "        try:\n",
    "            time.sleep(0.1)             \n",
    "            response = client.generate(model=model,\n",
    "                options={'temperature': 0.3, 'num_ctx': 8192, 'top_p': 0.5, 'num_predict': 1024, 'num_gpu': 99},\n",
    "                stream=False,\n",
    "                system=\"\",\n",
    "                prompt=assurance_prompt,\n",
    "                format='json'\n",
    "            )\n",
    "            output = response['response'].strip()\n",
    "            response_json = json.loads(output)\n",
    "            return response_json            \n",
    "        except Exception as e:\n",
    "            print(f\"Exception found at assurance LLM for corrective action generation: {e}\")\n",
    "            sys.stdout.flush()\n",
    "            continue\n",
    "    return None # Return None if all models fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_command_full(command, timeout=15, with_sudo=False):\n",
    "    # Run the command as-is or under sudo (single layer)\n",
    "    cmd = command\n",
    "    if with_sudo:\n",
    "        # -S: read password from stdin; -p '' suppresses the prompt text\n",
    "        cmd = f\"sudo -S -p '' {command}\"\n",
    "        cmd = f\"printf '%s\\\\n' {shlex.quote(sudo_password)} | {cmd}\"\n",
    "    try:\n",
    "        res = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=timeout)\n",
    "        return res.stdout, res.stderr\n",
    "    except subprocess.TimeoutExpired as e:\n",
    "        print(f\"Command timed out: {command}\")\n",
    "        stdout_str = e.stdout if isinstance(e.stdout, str) else (e.stdout.decode('utf-8', 'ignore') if e.stdout else \"\")\n",
    "        stderr_str = e.stderr if isinstance(e.stderr, str) else (e.stderr.decode('utf-8', 'ignore') if e.stderr else \"Timeout\")\n",
    "        return stdout_str, stderr_str\n",
    "\n",
    "def get_mininet_host_pid(src_host):\n",
    "    \"\"\"\n",
    "    Robustly get the PID of a Mininet host process (e.g., 'h1' or 'h1onos') regardless of user.\n",
    "    Looks for a process with command containing 'mininet:<src_host>'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ps_output = subprocess.check_output([\"ps\", \"-eo\", \"pid,args\"], text=True).strip().splitlines()\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        raise RuntimeError(\"Failed to run 'ps -eo pid,args'\") from e\n",
    "\n",
    "    for line in ps_output:\n",
    "        if f\"mininet:{src_host}\" in line and \"grep\" not in line:\n",
    "            parts = line.strip().split(None, 1)\n",
    "            if len(parts) == 2:\n",
    "                pid_str, cmd = parts\n",
    "                try:\n",
    "                    pid = int(pid_str)\n",
    "                    return pid\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    raise RuntimeError(f\"No Mininet host process found for '{src_host}'.\")\n",
    "\n",
    "def extract_host_and_ip_floodlight(flow_data: dict):\n",
    "    \"\"\"\n",
    "    Extracts source and destination IPs from a Floodlight static flow rule (flat format).\n",
    "    Returns: (src_host, dst_host, src_ip, dst_ip)\n",
    "    \"\"\"\n",
    "    src_ip = dst_ip = None\n",
    "    \n",
    "    # Match keys are at the top level\n",
    "    if \"ipv4_src\" in flow_data:\n",
    "        src_ip = flow_data[\"ipv4_src\"].split(\"/\")[0]\n",
    "    if \"ipv4_dst\" in flow_data:\n",
    "        dst_ip = flow_data[\"ipv4_dst\"].split(\"/\")[0]\n",
    "\n",
    "    src_host = ip_to_host.get(src_ip) if src_ip else None\n",
    "    dst_host = ip_to_host.get(dst_ip) if dst_ip else None\n",
    "\n",
    "    return src_host, dst_host, src_ip, dst_ip\n",
    "\n",
    "def floodlight_assurance_for_security_intent(src_ip, dst_ip, rule_dpid_int, ping_count=2):\n",
    "    \"\"\"\n",
    "    Tests the data plane to assure a security (block) intent.\n",
    "    'rule_dpid_int' is the integer DPID of the switch where the rule lives.\n",
    "    \"\"\"\n",
    "    all_ips = [\"10.0.1.1\", \"10.0.1.2\", \"10.0.1.3\", \"10.0.1.4\"]\n",
    "    global llm_caller_flag\n",
    "    llm_caller_flag = 0\n",
    "\n",
    "    def perform_ping(source_ip, target_ip):\n",
    "        source_host = ip_to_host.get(source_ip)\n",
    "        if not source_host:\n",
    "            print(f\"[WARNING] Unknown source host for IP: {source_ip}\")\n",
    "            return \"\"\n",
    "        try:\n",
    "            host_pid = get_mininet_host_pid(source_host)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Cannot get PID for host {source_host}: {e}\")\n",
    "            return \"\"\n",
    "        ping_cmd = f\"mnexec -a {host_pid} ping -c {ping_count} {target_ip}\"\n",
    "        out, err = execute_command_full(ping_cmd, with_sudo=True)\n",
    "        return f\"{out or ''}{err or ''}\"\n",
    "\n",
    "    # Case 1: src and dst IP both present\n",
    "    if src_ip and dst_ip:\n",
    "        print(f\"[INFO] Testing {src_ip} to {dst_ip}\")\n",
    "        output = perform_ping(src_ip, dst_ip)\n",
    "        if output and \"100% packet loss\" in output:\n",
    "            print(f\"[PASS] Intent effective. Traffic from {src_ip} to {dst_ip} is blocked.\")\n",
    "        elif output and \"0% packet loss\" in output:\n",
    "            print(f\"[FAIL] Intent NOT effective. Traffic from {src_ip} to {dst_ip} is NOT blocked.\")\n",
    "            llm_caller_flag = 1\n",
    "        else:\n",
    "            print(\"[WARN] Inconclusive result. Ping output:\\n\", output, \"\\nPlease check first why ping is not working.\")\n",
    "            llm_caller_flag = 2\n",
    "        return ping_count, src_ip, dst_ip, output\n",
    "    \n",
    "    # Case 2: src_ip is None → test all src IPs → dst_ip\n",
    "    elif src_ip is None and dst_ip:\n",
    "        print(f\"[INFO] Testing multiple sources to {dst_ip}\")\n",
    "        candidate_src = \"\" # Keep scope\n",
    "        for candidate_src in all_ips:\n",
    "            if candidate_src == dst_ip:\n",
    "                continue\n",
    "            output = perform_ping(candidate_src, dst_ip)\n",
    "            if output and \"100% packet loss\" in output:\n",
    "                continue\n",
    "            elif output and \"0% packet loss\" in output:\n",
    "                print(f\"[FAIL] Intent NOT effective. {candidate_src} → {dst_ip} was NOT blocked.\")\n",
    "                llm_caller_flag = 1\n",
    "                return ping_count, candidate_src, dst_ip, output\n",
    "            else:\n",
    "                print(f\"[WARN] Inconclusive result from {candidate_src} → {dst_ip}:\\n{output}\", \"\\nPlease check first why ping is not working.\")\n",
    "                llm_caller_flag = 2\n",
    "                return ping_count, candidate_src, dst_ip, output\n",
    "        print(f\"[PASS] Intent effective. All sources blocked from reaching {dst_ip}.\")\n",
    "        return ping_count, candidate_src, dst_ip, \"\" # Return last output (which was empty)\n",
    "\n",
    "    # Case 3: dst_ip is None → test src_ip → all destinations\n",
    "    elif dst_ip is None and src_ip:\n",
    "        print(f\"[INFO] Testing {src_ip} for multiple destinations\")\n",
    "        candidate_dst = \"\" # Keep scope\n",
    "        for candidate_dst in all_ips:\n",
    "            if candidate_dst == src_ip:\n",
    "                continue\n",
    "            try:\n",
    "                # Use the colon-hex DPID formats from SW_OF\n",
    "                src_attach_dev_str, _ = HOST_ATTACH.get(src_ip, (None, None))\n",
    "                dst_attach_dev_str, _ = HOST_ATTACH.get(candidate_dst, (None, None))\n",
    "                \n",
    "                # Convert the integer rule_dpid (e.g., 2) to its string key (\"2\")\n",
    "                rule_dev_str = SW_OF.get(str(rule_dpid_int))\n",
    "\n",
    "                if (src_attach_dev_str and \n",
    "                    src_attach_dev_str == dst_attach_dev_str and \n",
    "                    src_attach_dev_str != rule_dev_str):\n",
    "                    \n",
    "                    print(f\"[INFO] Skipping test {src_ip} -> {candidate_dst} (local on {src_attach_dev_str}, rule is on {rule_dev_str})\")\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Topology check failed: {e}. Proceeding with test.\")\n",
    "            \n",
    "            output = perform_ping(src_ip, candidate_dst)\n",
    "            if output and \"100% packet loss\" in output:\n",
    "                continue\n",
    "            elif output and \"0% packet loss\" in output:\n",
    "                print(f\"[FAIL] Intent NOT effective. {src_ip} → {candidate_dst} was NOT blocked.\")\n",
    "                llm_caller_flag = 1\n",
    "                return ping_count, src_ip, candidate_dst, output\n",
    "            else:\n",
    "                print(f\"[WARN] Inconclusive result from {src_ip} → {candidate_dst}:\\n{output}\", \"\\nPlease check first why ping is not working.\")\n",
    "                llm_caller_flag = 2\n",
    "                return ping_count, src_ip, candidate_dst, output\n",
    "        print(f\"[PASS] Intent effective.\")\n",
    "        return ping_count, src_ip, candidate_dst, \"\"\n",
    "    \n",
    "    else:\n",
    "        print(\"[ERROR] Both source and destination IPs are missing. Cannot evaluate intent.\")\n",
    "        llm_caller_flag = 2\n",
    "        return ping_count, None, None, None\n",
    "\n",
    "def correct_match_fields_floodlight(candidate_flow, set_fields, remove_fields):\n",
    "    \"\"\"\n",
    "    Update candidate_flow's top-level match fields for Floodlight.\n",
    "    \"\"\"\n",
    "    # Remove fields\n",
    "    for field in remove_fields:\n",
    "        candidate_flow.pop(field, None)\n",
    "    \n",
    "    # Set/update fields\n",
    "    candidate_flow.update(set_fields)\n",
    "    \n",
    "    return candidate_flow\n",
    "\n",
    "def increase_priority_floodlight(candidate_flow, recommended_priority):\n",
    "    # Ensure priority is stored as a string\n",
    "    candidate_flow['priority'] = str(recommended_priority)\n",
    "    return candidate_flow\n",
    "\n",
    "def fix_action_field_floodlight(candidate_flow):\n",
    "    \"\"\"\n",
    "    Make sure 'actions' key is REMOVED (for a block rule).\n",
    "    \"\"\"\n",
    "    candidate_flow.pop(\"actions\", None)\n",
    "    return candidate_flow\n",
    "\n",
    "def parse_and_execute_corrective_actions_floodlight(candidate_flow, llm_response, dpid_int):\n",
    "    \"\"\"\n",
    "    Applies corrective actions suggested by the LLM for Floodlight.\n",
    "    'candidate_flow' is the static flow rule that failed verification.\n",
    "    'dpid_int' is the integer DPID.\n",
    "    \"\"\"\n",
    "    if not llm_response or \"recommended_actions\" not in llm_response:\n",
    "        print(\"LLM provided no valid corrective actions.\")\n",
    "        return False\n",
    "        \n",
    "    actions = sorted(llm_response[\"recommended_actions\"], key=lambda x: x[\"rank\"])\n",
    "    \n",
    "    # We need the original src/dst for re-verification\n",
    "    _, _, src_ip, dst_ip = extract_host_and_ip_floodlight(candidate_flow)\n",
    "    \n",
    "    # We need the flow's name to delete/update it\n",
    "    original_flow_name = candidate_flow.get(\"name\")\n",
    "    if not original_flow_name:\n",
    "        print(\"Error: Cannot execute corrective action on flow with no 'name'.\")\n",
    "        return False\n",
    "\n",
    "    print(\"Deleting the original, failed flow rule...\")\n",
    "    delete_flow_floodlight(original_flow_name)\n",
    "    \n",
    "    for action_item in actions:\n",
    "        action = action_item[\"action\"]\n",
    "        suggestion = action_item.get(\"suggestion\", {})\n",
    "        print(f\"Triggering action: {action} (rank {action_item['rank']})\")\n",
    "\n",
    "        corrected_flow = candidate_flow.copy()\n",
    "        \n",
    "        if action == \"Correct Match Fields\":\n",
    "            set_fields = suggestion.get(\"set_fields\", {})\n",
    "            remove_fields = suggestion.get(\"remove_fields\", [])\n",
    "            corrected_flow = correct_match_fields_floodlight(corrected_flow, set_fields, remove_fields)\n",
    "        \n",
    "        elif action == \"Increase Priority\":\n",
    "            recommended_priority = suggestion.get(\"recommended_priority\")\n",
    "            if recommended_priority:\n",
    "                corrected_flow = increase_priority_floodlight(corrected_flow, recommended_priority)\n",
    "            else:\n",
    "                print(\"Warning: LLM suggested priority increase but gave no value.\")\n",
    "                continue\n",
    "                \n",
    "        elif action == \"Fix Action Field\":\n",
    "            corrected_flow = fix_action_field_floodlight(corrected_flow)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Unknown action: {action}\")\n",
    "            continue\n",
    "\n",
    "        # Ensure the name remains the same\n",
    "        corrected_flow[\"name\"] = original_flow_name\n",
    "        \n",
    "        # 2. Push corrected flow\n",
    "        print(f\"Pushing corrected flow: {json.dumps(corrected_flow)}\")\n",
    "        try:\n",
    "            add_flow_floodlight(corrected_flow)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"Exception while pushing corrected flow: {e}\")\n",
    "            continue # Try next action\n",
    "\n",
    "        # 3. Re-verify\n",
    "        print(\"Re-verifying intent...\")\n",
    "        global llm_caller_flag\n",
    "        # This will ping and set llm_caller_flag (0=PASS, 1=FAIL, 2=WARN)\n",
    "        floodlight_assurance_for_security_intent(src_ip, dst_ip, dpid_int) \n",
    "        \n",
    "        if llm_caller_flag == 0:\n",
    "            print(f\"Intent deviation resolved after action: {action}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Deviation not fixed after action: {action}. Deleting flow and trying next action...\")\n",
    "            delete_flow_floodlight(original_flow_name) # Clean up\n",
    "\n",
    "    print(\"\\nAll suggested corrective actions exhausted, but deviation remains. Escalate to operator.\")\n",
    "    return False\n",
    "\n",
    "def Floodlight_assurance_for_qos_intent(src_host, dst_host, src_ip, dst_ip, ping_count=2):\n",
    "    pass\n",
    "def Floodlight_assurance_for_forwarding_intent(src_host, dst_host, src_ip, dst_ip, ping_count=2):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QoS verification helper functions (Floodlight-compatible)\n",
    "\n",
    "# --- OVS/Mininet Helpers (Controller-Agnostic) ---\n",
    "# (These helpers interact with OVS/Mininet directly)\n",
    "\n",
    "def _sudo(cmd, timeout=30):\n",
    "    return execute_command_full(f\"bash -lc {shlex.quote(cmd)}\", timeout=timeout, with_sudo=True)\n",
    "\n",
    "def _ns(pid, cmd, timeout=90):\n",
    "    return execute_command_full(f\"mnexec -a {pid} bash -lc {shlex.quote(cmd)}\", timeout=timeout, with_sudo=True)\n",
    "\n",
    "def run_in_host(host_pid: int, cmd: str, timeout: int = 30) -> tuple[str, str]:\n",
    "    # Helper function to run a command inside a host namespace\n",
    "    base = f\"mnexec -a {host_pid} {cmd}\"\n",
    "    return execute_command_full(base, timeout=timeout, with_sudo=True)\n",
    "\n",
    "def ensure_no_iperf_server(host_pid: int, port: int) -> None:\n",
    "    run_in_host(host_pid, f\"/usr/bin/pkill -f {shlex.quote(f'iperf3 -s -p {int(port)}')} || true\", timeout=5)\n",
    "    # --- FIX: Corrected escaping for the bash command ---\n",
    "    run_in_host(\n",
    "        host_pid,\n",
    "        f\"bash -lc \\\"/usr/bin/pgrep -af 'iperf3.*-s.*-p {int(port)}' | /usr/bin/awk '{{{{print $1}}}}' | /usr/bin/xargs -r /bin/kill -9\\\"\",\n",
    "        timeout=5\n",
    "    )\n",
    "\n",
    "def start_iperf_server(host_pid: int, port: int, extra_args: str = \"\") -> None:\n",
    "    ensure_no_iperf_server(host_pid, port)\n",
    "    # --- FIX: Corrected escaping for the bash command ---\n",
    "    run_in_host(\n",
    "            host_pid,\n",
    "            f\"bash -lc 'nohup /usr/bin/iperf3 -s -p {int(port)} --one-off {extra_args} \"\n",
    "            f\">/tmp/iperf3_s_{int(port)}.log 2>&1 & echo $!'\",\n",
    "            timeout=5\n",
    "        )\n",
    "    deadline = time.time() + 4.0\n",
    "    while time.time() < deadline:\n",
    "        # --- FIX: Corrected escaping for the bash command ---\n",
    "        out, err = run_in_host(\n",
    "            host_pid,\n",
    "            f\"bash -lc \\\"/usr/bin/ss -ltnp | grep ':{int(port)} ' || true\\\"\",\n",
    "            timeout=3\n",
    "        )\n",
    "        if out.strip():\n",
    "            return\n",
    "    # --- FIX: Changed tail from '-n +200' (from line 200) to '-n 200' (last 200 lines) ---\n",
    "    log, _ = run_in_host(\n",
    "        host_pid,\n",
    "        f\"bash -lc 'tail -n 200 /tmp/iperf3_s_{int(port)}.log 2>/dev/null || true'\",\n",
    "        timeout=3\n",
    "    )\n",
    "    raise RuntimeError(f\"iperf3 server failed to bind/listen on port {port}. Server log:\\n{log}\")\n",
    "\n",
    "    # --- FIX: Corrected escaping for the bash command ---\n",
    "    # log, _ = run_in_host(\n",
    "    #     host_pid,\n",
    "    #     f\"bash -lc 'tail -n +200 /tmp/iperf3_s_{int(port)}.log 2>/dev/null || true'\",\n",
    "    #     timeout=3\n",
    "    # )\n",
    "    # raise RuntimeError(f\"iperf3 server failed to bind/listen on port {port}. Server log:\\n{log}\")\n",
    "\n",
    "def stop_iperf_server(host_pid: int, port: int) -> None:\n",
    "    ensure_no_iperf_server(host_pid, port)\n",
    "\n",
    "def parse_qos_show(iface: str) -> dict:\n",
    "    cmd = f\"ovs-appctl -t ovs-vswitchd qos/show {shlex.quote(iface)}\"\n",
    "    stdout_text, stderr_text = execute_command_full(cmd, with_sudo=True)\n",
    "\n",
    "    if \"No QoS configured\" in stdout_text or \"No QoS configured\" in stderr_text:\n",
    "        return {\"raw\": stdout_text, \"queues\": {}}\n",
    "    if not stdout_text and \"Timeout\" in stderr_text:\n",
    "        print(f\"[WARN] Command timed out: {cmd}\")\n",
    "        return {\"raw\": \"Timeout\", \"queues\": {}}\n",
    "    \n",
    "    raw = stdout_text\n",
    "    queues = {}\n",
    "    qos_meta = {\"raw\": raw}\n",
    "    current_obj = None\n",
    "    \n",
    "    for raw_line in raw.splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        header_match = re.match(r\"^(QoS|Queue|Default):\\s*(.*)\", line)\n",
    "        if header_match:\n",
    "            obj_type = header_match.group(1)\n",
    "            if obj_type == \"QoS\":\n",
    "                current_obj = qos_meta\n",
    "            elif obj_type == \"Queue\":\n",
    "                q_id = line.split()[1].replace(\":\", \"\")\n",
    "                queues[q_id] = {}\n",
    "                current_obj = queues[q_id]\n",
    "            elif obj_type == \"Default\":\n",
    "                queues[\"default\"] = {}\n",
    "                current_obj = queues[\"default\"]\n",
    "            continue\n",
    "        \n",
    "        if current_obj is not None:\n",
    "            kv_match = re.match(r\"^([\\w-]+):\\s*(\\d+)\", line)\n",
    "            if kv_match:\n",
    "                key = kv_match.group(1)\n",
    "                try:\n",
    "                    value = int(kv_match.group(2))\n",
    "                except ValueError:\n",
    "                    value = kv_match.group(2)\n",
    "                current_obj[key] = value\n",
    "\n",
    "    qos_meta[\"queues\"] = queues\n",
    "    return qos_meta\n",
    "\n",
    "def snapshot_queue(iface: str, queue_id: int | str):\n",
    "    parsed = parse_qos_show(iface)\n",
    "    q = parsed[\"queues\"].get(str(queue_id)) or parsed[\"queues\"].get(\"default\") or {}\n",
    "    return {\n",
    "        \"min_rate\": q.get(\"min-rate\"),\n",
    "        \"max_rate\": q.get(\"max-rate\"),\n",
    "        \"tx_bytes\": q.get(\"tx_bytes\"),\n",
    "        \"tx_packets\": q.get(\"tx_packets\"),\n",
    "        \"_raw\": parsed[\"raw\"][:1200]\n",
    "    }\n",
    "\n",
    "def _parse_iperf_sender(line: str) -> tuple[float, int] | tuple[None, None]:\n",
    "    try:\n",
    "        # --- FIX: Removed double-escaped backslashes (e.g., \\\\d -> \\d) ---\n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+MBytes\\s+(\\d+(?:\\.\\d+)?)\\s+Mbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024 * 1024)\n",
    "            mbps = float(match.group(2))\n",
    "            return mbps, bytes_sent\n",
    "        \n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+KBytes\\s+(\\d+(?:\\.\\d+)?)\\s+Mbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024)\n",
    "            mbps = float(match.group(2)) # It's already in Mbits/sec\n",
    "            return mbps, bytes_sent\n",
    "\n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+KBytes\\s+(\\d+(?:\\.\\d+)?)\\s+Kbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024)\n",
    "            mbps = float(match.group(2)) / 1024.0\n",
    "            return mbps, bytes_sent\n",
    "        \n",
    "        match = re.search(r\"(\\d+(?:\\.\\d+)?)\\s+Bytes\\s+(\\d+(?:\\.\\d+)?)\\s+bits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)))\n",
    "            mbps = float(match.group(2)) / 1_000_000.0\n",
    "            return mbps, bytes_sent\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse iperf line '{line}': {e}\")\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def _parse_iperf_sender_old(line: str) -> tuple[float, int] | tuple[None, None]:\n",
    "    try:\n",
    "        match = re.search(r\"(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+MBytes\\\\s+(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+Mbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024 * 1024)\n",
    "            mbps = float(match.group(2))\n",
    "            return mbps, bytes_sent\n",
    "        \n",
    "        # --- THIS IS THE CRITICAL FIX FOR THE PARSER ---\n",
    "        match = re.search(r\"(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+KBytes\\\\s+(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+Mbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024)\n",
    "            mbps = float(match.group(2)) # It's already in Mbits/sec\n",
    "            return mbps, bytes_sent\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "        match = re.search(r\"(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+KBytes\\\\s+(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+Kbits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)) * 1024)\n",
    "            mbps = float(match.group(2)) / 1024.0\n",
    "            return mbps, bytes_sent\n",
    "        match = re.search(r\"(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+Bytes\\\\s+(\\\\d+(?:\\\\.\\\\d+)?)\\\\s+bits/sec\", line)\n",
    "        if match:\n",
    "            bytes_sent = int(float(match.group(1)))\n",
    "            mbps = float(match.group(2)) / 1_000_000.0\n",
    "            return mbps, bytes_sent\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to parse iperf line '{line}': {e}\")\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "def _iperf_text_summary(client_pid, dst_ip, dst_port, duration, extra_args=\"\") -> dict:\n",
    "    iperf_cmd_str = f'/usr/bin/iperf3 -c {dst_ip} -p {dst_port} -t {duration} {extra_args}'\n",
    "    cmd = f\"mnexec -a {client_pid} bash -lc {shlex.quote(iperf_cmd_str)}\"\n",
    "    print(f\"\\n[DEBUG] Running iperf command: {cmd}\\n\") # <-- ADDED DEBUG\n",
    "    stdout_text, stderr_text = execute_command_full(cmd, timeout=max(15, duration + 10), with_sudo=True)\n",
    "    \n",
    "    if \"Connection refused\" in stderr_text:\n",
    "        print(\"[DEBUG] iperf stdout (raw):\", stdout_text) # <-- ADDED DEBUG\n",
    "        print(\"[DEBUG] iperf stderr (raw):\", stderr_text) # <-- ADDED DEBUG\n",
    "        raise RuntimeError(f\"Iperf connection refused. Is server running on {dst_ip}:{dst_port}?\")\n",
    "    if \"No route to host\" in stderr_text:\n",
    "        print(\"[DEBUG] iperf stdout (raw):\", stdout_text) # <-- ADDED DEBUG\n",
    "        print(\"[DEBUG] iperf stderr (raw):\", stderr_text) # <-- ADDED DEBUG\n",
    "        raise RuntimeError(f\"Iperf 'No route to host' for {dst_ip}. Check pinning flows.\")\n",
    "\n",
    "    stdout_lines = stdout_text.splitlines()\n",
    "    target_line = None\n",
    "    for ln in reversed(stdout_lines):\n",
    "        if (\"bits/sec\" in ln and \n",
    "           (f\" 0.0-{duration:.1f}\" in ln or f\" 0.00-{duration:.02f}\" in ln or \" 0.0-\" in ln)):\n",
    "            if ln.strip().startswith(\"[SUM]\"):\n",
    "                target_line = ln\n",
    "                break\n",
    "            # This regex is safer for lines like \"[ 5]\"\n",
    "            if re.match(r'^\\\\s*\\\\[\\\\s*\\\\d+\\\\]', ln.strip()):\n",
    "                target_line = ln\n",
    "            elif ln.strip().startswith(\"[\") and \"sender\" in ln: # Fallback for older iperf\n",
    "                target_line = ln\n",
    "    \n",
    "    if not target_line:\n",
    "        # --- THIS IS THE MOST IMPORTANT PART ---\n",
    "        # If parsing fails, print everything so we can see the error\n",
    "        print(\"\\n[DEBUG] --- IPERF SUMMARY PARSING FAILED ---\")\n",
    "        print(f\"[DEBUG] iperf stdout (raw):\\n{stdout_text}\")\n",
    "        print(f\"[DEBUG] iperf stderr (raw):\\n{stderr_text}\\n\")\n",
    "        # We will return None instead of raising an error\n",
    "        # to allow the test to fail gracefully.\n",
    "        return {\"sender_mbps\": None, \"bytes_sent\": None, \"sender_line\": None}\n",
    "\n",
    "    mbps, bytes_sent = _parse_iperf_sender(target_line)\n",
    "    return {\"sender_mbps\": mbps, \"bytes_sent\": bytes_sent, \"sender_line\": target_line}\n",
    "\n",
    "\n",
    "def _iperf_text_summary_old(client_pid, dst_ip, dst_port, duration, extra_args=\"\") -> dict:\n",
    "    iperf_cmd_str = f'/usr/bin/iperf3 -c {dst_ip} -p {dst_port} -t {duration} {extra_args}'\n",
    "    cmd = f\"mnexec -a {client_pid} bash -lc {shlex.quote(iperf_cmd_str)}\"\n",
    "    stdout_text, stderr_text = execute_command_full(cmd, timeout=max(15, duration + 10), with_sudo=True)\n",
    "    \n",
    "    if \"Connection refused\" in stderr_text:\n",
    "        raise RuntimeError(f\"Iperf connection refused. Is server running on {dst_ip}:{dst_port}?\")\n",
    "    if \"No route to host\" in stderr_text:\n",
    "        raise RuntimeError(f\"Iperf 'No route to host' for {dst_ip}. Check pinning flows.\")\n",
    "\n",
    "    stdout_lines = stdout_text.splitlines()\n",
    "    target_line = None\n",
    "    for ln in reversed(stdout_lines):\n",
    "        if (\"bits/sec\" in ln and \n",
    "           (f\" 0.0-{duration:.1f}\" in ln or f\" 0.00-{duration:.02f}\" in ln or \" 0.0-\" in ln)):\n",
    "            if ln.strip().startswith(\"[SUM]\"):\n",
    "                target_line = ln\n",
    "                break\n",
    "            # This regex is safer for lines like \"[ 5]\"\n",
    "            if re.match(r'^\\s*\\[\\s*\\d+\\]', ln.strip()):\n",
    "                target_line = ln\n",
    "            elif ln.strip().startswith(\"[\") and \"sender\" in ln: # Fallback for older iperf\n",
    "                target_line = ln\n",
    "    \n",
    "    if not target_line:\n",
    "        print(\"[DEBUG] iperf stdout:\", stdout_text)\n",
    "        print(\"[DEBUG] iperf stderr:\", stderr_text)\n",
    "        raise RuntimeError(\"Could not find iperf sender summary line. iperf output was not as expected.\")\n",
    "\n",
    "    mbps, bytes_sent = _parse_iperf_sender(target_line)\n",
    "    return {\"sender_mbps\": mbps, \"bytes_sent\": bytes_sent, \"sender_line\": target_line}\n",
    "\n",
    "def ensure_qos_cap(device_id_int: int, port_no: int, qid: int, min_bps: int, max_bps: int, port_cap_bps: int = 100_000_000):\n",
    "    iface = get_iface_for_port(device_id_int, port_no)\n",
    "    print(f\"[INFO] Setting QoS on {iface}: q{qid} min={min_bps} max={max_bps}, root max-rate={port_cap_bps}bps\")\n",
    "    _sudo(f\"ovs-vsctl --if-exists clear Port {shlex.quote(iface)} qos\")\n",
    "    cmd = (\n",
    "        \"ovs-vsctl \"\n",
    "        f\"-- --id=@q create Queue other-config:min-rate={min_bps} other-config:max-rate={max_bps} \"\n",
    "        f\"-- --id=@qos create QoS type=linux-htb other-config:max-rate={port_cap_bps} queues:{qid}=@q \"\n",
    "        f\"-- set Port {shlex.quote(iface)} qos=@qos\"\n",
    "    )\n",
    "    print(_sudo(cmd))\n",
    "    print(_sudo(f\"ovs-appctl -t ovs-vswitchd qos/show {shlex.quote(iface)}\"))\n",
    "\n",
    "def _floodlight_flow_counters(flow_json):\n",
    "    \"\"\"\n",
    "    Extracts packet and byte counters from a Floodlight *operational* flow rule.\n",
    "    \"\"\"\n",
    "    if flow_json:\n",
    "        # FIX: Changed keys to match Floodlight's operational flow output\n",
    "        return int(flow_json.get(\"packet_count\", 0)), int(flow_json.get(\"byte_count\", 0))\n",
    "    return None, None\n",
    "\n",
    "def _floodlight_flow_counters_old(flow_json):\n",
    "    \"\"\"\n",
    "    Extracts packet and byte counters from a Floodlight *operational* flow rule.\n",
    "    \"\"\"\n",
    "    if flow_json:\n",
    "        return int(flow_json.get(\"packetCount\", 0)), int(flow_json.get(\"byteCount\", 0))\n",
    "    return None, None\n",
    "\n",
    "# --- Main QoS Verifier ---\n",
    "\n",
    "def verify_qos_flow_with_iperf(\n",
    "    flow_device_dpid_str: str, flow_name: str, # <-- Changed\n",
    "    queue_device_dpid_int: int, queue_port_no: int, queue_id: int,\n",
    "    src_ip: str, dst_ip: str, dst_port: int,\n",
    "    target_mbps: float,\n",
    "    main_static_flow_rule: dict, # <-- Added this\n",
    "    duration_sec: int = 8, parallel: int = 8, tcp_mss: int = 1200,\n",
    "    tolerance_pct: float = 10.0,\n",
    "    pin_path_flows: list | None = None,\n",
    "    protocol: str = \"tcp\",\n",
    "    udp_bw_mbps: float = 50.0,\n",
    "    udp_len_bytes: int = 1200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Verifies a QoS flow using iperf.\n",
    "    This version is adapted for Floodlight (uses dpid_str and flow_name).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Resolve actors (OVS/Mininet layer)\n",
    "    iface = get_iface_for_port(queue_device_dpid_int, queue_port_no)\n",
    "    client_pid = get_mininet_host_pid(ip_to_host[src_ip])\n",
    "    server_pid = get_mininet_host_pid(ip_to_host[dst_ip])\n",
    "    proto = protocol.lower()\n",
    "    \n",
    "    # if proto == \"tcp\":\n",
    "    #     extra_args = f\"-P {parallel} -M {tcp_mss}\"\n",
    "    # else:\n",
    "    #     #extra_args = f\"-u -b {udp_bw_mbps}M -l {udp_len_bytes} -P {max(1, parallel)}\"\n",
    "    #     offer_mbps = max(target_mbps * 1.25, 6.0)  # >= 25% headroom, min 6 Mbps\n",
    "    #     extra_args = f\"-u -b {offer_mbps}M -l {udp_len_bytes} -P 1\"\n",
    "\n",
    "    if proto == \"tcp\":\n",
    "        extra_args = f\"-P {parallel} -M {tcp_mss}\"\n",
    "    else:\n",
    "        offer_mbps = max(target_mbps * 1.25, 6.0)  # >= 25% headroom, min 6 Mbps\n",
    "        \n",
    "        # --- FIX: Convert bandwidth to an integer in bits per second ---\n",
    "        # This avoids iperf3 misinterpreting \"6.0M\"\n",
    "        offer_bps = int(offer_mbps * 1_000_000)\n",
    "        extra_args = f\"-u -b {offer_bps} -l {udp_len_bytes} -P 1\"\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "    # 1. Install Pinning Flows\n",
    "    pushed_pin_flows = [] # This will store the actual flow dicts\n",
    "    if pin_path_flows:\n",
    "        pushed_pin_flows = install_pins_from_plan_floodlight(\n",
    "            pins=pin_path_flows,\n",
    "            src_ip=src_ip, dst_ip=dst_ip,\n",
    "            dst_port=dst_port, protocol=protocol\n",
    "        )\n",
    "        print(f\"Installed {len(pushed_pin_flows)} pinning flows.\")\n",
    "\n",
    "        # --- FIX: Install TCP Helper Flow for iperf3 Control Channel ---\n",
    "        # This flow matches the TCP control packets and forwards them\n",
    "        # on the same path as the QoS rule, but to the default queue.\n",
    "        tcp_helper_flow = {}\n",
    "        try:\n",
    "            # We build the match based on the main QoS flow\n",
    "            # We need all match fields EXCEPT the L4 proto ones (ip_proto, udp_dst)\n",
    "            control_keys = {\"switch\", \"name\", \"active\", \"priority\", \"actions\", \"ip_proto\", \"udp_dst\", \"tcp_dst\"}\n",
    "            \n",
    "            # Copy all L2/L3 match fields (like in_port, eth_type, etc.)\n",
    "            for k, v in main_static_flow_rule.items():\n",
    "                if k not in control_keys:\n",
    "                    tcp_helper_flow[k] = v\n",
    "            \n",
    "            # Add the TCP-specific L4 match fields\n",
    "            tcp_helper_flow[\"ip_proto\"] = \"6\" # TCP\n",
    "            tcp_helper_flow[\"tcp_dst\"] = str(dst_port)\n",
    "            \n",
    "            # Add the control fields\n",
    "            tcp_helper_flow[\"switch\"] = flow_device_dpid_str\n",
    "            tcp_helper_flow[\"priority\"] = main_static_flow_rule.get(\"priority\", \"200\") # Same priority\n",
    "            tcp_helper_flow[\"active\"] = \"true\"\n",
    "            tcp_helper_flow[\"name\"] = f\"pin-tcp-helper-{secrets.token_hex(4)}\"\n",
    "            \n",
    "            # Find the output action, but remove the set_queue action\n",
    "            original_actions = main_static_flow_rule.get(\"actions\", \"\").split(',')\n",
    "            new_actions = [a for a in original_actions if \"output=\" in a]\n",
    "            tcp_helper_flow[\"actions\"] = \",\".join(new_actions)\n",
    "\n",
    "            if \"output=\" in tcp_helper_flow[\"actions\"]:\n",
    "                print(f\"[DEBUG] Installing TCP helper flow: {tcp_helper_flow['name']}\")\n",
    "                add_flow_floodlight(tcp_helper_flow)\n",
    "                pushed_pin_flows.append(tcp_helper_flow) # Add to cleanup list\n",
    "            else:\n",
    "                print(\"[WARN] Could not create TCP helper flow, 'actions' missing output.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Failed to install TCP helper flow: {e}\")\n",
    "        # --- END OF FIX ---\n",
    "\n",
    "    # --- FIX: Main try block STARTS here ---\n",
    "    try:\n",
    "        try:\n",
    "            _sudo(f\"ethtool -K {shlex.quote(iface)} gro off gso off tso off\", timeout=6)\n",
    "        except Exception: \n",
    "            pass # This command is non-critical\n",
    "\n",
    "        # 2. Snapshots before\n",
    "        flow_before = get_operational_flow_by_match(flow_device_dpid_str, main_static_flow_rule)\n",
    "        f_pkts0, f_bytes0 = _floodlight_flow_counters(flow_before)\n",
    "        q0 = snapshot_queue(iface, queue_id)\n",
    "\n",
    "        # 3. Run iperf\n",
    "        #start_iperf_server(server_pid, dst_port)\n",
    "        #server_extra_args = \"-u\" if proto == \"udp\" else \"\"\n",
    "        #start_iperf_server(server_pid, dst_port, extra_args=server_extra_args)\n",
    "        start_iperf_server(server_pid, dst_port)\n",
    "\n",
    "        time.sleep(0.6)\n",
    "        t0 = time.time()\n",
    "        iptxt = _iperf_text_summary(client_pid, dst_ip, dst_port, duration_sec, extra_args=extra_args)\n",
    "        t1 = time.time()\n",
    "        stop_iperf_server(server_pid, dst_port)\n",
    "        time.sleep(0.8)\n",
    "\n",
    "        # 4. Snapshots after\n",
    "        flow_after = get_operational_flow_by_match(flow_device_dpid_str, main_static_flow_rule)\n",
    "        f_pkts1, f_bytes1 = _floodlight_flow_counters(flow_after)\n",
    "        q1 = snapshot_queue(iface, queue_id)\n",
    "\n",
    "        # 5. Deltas\n",
    "        elapsed = max(0.001, t1 - t0)\n",
    "        q_bytes = None if (q0.get(\"tx_bytes\") is None or q1.get(\"tx_bytes\") is None) else (q1[\"tx_bytes\"] - q0[\"tx_bytes\"])\n",
    "        q_mbps  = (q_bytes * 8 / elapsed / 1e6) if q_bytes is not None else None\n",
    "        f_pkts_delta = (f_pkts1 - f_pkts0) if (f_pkts0 is not None and f_pkts1 is not None) else None\n",
    "        f_bytes_delta = (f_bytes1 - f_bytes0) if (f_bytes0 is not None and f_bytes1 is not None) else None\n",
    "        f_mbps = (f_bytes_delta * 8 / elapsed / 1e6) if (f_bytes_delta is not None) else None\n",
    "\n",
    "        # 6. Decision\n",
    "        rate_ok   = (q_mbps is not None) and (abs(q_mbps - target_mbps) <= (tolerance_pct/100.0)*target_mbps)\n",
    "        packets_ok= (f_pkts_delta or 0) > 0\n",
    "        verdict   = \"PASS\" if (rate_ok and packets_ok) else \"FAIL\"\n",
    "\n",
    "        # 7. Print summary\n",
    "        print(\"\\n=== QoS FLOW VERIFICATION ===\")\n",
    "        print(f\"Flow (name:{flow_name}) @ DPID {flow_device_dpid_str}  → selector should match {protocol} dst {dst_port} to {dst_ip}\")\n",
    "        print(f\"Egress iface={iface} queue={queue_id}  target≈{target_mbps:.3f} Mbps  tol=±{tolerance_pct:.0f}%\")\n",
    "        iperf_mbps_str = f\"{iptxt['sender_mbps']:.3f}\" if iptxt['sender_mbps'] is not None else \"N/A\"\n",
    "        iperf_bytes_str = f\"{iptxt['bytes_sent']}\" if iptxt['bytes_sent'] is not None else \"N/A\"\n",
    "        print(f\"iperf sender: {iperf_mbps_str} Mbps  bytes≈{iperf_bytes_str}\")\n",
    "        print(f\"Queue Δbytes={q_bytes} over {elapsed:.3f}s  → queue_measured≈{(q_mbps or 0):.3f} Mbps\")\n",
    "        print(f\"Flow Δ: packets={f_pkts_delta} bytes={f_bytes_delta}  (flow_measured≈{(f_mbps or 0):.3f} Mbps)\")\n",
    "        print(f\"Queue caps (min/max): {q0.get('min_rate')} / {q0.get('max_rate')}  →  {q1.get('min_rate')} / {q1.get('max_rate')}\")\n",
    "        print(\"VERDICT:\", verdict)\n",
    "        \n",
    "        if(f_pkts_delta is None):\n",
    "            print(\"[WARN] Could not find operational flow to read stats. Verification may be unreliable.\")\n",
    "\n",
    "        # --- FIX: Return statement is now *inside* the try block ---\n",
    "        return {\n",
    "            \"elapsed_sec\": elapsed,\n",
    "            \"iperf_sender_mbps\": iptxt[\"sender_mbps\"],\n",
    "            \"queue_measured_mbps\": q_mbps,\n",
    "            \"queue_delta_bytes\": q_bytes,\n",
    "            \"flow_delta_packets\": f_pkts_delta,\n",
    "            \"flow_delta_bytes\": f_bytes_delta,\n",
    "            \"flow_measured_mbps\": f_mbps,\n",
    "            \"verdict\": verdict\n",
    "        }\n",
    "    \n",
    "    # --- FIX: 'finally' block is now correctly associated with the main 'try' block ---\n",
    "    # 8. Cleanup Pinning Flows (runs whether the 'try' block succeeded or failed)\n",
    "    finally:\n",
    "        print(f\"Cleaning up {len(pushed_pin_flows)} pinning flows...\")\n",
    "        unpin_path_floodlight(pushed_pin_flows)\n",
    "\n",
    "# --- Floodlight-compatible Pinning Functions ---\n",
    "\n",
    "def _pin_selector_floodlight(direction: str, protocol: str, src_ip: str, dst_ip: str, dst_port: int):\n",
    "    \"\"\"Builds a flat match dictionary for Floodlight.\"\"\"\n",
    "    # is_udp = protocol.lower() == \"udp\" # No longer needed\n",
    "    \n",
    "    # --- MODIFIED ---\n",
    "    # We remove L4 (TCP/UDP) matching from the pinning flows.\n",
    "    # This allows the TCP control channel AND the UDP data to use the same path.\n",
    "    match = {\n",
    "        \"eth_type\": \"0x0800\",\n",
    "        # \"ip_proto\": \"17\" if is_udp else \"6\"  # <-- REMOVED\n",
    "    }\n",
    "    \n",
    "    if direction == \"forward\":\n",
    "        match[\"ipv4_src\"] = f\"{src_ip}/32\"\n",
    "        match[\"ipv4_dst\"] = f\"{dst_ip}/32\"\n",
    "        # if is_udp:                          # <-- REMOVED\n",
    "        #     match[\"udp_dst\"] = str(dst_port)\n",
    "        # else:\n",
    "        #     match[\"tcp_dst\"] = str(dst_port)\n",
    "    else: # reverse\n",
    "        match[\"ipv4_src\"] = f\"{dst_ip}/32\"\n",
    "        match[\"ipv4_dst\"] = f\"{src_ip}/32\"\n",
    "        # if is_udp:                          # <-- REMOVED\n",
    "        #     match[\"udp_src\"] = str(dst_port)\n",
    "        # else:\n",
    "        #     match[\"tcp_src\"] = str(dst_port)\n",
    "    return match\n",
    "\n",
    "def _pin_selector_floodlight_old(direction: str, protocol: str, src_ip: str, dst_ip: str, dst_port: int):\n",
    "    \"\"\"Builds a flat match dictionary for Floodlight.\"\"\"\n",
    "    is_udp = protocol.lower() == \"udp\"\n",
    "    match = {\n",
    "        \"eth_type\": \"0x0800\",\n",
    "        \"ip_proto\": \"17\" if is_udp else \"6\"\n",
    "    }\n",
    "    \n",
    "    if direction == \"forward\":\n",
    "        match[\"ipv4_src\"] = f\"{src_ip}/32\"\n",
    "        match[\"ipv4_dst\"] = f\"{dst_ip}/32\"\n",
    "        if is_udp:\n",
    "            match[\"udp_dst\"] = str(dst_port)\n",
    "        else:\n",
    "            match[\"tcp_dst\"] = str(dst_port)\n",
    "    else: # reverse\n",
    "        match[\"ipv4_src\"] = f\"{dst_ip}/32\"\n",
    "        match[\"ipv4_dst\"] = f\"{src_ip}/32\"\n",
    "        if is_udp:\n",
    "            match[\"udp_src\"] = str(dst_port)\n",
    "        else:\n",
    "            match[\"tcp_src\"] = str(dst_port)\n",
    "    return match\n",
    "\n",
    "def install_pins_from_plan_floodlight(pins: list, src_ip: str, dst_ip: str, dst_port: int, protocol: str) -> list:\n",
    "    \"\"\"\n",
    "    Install the pin flows (Floodlight format) and return the list of installed flows.\n",
    "    \"\"\"\n",
    "    pushed_flows = []\n",
    "    \n",
    "    for i, p in enumerate(pins):\n",
    "        # dev_dpid is already the colon-hex string\n",
    "        dev_dpid = p[\"deviceId\"] \n",
    "        outp = str(p[\"out_port\"])\n",
    "        direction = p[\"direction\"]\n",
    "        \n",
    "        flow_name = f\"pin-{direction}-{i}-{secrets.token_hex(4)}\"\n",
    "        \n",
    "        flow_body = {\n",
    "            \"switch\": dev_dpid,\n",
    "            \"name\": flow_name,\n",
    "            \"priority\": \"65000\",\n",
    "            \"active\": \"true\",\n",
    "            \"actions\": f\"output={outp}\"\n",
    "        }\n",
    "        \n",
    "        # Add match criteria\n",
    "        flow_body.update(_pin_selector_floodlight(direction, protocol, src_ip, dst_ip, dst_port))\n",
    "        \n",
    "        try:\n",
    "            add_flow_floodlight(flow_body)\n",
    "            pushed_flows.append(flow_body) # Store the whole flow for deletion\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to install pin flow: {e}\")\n",
    "            \n",
    "    return pushed_flows\n",
    "\n",
    "def unpin_path_floodlight(pushed_flows: list):\n",
    "    \"\"\"\n",
    "    Remove all pinned rules by their 'name'.\n",
    "    \"\"\"\n",
    "    for flow in pushed_flows:\n",
    "        delete_flow_floodlight(flow[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- New helpers to satisfy the “missing src/dst” rules ---\n",
    "\n",
    "def choose_dst_for_port(device_id: str, port_no: int) -> str:\n",
    "    \"\"\"\n",
    "    Pick a destination IP that makes forward traffic EXIT on (device_id, port_no).\n",
    "    Uses your diamond wiring.\n",
    "    \"\"\"\n",
    "    # Final-hop host ports first\n",
    "    if device_id == SW_OF[\"1\"] and port_no == 3:  # s1 -> h1\n",
    "        return HOSTS[\"h1\"]\n",
    "    if device_id == SW_OF[\"1\"] and port_no == 4:  # s1 -> h2\n",
    "        return HOSTS[\"h2\"]\n",
    "    if device_id == SW_OF[\"4\"] and port_no == 3:  # s4 -> h3\n",
    "        return HOSTS[\"h3\"]\n",
    "    if device_id == SW_OF[\"4\"] and port_no == 4:  # s4 -> h4\n",
    "        return HOSTS[\"h4\"]\n",
    "\n",
    "    # Inter-switch egress: choose a host \"behind\" the far side so packets must traverse this link\n",
    "    # s1:1->s2 (right), s1:2->s3 (right) → pick a right-side host (h3 default)\n",
    "    if device_id == SW_OF[\"1\"] and port_no in (1, 2):\n",
    "        return HOSTS[\"h3\"]\n",
    "    # s2:2->s4 (right) → pick right-side host; s2:1->s1 (left) → pick left-side host\n",
    "    if device_id == SW_OF[\"2\"] and port_no == 2:\n",
    "        return HOSTS[\"h3\"]\n",
    "    if device_id == SW_OF[\"2\"] and port_no == 1:\n",
    "        return HOSTS[\"h1\"]\n",
    "    # s3:2->s4 (right) → right host; s3:1->s1 (left) → left host\n",
    "    if device_id == SW_OF[\"3\"] and port_no == 2:\n",
    "        return HOSTS[\"h3\"]\n",
    "    if device_id == SW_OF[\"3\"] and port_no == 1:\n",
    "        return HOSTS[\"h1\"]\n",
    "    # s4 inter-switch (rare in your tests): port1->s2 (left) pick left host; port2->s3 (left) pick left host\n",
    "    if device_id == SW_OF[\"4\"] and port_no in (1, 2):\n",
    "        return HOSTS[\"h1\"]\n",
    "\n",
    "    # Fallback\n",
    "    return HOSTS[\"h3\"]\n",
    "\n",
    "def fill_missing_endpoints(plan: dict, device_id: str, port_no: int) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Apply your policy:\n",
    "      - If only dst is present → choose src that forces egress at (device,port)\n",
    "      - If only src is present → choose dst that forces egress at (device,port)\n",
    "      - If both missing → pick both to force egress at (device,port)\n",
    "      - Ensure src != dst\n",
    "    \"\"\"\n",
    "    src = plan.get(\"src_ip\")\n",
    "    dst = plan.get(\"dst_ip\")\n",
    "\n",
    "    # Normalize empty strings to None\n",
    "    src = src or None\n",
    "    dst = dst or None\n",
    "\n",
    "    if dst is None:\n",
    "        dst = choose_dst_for_port(device_id, port_no)\n",
    "    if src is None:\n",
    "        src = choose_src_for_port(device_id, port_no)\n",
    "\n",
    "    # If they accidentally collide, flip src to the opposite side\n",
    "    if src == dst:\n",
    "        # If dst is on right side, move src to left; else move to right\n",
    "        try:\n",
    "            dst_edge, _ = _edge_for_ip(dst)\n",
    "        except Exception:\n",
    "            dst_edge = SW_OF[\"4\"]  # assume right if unknown\n",
    "        src = HOSTS[\"h1\"] if dst_edge in (SW_OF[\"3\"], SW_OF[\"4\"]) else HOSTS[\"h3\"]\n",
    "\n",
    "    return src, dst\n",
    "\n",
    "\n",
    "\n",
    "# --- QoS Topology-Aware Helpers ---\n",
    "# (These helpers use the maps defined in Cell 3)\n",
    "\n",
    "def _strip32(ip: str) -> str:\n",
    "    return ip.split(\"/\", 1)[0] if ip else None\n",
    "\n",
    "def find_edge_for_ip(dst_ip: str):\n",
    "    \"\"\"Return (deviceId_str, portNo) for the host that owns dst_ip.\"\"\"\n",
    "    ip = _strip32(dst_ip)\n",
    "    if ip in HOST_ATTACH:\n",
    "        return HOST_ATTACH[ip]\n",
    "    raise RuntimeError(f\"Could not infer edge device/port for dst_ip={dst_ip}\")\n",
    "\n",
    "def infer_enforcement_point(plan):\n",
    "    \"\"\"\n",
    "    Decide where to enforce if device/port not fully specified.\n",
    "    \"\"\"\n",
    "    device_id, port_no = plan.get(\"device_id_str\"), plan.get(\"port_no\")\n",
    "    if device_id and port_no:\n",
    "        return device_id, port_no\n",
    "    \n",
    "    # Fallback: enforce at the final hop to the destination host\n",
    "    return find_edge_for_ip(plan[\"dst_ip\"])\n",
    "\n",
    "def _is(dev: str, n: str) -> bool:\n",
    "    \"\"\"Checks if dev_dpid_str matches SW_OF[n]\"\"\"\n",
    "    return dev == SW_OF.get(n)\n",
    "\n",
    "def choose_dst_for_port(device_id_str: str, port_no: int) -> str:\n",
    "    \"\"\"Pick a destination IP that makes forward traffic EXIT on (device_id_str, port_no).\"\"\"\n",
    "    if _is(device_id_str, \"1\") and port_no == 3: return HOSTS[\"h1\"]\n",
    "    if _is(device_id_str, \"1\") and port_no == 4: return HOSTS[\"h2\"]\n",
    "    if _is(device_id_str, \"4\") and port_no == 3: return HOSTS[\"h3\"]\n",
    "    if _is(device_id_str, \"4\") and port_no == 4: return HOSTS[\"h4\"]\n",
    "\n",
    "    if _is(device_id_str, \"1\") and port_no in (1, 2): return HOSTS[\"h3\"]\n",
    "    if _is(device_id_str, \"2\") and port_no == 2: return HOSTS[\"h3\"]\n",
    "    if _is(device_id_str, \"2\") and port_no == 1: return HOSTS[\"h1\"]\n",
    "    if _is(device_id_str, \"3\") and port_no == 2: return HOSTS[\"h3\"]\n",
    "    if _is(device_id_str, \"3\") and port_no == 1: return HOSTS[\"h1\"]\n",
    "    if _is(device_id_str, \"4\") and port_no in (1, 2): return HOSTS[\"h1\"]\n",
    "    return HOSTS[\"h3\"] # Fallback\n",
    "\n",
    "def choose_src_for_port(device_id_str: str, port_no: int) -> str:\n",
    "    \"\"\"Pick a source IP that will make *forward* traffic egress on (device_id_str, port_no).\"\"\"\n",
    "    if _is(device_id_str, \"1\"):\n",
    "        if port_no in (3, 4): return HOSTS[\"h3\"]\n",
    "        elif port_no in (1, 2): return HOSTS[\"h1\"]\n",
    "    if _is(device_id_str, \"2\"):\n",
    "        return HOSTS[\"h1\"] if port_no == 2 else HOSTS[\"h3\"]\n",
    "    if _is(device_id_str, \"3\"):\n",
    "        return HOSTS[\"h1\"] if port_no == 2 else HOSTS[\"h3\"]\n",
    "    if _is(device_id_str, \"4\"):\n",
    "        if port_no in (3, 4): return HOSTS[\"h1\"]\n",
    "        elif port_no in (1, 2): return HOSTS[\"h3\"]\n",
    "    return HOSTS[\"h1\"] # Fallback\n",
    "\n",
    "def fill_missing_endpoints(plan: dict, device_id_str: str, port_no: int) -> tuple[str, str]:\n",
    "    \"\"\"Fill missing src/dst IPs based on topology.\"\"\"\n",
    "    src = plan.get(\"src_ip\") or None\n",
    "    dst = plan.get(\"dst_ip\") or None\n",
    "\n",
    "    if dst is None:\n",
    "        dst = choose_dst_for_port(device_id_str, port_no)\n",
    "    if src is None:\n",
    "        src = choose_src_for_port(device_id_str, port_no)\n",
    "\n",
    "    if src == dst:\n",
    "        # Flip src to the opposite side\n",
    "        dst_edge, _ = find_edge_for_ip(dst)\n",
    "        src = HOSTS[\"h1\"] if dst_edge in (SW_OF[\"3\"], SW_OF[\"4\"]) else HOSTS[\"h3\"]\n",
    "    return src, dst\n",
    "\n",
    "def make_pin_path_flows(device_id_str: str, port_no: int,\n",
    "                        src_ip: str, dst_ip: str, dst_port: int,\n",
    "                        protocol: str = \"tcp\"):\n",
    "    \"\"\"\n",
    "    Build a pinning plan (list of dicts) so src_ip -> dst_ip forward traffic\n",
    "    *must* traverse (device_id_str, port_no).\n",
    "    \n",
    "    This version correctly installs the reverse path for BOTH TCP and UDP\n",
    "    to allow iperf server to reply to the client.\n",
    "    \"\"\"\n",
    "    proto = protocol.lower()\n",
    "    is_udp = (proto == \"udp\")\n",
    "    pins = []\n",
    "    src_edge_dev, src_edge_port = find_edge_for_ip(src_ip)\n",
    "    dst_edge_dev, dst_edge_port = find_edge_for_ip(dst_ip)\n",
    "\n",
    "    def add(dev_key, outp, direction):\n",
    "        pins.append({\"deviceId\": SW_OF[dev_key], \"out_port\": int(outp), \"direction\": direction})\n",
    "\n",
    "    # --- cases by enforcement point ---\n",
    "    \n",
    "    # s4 -> h3/h4\n",
    "    if _is(device_id_str, \"4\") and port_no in (3, 4): \n",
    "        if _is(src_edge_dev, \"1\"):\n",
    "            add(\"1\", 1, \"forward\") # s1 -> s2\n",
    "            add(\"2\", 2, \"forward\") # s2 -> s4\n",
    "            # Reverse path\n",
    "            add(\"4\", 1, \"reverse\") # s4 -> s2\n",
    "            add(\"2\", 1, \"reverse\") # s2 -> s1\n",
    "            add(\"1\", src_edge_port, \"reverse\") # s1 -> h1/h2\n",
    "    \n",
    "    # s1 -> h1/h2        \n",
    "    elif _is(device_id_str, \"1\") and port_no in (3, 4): \n",
    "        if _is(src_edge_dev, \"4\"):\n",
    "            add(\"4\", 2, \"forward\") # s4 -> s3\n",
    "            add(\"3\", 1, \"forward\") # s3 -> s1\n",
    "            # Reverse path\n",
    "            add(\"1\", 2, \"reverse\") # s1 -> s3\n",
    "            add(\"3\", 2, \"reverse\") # s3 -> s4\n",
    "            add(\"4\", src_edge_port, \"reverse\") # s4 -> h3/h4\n",
    "\n",
    "    # s1 -> s3\n",
    "    elif _is(device_id_str, \"1\") and port_no == 2: \n",
    "        add(\"1\", 2, \"forward\")\n",
    "        add(\"3\", 2, \"forward\")\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        # Reverse path\n",
    "        add(\"4\", 2, \"reverse\")\n",
    "        add(\"3\", 1, \"reverse\")\n",
    "        add(\"1\", src_edge_port, \"reverse\")\n",
    "    \n",
    "    # s1 -> s2        \n",
    "    elif _is(device_id_str, \"1\") and port_no == 1: \n",
    "        add(\"1\", 1, \"forward\")\n",
    "        add(\"2\", 2, \"forward\")\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        # Reverse path\n",
    "        add(\"4\", 1, \"reverse\")\n",
    "        add(\"2\", 1, \"reverse\")\n",
    "        add(\"1\", src_edge_port, \"reverse\")\n",
    "    \n",
    "    # s2 -> s4\n",
    "    elif _is(device_id_str, \"2\") and port_no == 2: \n",
    "        add(\"1\", 1, \"forward\")   # s1 -> s2\n",
    "        add(\"2\", 2, \"forward\")   # s2 -> s4\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        # Reverse path\n",
    "        add(\"4\", 1, \"reverse\")\n",
    "        add(\"2\", 1, \"reverse\")\n",
    "        add(\"1\", src_edge_port, \"reverse\")\n",
    "            \n",
    "    # s3 -> s4\n",
    "    elif _is(device_id_str, \"3\") and port_no == 2: \n",
    "        add(\"1\", 2, \"forward\")   # s1 -> s3\n",
    "        add(\"3\", 2, \"forward\")   # s3 -> s4\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        # Reverse path\n",
    "        add(\"4\", 2, \"reverse\")\n",
    "        add(\"3\", 1, \"reverse\")\n",
    "        add(\"1\", src_edge_port, \"reverse\")\n",
    "            \n",
    "    else: # Fallback (e.g., h1->s1->s2->s4->h3)\n",
    "        add(\"1\", 1, \"forward\") # s1->s2\n",
    "        add(\"2\", 2, \"forward\") # s2->s4\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3\n",
    "        # Reverse path\n",
    "        add(\"4\", 1, \"reverse\")\n",
    "        add(\"2\", 1, \"reverse\")\n",
    "        add(\"1\", src_edge_port, \"reverse\")\n",
    "\n",
    "    # --- Filter pins ---\n",
    "    final_pins = []\n",
    "    seen = set()\n",
    "    for p in pins:\n",
    "        # Don't overshadow the QoS rule on the enforcement port\n",
    "        if (p[\"direction\"] == \"forward\" and \n",
    "            p[\"deviceId\"] == device_id_str and \n",
    "            int(p[\"out_port\"]) == int(port_no)):\n",
    "            continue\n",
    "            \n",
    "        key = (p[\"deviceId\"], int(p[\"out_port\"]), p[\"direction\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            final_pins.append(p)\n",
    "            \n",
    "    return final_pins\n",
    "\n",
    "def make_pin_path_flows_old2(device_id_str: str, port_no: int,\n",
    "                        src_ip: str, dst_ip: str, dst_port: int,\n",
    "                        protocol: str = \"tcp\"):\n",
    "    \"\"\"\n",
    "    Build a pinning plan (list of dicts) so src_ip -> dst_ip forward traffic\n",
    "    *must* traverse (device_id_str, port_no).\n",
    "    \"\"\"\n",
    "    proto = protocol.lower()\n",
    "    is_udp = (proto == \"udp\")\n",
    "    pins = []\n",
    "    src_edge_dev, src_edge_port = find_edge_for_ip(src_ip)\n",
    "    dst_edge_dev, dst_edge_port = find_edge_for_ip(dst_ip)\n",
    "\n",
    "    def add(dev_key, outp, direction):\n",
    "        pins.append({\"deviceId\": SW_OF[dev_key], \"out_port\": int(outp), \"direction\": direction})\n",
    "\n",
    "    # --- cases by enforcement point ---\n",
    "    if _is(device_id_str, \"4\") and port_no in (3, 4): # s4 -> h3/h4\n",
    "        if _is(src_edge_dev, \"1\"):\n",
    "            add(\"1\", 1, \"forward\") # s1 -> s2\n",
    "            add(\"2\", 2, \"forward\") # s2 -> s4\n",
    "        if not is_udp and _is(src_edge_dev, \"1\"):\n",
    "            add(\"4\", 1, \"reverse\") # s4 -> s2\n",
    "            add(\"2\", 1, \"reverse\") # s2 -> s1\n",
    "            add(\"1\", src_edge_port, \"reverse\") # s1 -> h1/h2\n",
    "            \n",
    "    elif _is(device_id_str, \"1\") and port_no in (3, 4): # s1 -> h1/h2\n",
    "        if _is(src_edge_dev, \"4\"):\n",
    "            add(\"4\", 2, \"forward\") # s4 -> s3\n",
    "            add(\"3\", 1, \"forward\") # s3 -> s1\n",
    "        if not is_udp and _is(src_edge_dev, \"4\"):\n",
    "            add(\"1\", 2, \"reverse\") # s1 -> s3\n",
    "            add(\"3\", 2, \"reverse\") # s3 -> s4\n",
    "            add(\"4\", src_edge_port, \"reverse\") # s4 -> h3/h4\n",
    "\n",
    "    # --- START OF FIX ---\n",
    "    # The inter-switch paths must include the final hop to the destination host\n",
    "    \n",
    "    elif _is(device_id_str, \"1\") and port_no == 2: # s1 -> s3\n",
    "        add(\"1\", 2, \"forward\") # s1 -> s3 (will be filtered)\n",
    "        add(\"3\", 2, \"forward\") # s3 -> s4\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        if not is_udp:\n",
    "            add(\"4\", 2, \"reverse\")\n",
    "            add(\"3\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "            \n",
    "    elif _is(device_id_str, \"1\") and port_no == 1: # s1 -> s2\n",
    "        add(\"1\", 1, \"forward\") # s1 -> s2 (will be filtered)\n",
    "        add(\"2\", 2, \"forward\") # s2 -> s4\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        if not is_udp:\n",
    "            add(\"4\", 1, \"reverse\")\n",
    "            add(\"2\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "    \n",
    "    elif _is(device_id_str, \"2\") and port_no == 2: # s2 -> s4\n",
    "        add(\"1\", 1, \"forward\")   # s1 -> s2\n",
    "        add(\"2\", 2, \"forward\")   # s2 -> s4 (will be filtered)\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        if not is_udp:\n",
    "            add(\"4\", 1, \"reverse\")\n",
    "            add(\"2\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "            \n",
    "    elif _is(device_id_str, \"3\") and port_no == 2: # s3 -> s4\n",
    "        add(\"1\", 2, \"forward\")   # s1 -> s3\n",
    "        add(\"3\", 2, \"forward\")   # s3 -> s4 (will be filtered)\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3/h4\n",
    "        if not is_udp:\n",
    "            add(\"4\", 2, \"reverse\")\n",
    "            add(\"3\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "            \n",
    "    # (Other inter-switch paths like s2->s1 or s3->s1 can be added if needed)\n",
    "    # --- END OF FIX ---\n",
    "\n",
    "    else: # Fallback (e.g., h1->s1->s2->s4->h3)\n",
    "        add(\"1\", 1, \"forward\") # s1->s2\n",
    "        add(\"2\", 2, \"forward\") # s2->s4\n",
    "        add(\"4\", dst_edge_port, \"forward\") # s4 -> h3\n",
    "        if not is_udp:\n",
    "            add(\"4\", 1, \"reverse\")\n",
    "            add(\"2\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "\n",
    "    # --- Filter pins ---\n",
    "    final_pins = []\n",
    "    seen = set()\n",
    "    for p in pins:\n",
    "        # Don't overshadow the QoS rule on the enforcement port\n",
    "        if (p[\"direction\"] == \"forward\" and \n",
    "            p[\"deviceId\"] == device_id_str and \n",
    "            int(p[\"out_port\"]) == int(port_no)):\n",
    "            continue\n",
    "        \n",
    "        # For UDP, only keep forward pins\n",
    "        if is_udp and p[\"direction\"] != \"forward\":\n",
    "            continue\n",
    "            \n",
    "        key = (p[\"deviceId\"], int(p[\"out_port\"]), p[\"direction\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            final_pins.append(p)\n",
    "            \n",
    "    return final_pins\n",
    "\n",
    "def make_pin_path_flows_old(device_id_str: str, port_no: int,\n",
    "                        src_ip: str, dst_ip: str, dst_port: int,\n",
    "                        protocol: str = \"tcp\"):\n",
    "    \"\"\"\n",
    "    Build a pinning plan (list of dicts) so src_ip -> dst_ip forward traffic\n",
    "    *must* traverse (device_id_str, port_no).\n",
    "    \"\"\"\n",
    "    proto = protocol.lower()\n",
    "    is_udp = (proto == \"udp\")\n",
    "    pins = []\n",
    "    src_edge_dev, src_edge_port = find_edge_for_ip(src_ip)\n",
    "    dst_edge_dev, dst_edge_port = find_edge_for_ip(dst_ip)\n",
    "\n",
    "    def add(dev_key, outp, direction):\n",
    "        pins.append({\"deviceId\": SW_OF[dev_key], \"out_port\": int(outp), \"direction\": direction})\n",
    "\n",
    "    # --- cases by enforcement point ---\n",
    "    if _is(device_id_str, \"4\") and port_no in (3, 4): # s4 -> h3/h4\n",
    "        if _is(src_edge_dev, \"1\"):\n",
    "            add(\"1\", 1, \"forward\") # s1 -> s2\n",
    "            add(\"2\", 2, \"forward\") # s2 -> s4\n",
    "        if not is_udp and _is(src_edge_dev, \"1\"):\n",
    "            add(\"4\", 1, \"reverse\") # s4 -> s2\n",
    "            add(\"2\", 1, \"reverse\") # s2 -> s1\n",
    "            add(\"1\", src_edge_port, \"reverse\") # s1 -> h1/h2\n",
    "            \n",
    "    elif _is(device_id_str, \"1\") and port_no in (3, 4): # s1 -> h1/h2\n",
    "        if _is(src_edge_dev, \"4\"):\n",
    "            add(\"4\", 2, \"forward\") # s4 -> s3\n",
    "            add(\"3\", 1, \"forward\") # s3 -> s1\n",
    "        if not is_udp and _is(src_edge_dev, \"4\"):\n",
    "            add(\"1\", 2, \"reverse\") # s1 -> s3\n",
    "            add(\"3\", 2, \"reverse\") # s3 -> s4\n",
    "            add(\"4\", src_edge_port, \"reverse\") # s4 -> h3/h4\n",
    "\n",
    "    elif _is(device_id_str, \"1\") and port_no == 2: # s1 -> s3\n",
    "        add(\"1\", 2, \"forward\")\n",
    "        add(\"3\", 2, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(\"4\", 2, \"reverse\")\n",
    "            add(\"3\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "            \n",
    "    elif _is(device_id_str, \"1\") and port_no == 1: # s1 -> s2\n",
    "        add(\"1\", 1, \"forward\")\n",
    "        add(\"2\", 2, \"forward\")\n",
    "        if not is_udp:\n",
    "            add(\"4\", 1, \"reverse\")\n",
    "            add(\"2\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "    \n",
    "    # (Add other inter-switch cases if needed, e.g., s2, s3)\n",
    "\n",
    "    else: # Fallback\n",
    "        add(\"1\", 1, \"forward\") # s1->s2\n",
    "        add(\"2\", 2, \"forward\") # s2->s4\n",
    "        if not is_udp:\n",
    "            add(\"4\", 1, \"reverse\")\n",
    "            add(\"2\", 1, \"reverse\")\n",
    "            add(\"1\", src_edge_port, \"reverse\")\n",
    "\n",
    "    # --- Filter pins ---\n",
    "    final_pins = []\n",
    "    seen = set()\n",
    "    for p in pins:\n",
    "        # Don't overshadow the QoS rule on the enforcement port\n",
    "        if (p[\"direction\"] == \"forward\" and \n",
    "            p[\"deviceId\"] == device_id_str and \n",
    "            int(p[\"out_port\"]) == int(port_no)):\n",
    "            continue\n",
    "        \n",
    "        # For UDP, only keep forward pins\n",
    "        if is_udp and p[\"direction\"] != \"forward\":\n",
    "            continue\n",
    "            \n",
    "        key = (p[\"deviceId\"], int(p[\"out_port\"]), p[\"direction\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            final_pins.append(p)\n",
    "            \n",
    "    return final_pins\n",
    "\n",
    "def extract_port_number(text: str):\n",
    "    \"\"\"\n",
    "    Extract the Ethernet port number from a natural language text.\n",
    "    \n",
    "    Parameters:\n",
    "        text (str): The input text containing the port reference.\n",
    "    \n",
    "    Returns:\n",
    "        int: Extracted port number or None if not found.\n",
    "    \"\"\"\n",
    "    # Mapping of ordinal words to numeric values\n",
    "    ordinals = {\n",
    "        \"first\": 1,\n",
    "        \"second\": 2,\n",
    "        \"third\": 3,\n",
    "        \"fourth\": 4,\n",
    "        \"fifth\": 5,\n",
    "        \"sixth\": 6,\n",
    "        \"seventh\": 7,\n",
    "        \"eighth\": 8,\n",
    "        \"ninth\": 9,\n",
    "        \"tenth\": 10\n",
    "    }\n",
    "\n",
    "    # Match explicit numbers after keywords\n",
    "    match = re.search(r'\\b(?:port|interface|output\\s+node\\s+connector|ethernet)\\s*(\\d+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "\n",
    "    # Match ordinal words (e.g., 'second port', 'third interface')\n",
    "    match = re.search(r'\\b(?:port|interface|output\\s+node\\s+connector|ethernet)\\s*(\\w+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        ordinal_word = match.group(1).lower()\n",
    "        if ordinal_word in ordinals:\n",
    "            return ordinals[ordinal_word]\n",
    "\n",
    "    # Match standalone ordinal words (e.g., 'second')\n",
    "    for word, number in ordinals.items():\n",
    "        if word in text.lower():\n",
    "            return number\n",
    "\n",
    "    return None\n",
    "\n",
    "def parse_intent_text(slicing_info, src_ip, dst_ip):\n",
    "    \"\"\"\n",
    "    Parses the LLM slicing output into a structured plan.\n",
    "    \"\"\"\n",
    "    slicing_queue_id = slicing_info['queue_id'] if (slicing_info['queue_id']) != \"\" else 1\n",
    "    proto = slicing_info['traffic_type'] if \"udp\" in slicing_info['traffic_type'] else (\"tcp\" if \"tcp\" in slicing_info['traffic_type'] or \"http\" in slicing_info['traffic_type'] else \"tcp\")\n",
    "    slicing_l4_port = slicing_info['l4_port'] if slicing_info['l4_port'] != \"\" else 80\n",
    "    port_no = extract_port_number(slicing_info['port_id'])\n",
    "    \n",
    "    # Use the Floodlight dpid extractor\n",
    "    device_id_str = extract_dpid_string_floodlight(slicing_info['switch_id'])\n",
    "\n",
    "    #print(\"\\n***************\\n\", proto, \"\\n*************************\\n\")\n",
    "\n",
    "    return {\n",
    "        \"protocol\": proto,\n",
    "        \"dst_port\": int(slicing_l4_port),\n",
    "        \"device_id_str\": device_id_str, # colon-hex string (or None)\n",
    "        \"port_no\": port_no,             # integer (or None)\n",
    "        \"queue_id\": int(slicing_queue_id),\n",
    "        \"src_ip\": src_ip,               # may be None\n",
    "        \"dst_ip\": dst_ip,               # may be None\n",
    "    }\n",
    "\n",
    "def intent_to_verifier_args(slicing_info, old_src_ip, old_dst_ip, \n",
    "                            main_flow_name: str, \n",
    "                            main_static_flow_rule: dict,\n",
    "                            target_mbps: float = 4.0):\n",
    "    \"\"\"\n",
    "    Parses slicing info and the main flow rule to create\n",
    "    arguments for the verify_qos_flow_with_iperf function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Parse the text intent\n",
    "    plan = parse_intent_text(slicing_info, old_src_ip, old_dst_ip)\n",
    "    \n",
    "    # 2. Check for in_port to lock the source host\n",
    "    if main_static_flow_rule:\n",
    "        in_port = main_static_flow_rule.get(\"in_port\")\n",
    "        if in_port:\n",
    "            # Build reverse map: {(dpid_str, port_num): ip}\n",
    "            port_to_ip = {v: k for k, v in HOST_ATTACH.items()}\n",
    "            \n",
    "            # The device_id_str is from the *intent*. The rule's\n",
    "            # \"switch\" field is the source of truth.\n",
    "            rule_device_str = main_static_flow_rule.get(\"switch\")\n",
    "            \n",
    "            host_ip = port_to_ip.get( (rule_device_str, int(in_port)) )\n",
    "            \n",
    "            if host_ip:\n",
    "                print(f\"[INFO] Rule has in_port:{in_port}, locking source host to {host_ip}\")\n",
    "                plan[\"src_ip\"] = host_ip\n",
    "            else:\n",
    "                print(f\"[WARN] Rule has in_port:{in_port}, but no host is mapped to ({rule_device_str}, {in_port}).\")\n",
    "\n",
    "    # 3. Find the enforcement point (device/port for the queue)\n",
    "    # If dst_ip is now set (from 'plan'), use it\n",
    "    if plan[\"dst_ip\"] and not (plan[\"device_id_str\"] and plan[\"port_no\"]):\n",
    "         device_id_str, port_no = find_edge_for_ip(plan[\"dst_ip\"])\n",
    "    else:\n",
    "         device_id_str, port_no = infer_enforcement_point(plan)\n",
    "        \n",
    "    plan[\"device_id_str\"], plan[\"port_no\"] = device_id_str, port_no\n",
    "\n",
    "    # 4. Fill any remaining missing endpoints\n",
    "    src_ip, dst_ip = fill_missing_endpoints(plan, device_id_str, port_no)\n",
    "    plan[\"src_ip\"], plan[\"dst_ip\"] = src_ip, dst_ip\n",
    "\n",
    "    # 5. Get integer DPID for OVS/Mininet helpers\n",
    "    dpid_int = int(device_id_str.split(\":\")[-1], 16) \n",
    "\n",
    "    # 6. Build verifier args\n",
    "    args = {\n",
    "        \"flow_device_dpid_str\": device_id_str,\n",
    "        \"flow_name\":            main_flow_name,\n",
    "        \"queue_device_dpid_int\":dpid_int,\n",
    "        \"queue_port_no\":        port_no,\n",
    "        \"queue_id\":             plan[\"queue_id\"],\n",
    "        \"src_ip\":               src_ip,\n",
    "        \"dst_ip\":               dst_ip,\n",
    "        \"dst_port\":             plan[\"dst_port\"],\n",
    "        \"target_mbps\":          target_mbps,\n",
    "        \"protocol\":             plan[\"protocol\"],\n",
    "        \"main_static_flow_rule\": main_static_flow_rule\n",
    "    }\n",
    "\n",
    "    # 7. Build Pin plan\n",
    "    pins_plan = make_pin_path_flows(\n",
    "        device_id_str, \n",
    "        port_no, \n",
    "        src_ip, \n",
    "        dst_ip, \n",
    "        plan[\"dst_port\"],\n",
    "        plan[\"protocol\"]\n",
    "    )\n",
    "    \n",
    "    return args, pins_plan, plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# done intent = \"Forward TCP traffic on port 80 destined for 10.0.1.3 via interface 3, assigning it to queue 1 for prioritized handling in switch 4.\"\n",
    "# done intent = \"In switch 4, traffic destined for 10.0.1.4 should use port 4.\"\n",
    "# done intent =  \"In switch 4, block all IPv4 traffic from 10.0.1.1 to 10.0.1.4 with a high priority, ensuring the switch operates as a firewall.\"\n",
    "# done intent = \"Drop all traffic from 10.0.1.3 on switch 2 while forwarding all other traffic normally.\"\n",
    "# done intent = \"If interface 1 on node 2 receives UDP to port 80, pass via port 2, queue 1.\"\n",
    "# done intent = \"If interface 1 on node 2 receives UDP to port 5201, pass via port 2, queue 1.\"\n",
    "# done intent = \"In node 1, traffic destined for 10.0.1.2 should use port 4.\"\n",
    "# done intent = \"In switch 2, traffic from port 1 should pass through port 2.\"\n",
    "# done intent = \"Traffic from port 2 of switch 2 to 10.0.1.1 should use interface 1.\"\n",
    "# done intent = \"In switch 4, traffic from 10.0.1.1 to 10.0.1.4 should use output interface 4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = time.time()\n",
    "\n",
    "slicing_info = run_LLM_Slice(intent)\n",
    "\n",
    "# This function is OVS-based and fine\n",
    "# create_two_queue_for_switch_handler(slicing_info)\n",
    "\n",
    "# device_id here is the integer DPID\n",
    "deployment_status, flow_id, device_id_int, translated_flow_rule, operational_flow_rule = end_to_end_IBN(intent)\n",
    "\n",
    "if 'use_queue' in slicing_info and slicing_info['use_queue'] == 1 and deployment_status:\n",
    "    #Creating one queue\n",
    "    expected_queue_rate_mbps = 4.0\n",
    "    port_max = 100_000_000\n",
    "    min_rate = max_rate = int(expected_queue_rate_mbps * 1_000_000)\n",
    "    slicing_queue_id = slicing_info['queue_id'] if (slicing_info['queue_id']) != \"\" else 1\n",
    "    port_no = extract_port_number(slicing_info['port_id'])\n",
    "    \n",
    "    # We need the integer DPID for this OVS helper\n",
    "    ensure_qos_cap(device_id_int, port_no, slicing_queue_id, min_bps=min_rate, max_bps=max_rate, port_cap_bps=port_max)\n",
    "\n",
    "global llm_caller_flag\n",
    "\n",
    "if (deployment_status == True):\n",
    "        proc_time_s = (time.time() - current_time)\n",
    "        print(f\"\\n\\nSuccessfully translated and installed the rule in Floodlight Controller. Time taken: {proc_time_s:.2f}s\")\n",
    "        print(\"\\nThe translated flow rule is: \", translated_flow_rule)\n",
    "        \n",
    "        # 'flow_id' is the flow 'name'\n",
    "        # 'device_id_int' is the integer DPID\n",
    "        \n",
    "        src_host, dst_host, src_ip, dst_ip = extract_host_and_ip_floodlight(translated_flow_rule)\n",
    "        flow_rule_type, flow_rule_specificity = classify_floodlight_flow_rule(translated_flow_rule)\n",
    "        \n",
    "        append_intent_to_store(\n",
    "            \"IntentStore_Floodlight.jsonl\", # <-- Use a new store file\n",
    "            nl_intent=intent,\n",
    "            json_flow_rule=translated_flow_rule,\n",
    "            device_id=translated_flow_rule.get(\"switch\"), # Store DPID string\n",
    "            flow_id=flow_id, # Store flow name\n",
    "            intent_type=flow_rule_type,\n",
    "            intent_specificity=flow_rule_specificity\n",
    "            )\n",
    "        \n",
    "        if (flow_rule_type== \"security\"):\n",
    "            # Get the DPID string for the API\n",
    "            dpid_str = translated_flow_rule.get(\"switch\")\n",
    "            ping_count, candidate_src_ip, candidate_dst_ip, ping_output = floodlight_assurance_for_security_intent(src_ip, dst_ip, device_id_int)\n",
    "            \n",
    "            if (llm_caller_flag == 1):\n",
    "                print(\"\\nAsking LLM to generate corrective actions...\")\n",
    "                assurance_LLM_prompt = generate_corrective_action_prompt_floodlight(intent, operational_flow_rule, dpid_str, ping_count,\n",
    "                                    candidate_src_ip, candidate_dst_ip, ping_output)\n",
    "                llm_response = Run_assurance_LLM (assurance_LLM_prompt)\n",
    "                print(llm_response)\n",
    "                parse_and_execute_corrective_actions_floodlight(operational_flow_rule, llm_response, device_id_int)\n",
    "\n",
    "\n",
    "        elif (flow_rule_type== \"qos\"):\n",
    "            \n",
    "            target_mbps = 4.0\n",
    "            \n",
    "            # flow_id is the 'name'\n",
    "            main_flow_name = flow_id \n",
    "\n",
    "            # NEW LINE:\n",
    "            # We pass 'operational_flow_rule' which has the correct \"0x11\"\n",
    "            #args, pins_plan, plan = intent_to_verifier_args(slicing_info, src_ip, dst_ip, main_flow_name, operational_flow_rule, target_mbps)\n",
    "            \n",
    "            \n",
    "            args, pins_plan, plan = intent_to_verifier_args(\n",
    "                slicing_info, \n",
    "                src_ip, \n",
    "                dst_ip, \n",
    "                main_flow_name, \n",
    "                translated_flow_rule, # Pass the rule itself\n",
    "                target_mbps\n",
    "            )\n",
    "            protocol = plan[\"protocol\"]\n",
    "            dst_port = args[\"dst_port\"]\n",
    "\n",
    "            try:\n",
    "                # 2) Call the verifier\n",
    "                verify_qos_flow_with_iperf(\n",
    "                    flow_device_dpid_str=args[\"flow_device_dpid_str\"],\n",
    "                    flow_name=args[\"flow_name\"], \n",
    "                    queue_device_dpid_int=args[\"queue_device_dpid_int\"],\n",
    "                    queue_port_no=args[\"queue_port_no\"],\n",
    "                    queue_id=args[\"queue_id\"],\n",
    "                    src_ip=args[\"src_ip\"],\n",
    "                    dst_ip=args[\"dst_ip\"],\n",
    "                    dst_port=dst_port,\n",
    "                    target_mbps=args[\"target_mbps\"],\n",
    "                    protocol=protocol,\n",
    "                    pin_path_flows=pins_plan,\n",
    "                    main_static_flow_rule=args[\"main_static_flow_rule\"],\n",
    "                    \n",
    "                    # --- Explicit iperf parameters ---\n",
    "                    duration_sec=8,\n",
    "                    parallel=8 if protocol == \"tcp\" else 1,\n",
    "                    tcp_mss=1200,\n",
    "                    tolerance_pct=10.0,\n",
    "                    udp_bw_mbps=50.0,\n",
    "                    udp_len_bytes=1200\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"\\n--- QoS VERIFICATION FAILED ---\")\n",
    "                print(f\"An error occurred during QoS verification: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            finally:\n",
    "                # Verifier has its own cleanup, but we pass\n",
    "                pass    \n",
    "             \n",
    "        elif (flow_rule_type== \"forwarding\"):\n",
    "             # You can implement a forwarding assurance test here (e.g., ping)\n",
    "             print(f\"\\n[INFO] Forwarding rule {flow_id} installed.\")\n",
    "             # ONOS_assurance_for_forwarding_intent(src_host, dst_host, src_ip, dst_ip)\n",
    "\n",
    "        elapsed_time = (time.time() - current_time)\n",
    "        print(f\"\\nTime taken for end-to-end IBN: {elapsed_time:.2f}s\")\n",
    "\n",
    "elif (flow_id == \"Tie\"):\n",
    "    print(\"\\n\\nReport to the operator about this conflict resolution issue. Need adjustment to conflict resolution policy.\\n\")\n",
    "\n",
    "elif (flow_id == \"existing_rule_win\"):\n",
    "    print(\"\\n\\nThe new intent conflicts with an existing one having a higher priority according to current policy, hence the new intent was not installed. See the existing flow rule that conflicts.\\n\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\\nLLM failed to produce meaningful response. Either update context example or model.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_id = \"intent_45-38f99444\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- Attempting to delete flow: {flow_id} ---\")\n",
    "try:\n",
    "    # Call the delete function from [Cell 4]\n",
    "    delete_success = delete_flow_floodlight(flow_id)\n",
    "    \n",
    "    if delete_success:\n",
    "        print(\"--- Deletion Successful ---\")\n",
    "    else:\n",
    "        print(\"--- Deletion Failed (see warning above) ---\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during deletion: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
